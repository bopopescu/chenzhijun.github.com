<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Today-2018-12-18]]></title>
    <url>%2F2018%2F12%2F18%2Ftoday-2018-12-18%2F</url>
    <content type="text"><![CDATA[Today-2018-12-18 mysql pxc 无法启动，not the lastone leave cluster A: vi /data/mysql/grastate.dat safe_to_bootstrap 0 改为1 mysql字符问题。 A： 今日遇到遇到一个mysql诡异的问题，用docker-compose启动的应用，第一次新建的时候字符乱码，重启一次就好了。这个问题异常诡异。找了许久没有找到原因。暂且记录一下。 squid 代理access deny。 A: 搭建完squid代理，在squid机器上通过curl -xlocalhost:3128 www.baidu.com可以访问，但是在其他机器使用curl -x192.168.12.1:3128 www.baidu.com出现access deny。将/etc/squid/squid.conf中在http_access deny all前面添加http_access allow all golang template A： 这个变量如果不定义loop将会引起golang template异常。使用funcMap()]]></content>
      <categories>
        <category>Daily</category>
      </categories>
      <tags>
        <tag>daily</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 用户与权限设置]]></title>
    <url>%2F2018%2F12%2F17%2Fmysql-grant%2F</url>
    <content type="text"><![CDATA[MySQL 用户与权限设置这几天开发完发现dba对于权限控制的比较严，通常是没有root权限的，在正式上线前，还是希望能再通过一个普通用户的权限来做一次预发布。这样可以看看到底会有哪些坑，提前踩一下可能比较好。后来发现，其实你只要准备需求提给dba就ok了，人家会帮你处理的非常好~~~。总之在这过程中遇到的问题，先记录一下吧。 MySQL创建用户第一个就是创建一个普通用户。使用root登陆后，创建一个用户：CREATE USER &#39;username&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;paasword&#39;;其中username,paasword按需修改。一般来说这里需要注意的就是localhost，为什么？因为这个字段稍不注意就会才坑，这里的localhost和%，其实指代的是只允许本地登录和允许所有地方登陆。如果你的应用部署在同一台机器上，那么你用localhost没问题，但是如果应用和数据库是分开机器部署的，那么这里要写成%，不然就会出现远程无法连接。当然还有很多其他的设置，比如什么过期时间啊，证书登录啊，具体的可以看下官方文档:Create User。 给MySQL用户赋权限如果做开发的话，你就会发现，你的jdbc或者其他语言连接数据库，都需要选择选择一个库。也就是你必须先在数据库里面建立一个database，但是如果你用root建立一个database，比如：create database DB_USER;这个时候上一步创建的用户是无法访问这个库的。如果你切换到刚刚的用户，那么你也是没有权限建立数据库的。但我们通常开发都会写上库名，那这个时候怎么操作了？嗯，就是先用root建库，然后将权限库的权限赋值给新用户。具体操作如下： 12CREATE DATABASE db_user;GRANT ALL ON db_user.* TO 'username'@'%'; 这里一定要注意%,localhost，如果这里不同，那就是两个用户。 当然，如果你觉得只给查的权限就足够了，那么只需要GRANT SELECT ON db_user.* TO &#39;test&#39;@&#39;%&#39;;。那么有没有更详细的了了？当然有，文档始终是最详细的，我只是记录我使用的过程。文档地址：grant 一些比较使用的使用方式： 查看当前用户：select CURRENT_USER(); 查看当前用户权限：show grants;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 快速搭建 MySQL 和 Redis]]></title>
    <url>%2F2018%2F12%2F16%2Fdocker-mysql-and-redis%2F</url>
    <content type="text"><![CDATA[Docker 快速搭建 MySQL 和 RedisMySQL 环境搭建最近做开发的时候遇到一个比较有意思的事情，如何搭建一个数据库，还有相关的redis等。因为没有外网，不可能说直接yum安装，又不想到每个官网去找相应的安装包。自然的，就想到了用Docker。一开始用到docker安装一个mysql的时候确实也是非常方便，比如一个docker run命令就启动了一个mysql，但是开发到一部分的时候，发现。我去！怎么又乱码？？ 咦，怎么市区也不对了？GG，发现还有很多小问题。今天有空一并总结下，下次如果有这种事就可以直接用了。 使用Docker启动启动Mysql容器如果需要一个mysql数据库，直接使用docker来运行一个容器：docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root123456 mysql:5.7 这样你就可以在本地ip+3306端口来访问一个mysql数据库了，root密码是：root123456。第一次使用docker的时候就是因为这个原因被吸引了。想想如果本地安装需要做多少配置，而是用docker一条命令就帮你把那些复杂的操作都隐藏了。这种便利性，我想谁都不会说不想要。 不过虽然便利是便利了，但是还是需要注意一些问题： 字符问题mysql默认其实是latin的字符集，docker 启动mysql的时候其实也是使用的默认字符。而我们做开发一般都是使用UTF-8的字符集，那出现这种情况该如何更改配置了？可以在启动的时候加上两个变量：docker run -d -e MYSQL_ROOT_PASSWORD=root123456 mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci 数据磁盘问题一个容器，如果那天不小心删除了，然后你重建就会发现之前的数据没有了，这种情况当然是不行啦。那么怎么将数据盘挂载出来：加上 -v，让存储在容器里面的数据存储到本地自定义的盘中：-v /data/mysql:/var/lib/mysql。 默认数据库，以及时区修改有的时候我们会需要导入一些表或者一些数据，这个时候该怎么操作? 其实也有一个环境变量：MYSQL_DATABASE;然后将数据库的初始化脚本放到/docker-entrypoint-initdb.d/目录下也就是将sql挂载到该目录下： 1docker run -d --name mysql -p 13306:3306 -e MYSQL_ROOT_PASSWORD=root123456 -e MYSQL_DATABASE=DB_USER -e TZ=Asia/Shanghai -v $PWD/sql-scripts/:/docker-entrypoint-initdb.d/ -v /data/mysql:/var/lib/mysql mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci 将数据库的sql放到当前目录的sql-scripts/目录下。 Redis 环境搭建redis的搭建其实要比mysql要简单些，毕竟redis我们一般都只是用来当作缓存，而不会将数据持久化，所以只需要将一个容器run起来就可以了。不过我们一般会有redis的密码需要，所以完整的命令如下： docker run -d --name redis -p 6379:6379 redis:latest --requirepass &quot;123456&quot; 这样redis就启动了，如果需要开机启动，加上--restart=always。 好了，今天一篇搭建mysql和redis的过程就到这里了。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重启Linux主机后自动运行任务或者脚本]]></title>
    <url>%2F2018%2F10%2F30%2Flinux-auto-excute-shell-script-after-rebooting%2F</url>
    <content type="text"><![CDATA[重启Linux主机后自动运行任务或者脚本有时候我们在主机上做了一些agent应用，这些应用平常都是主机启动，agent就需要启动。相当于“伴生”。尽管第一次或者第二次我们能依靠记忆或者自我约束来启动这些agent，但是有时候还是会免不了忘记。那么有没有办法让这种agent做成开机启动呢？下面提供两种Linux设置开机启动应用的方法。 1. crontab 实现脚本(应用)开机启动crontab的介绍在我的另一个博客里面crontab使用linux crontab定时清理n天前的日志文件，这里我们介绍一个使用它来实现开机启动的用法。 使用crontab -e，在crontab的编辑页面增加下面的内容:@reboot sleep 10 &amp;&amp; bash /root/test.sh，sleep 10 是指在开机10秒后启动，启动的脚本是/root/test.sh。里面的内容，可以自己根据需要编写。最后要记住 将test.sh脚本设置为可执行权限 744 或者 777 或者 a+x。这样一个简单的开启启动任务就完成了。 2. systemd 服务实现脚本(应用)开机启动systemd 是linux机器的一个daemon服务。主要是创建一个.service文件，然后进行开机启动。下面来看一下实际使用过程。 vi /etc/systemd/system/auto_exe.service 1234567891011[Unit]Description=auto exec after reboot[Service]Type=oneshotExecStart=/usr/local/bin/auto.shRemainAfterExit=yes[Install]WantedBy=multi-user.target 一个systemd的service由三部分构成:Unit,Service,Install。详细的内容或者介绍可以使用man，或者google。 我这里表示的是在机器启动后，执行/usr/local/bin/auto.sh。 auto.sh，记得将权限赋值为a+x，777，或者744123#!/bin/bashecho "hello" &gt;&gt; /home/auto_exe.log 之后使用systemctl daemon-reload刷新service，我们可以使用systemctl start auto_exe 进行调试，看脚本是否执行。使用systemctl status auto_exe查看服务状态。如果成功，那么就可以加入确保启动执行脚本，使用systemctl enable auto_exe，这样就设置好了。 简单的linux应用开机启动就设置好了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux cron clean log files older than N days]]></title>
    <url>%2F2018%2F10%2F29%2Flinux-crontab-clean-ndays-log%2F</url>
    <content type="text"><![CDATA[使用linux crontab定时清理n天前的日志文件最近有个需求，需要在linux机器上定时执行清理n天前的日志文件。其实我开始做了个更有意思的清理工具，根据alertmanager做webhook，然后在每个Linux机器上开启一个agent，收到请求再执行清理。不过使用crontab也是一个非常有用的工具。 crontab是Linux的一个守护进程，定时执行的工具。详细的内容可以使用man crontab查看。废话不多说，直接来看怎么使用它。只有用起来，才是属于自己的。 查看当前有哪些定时任务crontab -l 查看当前已经存在的定时任务。 可以看到，你需要的就是准备一个shell脚本（任务指令），一个定时时间（执行频率），一条触发指令（程序入口）。准备好这三个东西就可以。 下面我们以一个简单的需求来演示。 需要删除 /data 目录下文件名存在的带 log 的文件，修改这些文件的大小为0。 设置一个新的定时任务crontab -e 可以进入crontab编辑页面。将带‘*’的那行复制成新的行，如下： 可能会问，前面的‘*’的时间怎么设置。可以使用cat /etc/crontab查看解释： 之前的需求，我将clean3dayslog.sh文件放到/root目录下，然后执行文件，clear3dayslogs.sh的内容为： 123#!/bin/bashfind /data -mtime +3 -name "*.log*"|grep rtlog|xargs -i truncate -s 0 &#123;&#125; 这个demo比较简单，不过用处确实很大，一般清除的都在开发环境上，日志输出。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conclusion-201809]]></title>
    <url>%2F2018%2F09%2F08%2Fconclusion-201809%2F</url>
    <content type="text"><![CDATA[# 1: linux命令行替换文本中的字符串。sed -i &quot;s/100.69.224.18:10099/100.69.224.27:9999/g&quot; /file.xml 2: rancher平台的相关监控。 123456789101112131415mysql:获取mysql用户是否可以远程登陆CREATE USER &apos;exporter&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;exporter&apos;;GRANT PROCESS, REPLICATION CLIENT ON *.* TO &apos;exporter&apos;@&apos;localhost&apos;;GRANT SELECT ON cattle.* TO &apos;exporter&apos;@&apos;%&apos; with MAX_USER_CONNECTIONS 3;help flush;show grants for &apos;exporter&apos;@&apos;localhost&apos;;flush PRIVILEGES;docker run -d -p 9104:9104 -e DATA_SOURCE_NAME=&quot;exporter:exporter@(127.0.0.1:3306)/&quot; prom/mysqld-exporter:latesthaproxy:docker run -d -p 9101:9101 prom/haproxy-exporter:latest --haproxy.scrape-uri=&quot;http://www.haproxy.com/haproxy?stats;csv&quot; 3: sort 根据低三列排序 docker stats -a –no-stream |awk ‘{print $1,$8,$3,$4}’|sort -k 2,4n sort -k a,bna为第几列，b为几个字符，n为数字比较，1234，1222，从第一个字符开始，到第四个字符都会比较 4: 给基础镜像安装常用工具：ping,curl,wget,netstat。一般源里面就已经包含了，ping的源为：apt install inetutils-ping或者 apt install net-tools 5：某次给rancher平台扩容，直接将主机加上之后，将原来的应用直接*2启动，平台爆掉了。server不到。]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux 挂载磁盘]]></title>
    <url>%2F2018%2F07%2F26%2Flinux-mount-disk%2F</url>
    <content type="text"><![CDATA[linux 挂载磁盘最近需要挂载磁盘，记录一下。 一个500G的磁盘。使用fdisk查看，如果没有就使用lsblk可以查看到现在有哪个磁盘没有挂载。 之后就是挂载操作了。切换成root用户,查看磁盘使用的卷类型：ext4 , xfs 1: 然后使用mkfs -t xfs（或ext4） /dev/vdb xfs,ext4 ：是指格式化成什么磁盘类型 /dev/vdb ：是指要挂载的磁盘 2: 挂载点 mkdir /data ，建立一个挂载点 3：挂载磁盘mount /dev/vdb /data 将刚刚格式化的磁盘挂载到/data目录 4：修改/etc/fstab文件，复制一行然后修改就可以了，将最后一个数字改为2。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows使用sshkey连接GitHub]]></title>
    <url>%2F2018%2F07%2F17%2Fwindows-connect-github-with-sshkey%2F</url>
    <content type="text"><![CDATA[Windows使用sshkey连接GitHub经常连接github上传东西，经常push的时候需要输入用户名和密码。下面是使用ssh-key免密的方式访问自己的github仓库。 首先在本地生成ssh-key: 进入到~/,用户根目录，查看.ssh文件夹是否有相关的公钥私钥如果没有新生成： 1ssh-keygen -t rsa -C &quot;xxx@yy.com&quot; 之后再打开~/.ssh/id_rsa.pub拿到公钥内容，复制所有内容； 在github找到设置，add-sshkey，新建一个ssh-key。将内容粘贴进去。 之后再本地的git仓库下使用： ssh -T git@github.com 进行验证。]]></content>
      <categories>
        <category>ssh</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Ansible批量操作服务器]]></title>
    <url>%2F2018%2F07%2F16%2Fhow-to-use-ansible-to-do-something%2F</url>
    <content type="text"><![CDATA[使用Ansible批量操作服务器Anbible 是干嘛的？对于一个非专业运维人士（我）来说，它就是我批量操作服务器的一个神器。试想一个场景：公司内部DNS还未搭建好，业务系统使用了域名做请求，这个时候需要你将域名制定到某台机器上，你这个时候就只能修改hosts文件了。嗯，如果是一两台服务器就算了，大不了手动ssh上去改一改，但是如果是10台了？10台不够，100台了？这个时候怎么办？你可能说我召集了一帮兄弟，大家一人改几个。OK，好不容易你改完了，这个时候业务跟你说，嗯，那台服务器挂了，不稳定，暂时换到另一台服务器上另一个IP地址。兄弟，听说醉经淘宝刀打折，买一把吧。哈哈 但是如果这个时候我们使用ansible，这个时候你就可以早点干完，早点回家陪老婆孩子了。接下来我们看看怎么使用ansible，请注意，我只是说怎么使用，是的，怎么使用，没有任何理论，不会讲解任何深的东西，只是用而已。1：批量操作服务器。 ansible的一个简单目录结构： install.yml文件内容： 123456789---- hosts: all remote_user: rhlog gather_facts: no roles: - role: oam become: yes become_method: su become_user: root hosts 文件内容： 1210.0.80.34 ansible_become=true ansible_become_method=su ansible_user=ssh_user ansible_ssh_pass=&quot;password&quot; ansible_become_pass=&quot;ax=n@#*!EM&quot; ansible_become_user=root10.07.80.37 ansible_become=true ansible_become_method=su ansible_user=ssh_user ansible_ssh_pass=&quot;W)PIukAa&quot; ansible_become_pass=&quot;r.*o)Hg!z&quot; ansible_become_user=root 这两个是ansible的根目录里面比较重要的文件。install.yml里面remote_user是指登录到服务器的用户，通常大家不会让root用户远程登录的。roles是指在roles文件里面有哪些角色，你要用使用哪个角色。hosts文件一般保存的账号密码。按照这个模式登录就好了。之后我会提供这个安装包，大家可以自己照着改。哈哈。 oam角色里文件内容为：主要看tasks目录里面： 123456789101112131415161718---# tasks file for oam- name: upload file copy: src=/app/installation/resource/moving/test/roles/oam/files/data.sh dest=/home/data.sh mode=755- name: excute shell: bash /home/data.sh- name: find file find: paths: /home/ patterns: &quot;*.log&quot; recurse: no register: file_2_fetch- name: fetch fetch: src: &quot;&#123;&#123; item.path &#125;&#125;&quot; dest: /home/diskdata/ flat: yes with_items: &quot;&#123;&#123; file_2_fetch.files &#125;&#125;&quot; 1234# !/bin/bashname=$(hostname -i)file=$&#123;name%% *&#125;df -h &gt;/home/rhlog/$file.log 在ansible根目录我怎么使用了？ansible-playbook -i hosts install.yml 我的这个程序就是将files目录下的data.sh上传到hosts里面的所有主机中，然后执行data.sh，将df -h的内容输出到ip.log文件中，然后将文件下载到本地/home/diskdata中。这样就相当于批量操作了所有的机器。 另外一个使用方式，不使用上面那种方式，而是直接使用命令行的方式： 1： 判断所有机器是否可以访问某个地址 1ansible all -i hosts -m get_url -a &quot;url=http://192.168.11.32:8088/srv/releaseService?wsdl dest=./&quot; 2: 判断所有机器是否可以ping通某个地址 1ansible all -i hosts -m shell -a &quot;ping -c 3 10.7.18.3&quot; 3: 查找所有机器上某个应用是否启动 1ansible all -i hosts -m shell -a &quot;ps -ef | grep haproxy | grep -v grep&quot; 4: 判断所有机器能否连通某个端口 1ansible all -i hosts -m shell -a &quot;telnet 192.167.180.3 25&quot; 5: 批量修改某个文件 1ansible all -i ./hosts -s -m shell -a &quot;echo \&quot;127.0.0.1 test.local.com\&quot; &gt;&gt; /etc/hosts&quot; ps: -s,是指使用root账户]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-06月至07月的一些总结]]></title>
    <url>%2F2018%2F07%2F16%2F2018-06and07-conclusion%2F</url>
    <content type="text"><![CDATA[2018-06月至07月的一些总结最近好久都没有写博客了。其实最近做的事情都比较单调，就是迁移环境，不过再迁移环境中，我还是收获了很多我缺少的。单从非技术方面，我缺少勉才的那股韧劲（问题不会过夜），缺少华哥那种细致（仔细的做过一遍，还会仔细的检查一遍，不是那种大致检查，是那种一条一条的比对），当然我也看到有些同事的缺点也是我的缺点，有则改之。 在搬迁中，华哥是我们部的总负责人，他对所有的事情都会有把控，搞不定的事情都会找他。而且他总能找到合适的方法解决。最重要的是，我犯错他却从来没有指责过，只是说怎样弥补，然后跟我们说正确的方法，而后我觉得挺对不住他的。总会想着把事情再做好点。勉才是小组的执行人，执行人真的超级强，问题基本没过夜，总是在12点的时候或者第二天一早问他，他就说问题解决了。。简直了。 这次搬迁我们是轮值的，也就是一开始将所有任务分配好，然后各自自己实验一次，写下步骤，下一轮再给另一个人按照写的步骤实行一次，这样来回3轮。最后基本上，步骤文档就没啥问题了。任何人都可以按照文档完整的搭出系统。这个过程会遇到一些问题，有些已经遗忘，有些所幸有所备份。 Q: Docker 新镜像里面脚本无权限A: 这个问题是脚本在windows传递到linux系统里面可能会出现，如果看ls，那么X的权限都没有了。这个时候解决拌饭就是chmod a+x xxx.sh Q: Docker 导入导出镜像A：镜像保存：docker save -o image-name.tar image-name:latest，镜像导入：docker load &lt; image-name.tar Q: Docker 运行一个容器，执行完之后就退出A: docker run --rm image:latest bash/sh Q: 从运行中的容器拷贝文件/文件夹到宿主机A: docker cp container-id:/path/to/file /path/to/file Q: 修改Docker的docker.serviceA：sudo cat /lib/systemd/system/docker.service,可以使用find / -name docker.service来找到相应文件。修改这个文件之后需要重启daemon。systemctl daemon-reload。在这个文件里面可以增加docker的http,https的代理。 Q: 遇到Docker push镜像的时候出现https的问题A: 在/etc/docker/daemon.json文件中增加：[“insecure-registries”:[“registry.cmrh.com:5000”] Q: “can’t create unix socket /var/run/docker.sock: is a directory”A: rm -fr /var/run/docker.sock/ Q: 容器内部无法访问外网：A: 1：检查容器内部的dns，cat /etc/resolv.conf 2：检查宿主机的dns， Q: ubuntu18 重启无法进入桌面A: 选择联网，让其然后下载显卡厂商的驱动。 Q: Linux统计一个文件夹下某类文件的数量A: ls ./* | wc -l Q: 查找当前目录使用情况：A: du -h Q: 查看磁盘使用情况：A: df -h Q: 修改文件所有者A: chown -R user:group xxFile/xxDir Q: shell 文件分割字符串A:如下面的代码： 1234# !/bin/bashname=$(hostname -i)file=$&#123;name%% *&#125; //分割空格df -h &gt;/home/rhlog/$file.log Q: find 使用 Q: AWK 使用，grep 使用 Q: haproxy 使用 Q: Ansible 直接使用命令 Q: Dockerfile 制作镜像]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus 常用的查询语句]]></title>
    <url>%2F2018%2F07%2F16%2Fprometheus-query%2F</url>
    <content type="text"><![CDATA[prometheus 常用的查询语句通常我们会使用grafana作为图表展示，然后选择prometheus作为数据源的方式来进行我们想要的图表展示。当然我们也可以在grafana的官网上找到相应的dashboard来直接导入，这样省去了自己手工配置的麻烦。不过知道一些必要的prometheus查询语句能帮我们更好的选择grafana的dashboard，然后我们可以自定义做些配置。 prometheus一些术语prometheus的metrics分为四类（counter,gauge,histogram,summary）详情(metrics-type)，metrics_name的命令也应该符合一定的规范：metrics-name，METRIC AND LABEL NAMING metrics_name ： 也就是指标名，通常我们都会用如http_request_total等进行查询;metrics_label ：指标的标签，也就是metrics_name{label-1=”a”,label-2=”b”}这种;metrics_value : 通常用指标名+标签查出来一个值，该值根据metrics的类型可能为浮点数，也可能为整数。 0,基础查询也就是使用metrics_name+metrics_label的组合进行查询，这种查询的效率的基础是你要知道明确知道相应的name和label，如果有错误拼写，则可能数据无法展示，prometheus的查询工具能帮我们模糊匹配出所有的metrics_name。可以使用label进行筛选。如果只记得部分metrics_name,那么可以使用内置的label：{__name__=~&quot;metrics_name_you_remember:.*&quot;}这样去匹配出来你想要的标签。 1,正则匹配查询prometheus里面用的最多的查询可能就是正则匹配了。经常配合grafana的变量(variable)一起使用。比如：sum(irate(node_disk_reads_completed{device!~&quot;dm-.*&quot;}[5m])),这里面的device!~&quot;dm-.*&quot;，后面引号内的dm-.*就是不匹配dm-前缀的所有metrics_name。如果要匹配的话使用=~。这样要注意，如果是或的话要用(regrex_A|regrex_B),用小括号加|。 2,常用的操作符和函数prometheus支持常用的操作符：+,-,*,/,&gt;=,&lt;,&gt;,....等 count是对查询的结果数量进行总和+; 而sum是对查询出来的value进行总和+; 它也支持比如topk,bottomk,min,max等等。 更多的查询在官网还有更多的查询，通常我们是结合grafana来做。官网的查询:proetheus-query。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最近总结-2018-07-16]]></title>
    <url>%2F2018%2F07%2F16%2Fconclusion-2018-07-16%2F</url>
    <content type="text"><![CDATA[最近总结-2018-07-16最近公司机房搬迁，大大小小演练，解决问题搞了差不多一个月。在以前的公司其实从来没有遇到过应用搬迁的事情。那个时候还是个开发写代码的。可能有搬迁也不需要我来帮忙，但是这次公司搬迁，从前到后。基本上全程参与。只有一个感受，那就是真的是严谨的要命。 1：演练搬迁之前，我们内部就演练了3次，之后申请搬迁后的资源又演练一次，每一次都做了详尽的记录，细微到每一条命令是怎么敲，可以说，拿到文档，就算你啥也不会，你也可以完成。为啥这么细微了，因为有一部分是我的写的文档，写了之后，在下一轮测试演练，拿给别的同事做我的部分。发现他尽然无法完成。这个时候发现，原来我认为的简单，在别人那里是那么复杂。 2：记录在演练的时候会有各种异常，所有人都必须将遇到的异常以及所有异常的原因，解决方法，一一列出。这种情况保证了在异常出现，如果当事人不在，应该怎么处理。 3：协调协调指的是协调人员，协调机器。由统一的人负责协调资源，什么时候资源会出来，哪些资源先出来，资源出来后，哪些应用可以先启动。在统筹人员那里都有。甚至连后勤都来了。谁来保证住房，哪些人去订餐。相当于大事小事全都准备好了。 4：时间在开始切换前，先做一次正式环境测试，跑通所有应用。这个时候拿的都是一期的环境和配置，二期只是再重新部署一遍。第一次跑通之后开始切换，提前在官网发出声明，通知相应的切换人员，网络组，主机组，容器组等。之后再将所有的人员统一在办公环境办公，有问题及时解决，然后按照预定好的时间1——3点主机切换，2-4点网络切换，3点应用停掉一期，DB断开所有连接，。。。切换DB，启动应用…一步一步。基本上不会出错。 5：复查验证保证所有切换的应用都是可用的。必须是所有应用，然后才是所有人完成搬迁。 总体来说，每一个细节都会照顾到，而且在正式搬迁之前需要进行多次验证，多次演练，之后在正式搬迁的时候才能将风险降到最低。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alertmanager发送的邮件中external-url修改机器名为IP地址]]></title>
    <url>%2F2018%2F06%2F09%2Fprometheus-alertmanager-external-url%2F</url>
    <content type="text"><![CDATA[alertmanager发送的邮件中external-url修改机器名为IP地址在使用alertmanager发送报警邮件的时候，我们通常会采用模板。比如我的一个模板：12345678910111213141516171819202122232425262728293031&#123;&#123; define &quot;email.czj.html&quot; &#125;&#125;详情：&lt;br/&gt;&lt;br/&gt;&#123;&#123; range .Alerts.Firing &#125;&#125; &lt;tr&gt; &lt;td class=&quot;content-block&quot;&gt; &#123;&#123; if gt (len .Annotations) 0 &#125;&#125;&lt;strong&gt;描叙信息：&lt;/strong&gt;&lt;br /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br /&gt;&#123;&#123; end &#125;&#125; &lt;strong&gt;指标详情：&lt;/strong&gt;&lt;br /&gt; &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br /&gt;&#123;&#123; end &#125;&#125; &lt;a href=&quot;&#123;&#123; .GeneratorURL &#125;&#125;&quot;&gt;链接到Prometheus&lt;/a&gt;&lt;br /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;br/&gt;&lt;br/&gt;&#123;&#123; end &#125;&#125;&lt;br/&gt;&lt;br/&gt;&lt;div&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class=&quot;footer&quot;&gt; &lt;table width=&quot;100%&quot;&gt; &lt;tr&gt; &lt;td class=&quot;aligncenter content-block&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&apos;&#123;&#123; .ExternalURL &#125;&#125;&apos;&gt;由 &#123;&#123; template &quot;__alertmanager&quot; . &#125;&#125;发送&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&#123;&#123; end &#125;&#125; 如果注意到会发现有：GeneratorURL和ExternalURL。这两者默认使用的是机器名称也就是hostname。这样我们就很难在邮件中获取到实际的promtheus和alertmanager地址。查了很多资料，最后发现在prometheus和alertmanager启动的时候我们可以设置这两个值的。prometheus的启动命令是： 123./prometheus.exe --web.external-url="http://127.0.0.1:9090/prom" \ --web.route-prefix=prom \ --web.listen-address="0.0.0.0:9090" alertmanager启动命令是： 123./alertmanager.exe --config.file=config163.yml \ --web.external-url=http://127.0.0.1:9093/alertmanager \ --web.route-prefix=alertmanager 这里的web.external-url也就是GeneratorURL和ExternalURL两者在email中的指，在设置web.external-url的同时我们需要记得设置web.router-prefix的值，应为web.router-prefix的默认值是web.external-url，如果不同时指定web.router-prefix那么就将会出现特别神奇的效果，你需要重复输入两个地址才能访问到相应的prometheus和alertmanager，这个参数是指的路径。所以一定要设置web.router-prefix,你也可以设置成--web.route-prefix=&quot;&quot;这样来将子路径就设置为根路径。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
        <tag>alertmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus 使用influxdb 做永久存储]]></title>
    <url>%2F2018%2F06%2F09%2Fprometheus-storage-influxdb%2F</url>
    <content type="text"><![CDATA[prometheus 使用远端存储使用Prometheus的过程中，我们可以发现Prometheus默认是自己带有存储的，不过保存的时间为15天。但是对于公司而言，可能有时候会对数据进行统计分析，那么15天的数据将不会满足要求了。所以我们希望能够将数据永久存储起来，或者说能够让我们自己将数据进行处理。 这里我们要讲的就是Prometheus的 remote_storage 功能。Prometheus的remote_storage 其实是一个adapter，至于在adapter的另一端是什么类型的时序数据库它根本不关心，如果你愿意，你也可以编写自己的adpater。我这里采用官网提供的influxdb作为远端存储的实例。 存储的方式为：Prometheus —-发送数据—- &gt; remote_storage_adapter —- 存储数据 —-&gt; influxdb。 下载安装influxdb我采用的docker安装influxdb，非常容易。下载influxdb的镜像，然后让它暴露出相应的端口： docker run -p 8086:8086 -v $PWD:/var/lib/influxdb --name influxdb influxdb 这样，一个influxdb就准备好了。influxdb的一些介绍或者操作可以查看官网。我们安装好influxdb之后需要在influxdb中创建一个prometheus的库：curl -XPOST http://localhost:8086/query --data-urlencode &quot;q=CREATE DATABASE prometheus&quot;。 准备remote_storage_adapter在github上准备一个remote_storage_adapter的可执行文件，然后启动它，如果想获取相应的帮助可以使用:./remote_storage_adapter -h来获取相应帮助(修改绑定的端口，influxdb的设置等..)，现在我们启动一个remote_storage_adapter来对接influxdb和prometheus：./remote_storage_adapter -influxdb-url=http://localhost:8086/ -influxdb.database=prometheus -influxdb.retention-policy=autogen，influxdb默认绑定的端口为9201 修改 prometheus.yml 配置对接adapter前面的准备操作完了之后，就可以对prometheus进行配置了。修改prometheus.yml文件，在文件末尾增加： 12345remote_write: - url: "http://localhost:9201/write"remote_read: - url: "http://localhost:9201/read" 之后我们启动prometheus就可以看到influxdb中会有相应的数据了。如果验证我们采集的metrics数据被存储起来了呢？我们选取一个metric，过几分钟然后将prometheus停止，并且将data目录删除，重启prometheus，然后我们再查询这个metric，可以看到之前几分钟的数据还在那里。 prometheus 高可用最近在实践一个事情，就是Prometheus的高可用，我的想法是将所有数据都存储到influxdb，但是influxdb集群版本竟然是闭源的。我擦。。。不过这个事情倒不是最重要，实在不行我们自己弄集群版本。我的高可用选择是有多个prometheus进行的是采集，在yaml种只配置remote_write，然后让某几台Prometheus机器做查询，只配置remote_read，并且查询的prometheus不做scape-config，也就是没有采集任务，完全的查询客户端，当然还有规则报警。在实际中要注意global.external_labels.monitor这个配置必须是 相同的值。不然的话查询客户端不能查询到相应的metric。当然你也可以不设置这个值。另外实践中我发现federate机器也就是prometheus的联邦机器，在另一个prometheus配置了一台federate之后，它会将从federate采集到的数据收集起来，并且存储到influxdb中，也就是相应的做了持久化存储。 目前还有两个问题没有解决：多个查询客户端prometheus如果都配置了报警rule的话，会不会产生单个报警重复报？另外还有就是influxdb的集群方案，该怎么操作？还要进一步研究才行啊。~~ 附录，influxdb的一些操作：influxdb常用操作： 显示数据库：show databases 创建一个库：create database database_name 删除一个库：drop database database_name 使用库：use database 表操作：show measurements 插入数据 insert &lt;tbname&gt;,&lt;tags&gt; &lt;values&gt; [timestamp] 说明： tbname : 数据表名称 tags : 表的tag域 values : 表的value域 timestamp ：当前数据的时间戳（可选，没有提供的话系统会自带添加） 示例如下： 12345678&gt; use testdb;Using database testdb&gt; insert students,stuid=s123 score=89&gt; show measurements;name: measurementsname----students]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS 服务器搭建]]></title>
    <url>%2F2018%2F06%2F09%2Fself-dns-server-install%2F</url>
    <content type="text"><![CDATA[搭建公司内部DNS服务器平常有很多时候可能会通过改hosts文件的方式来访问自定义的某个域名，但仅限于单机而已。如果能用dns解析的方式来统一管理那将会很方便。 搭建 DNS 服务器实验基于centos7实践。切换到root用户（以下所有操作都在root下执行）： 安装软件： 我们使用dns软件 bind9 系列安装包。 yum -y install bind* 安装后的部分日志: 123456789101112Installed: bind.x86_64 32:9.9.4-61.el7 bind-chroot.x86_64 32:9.9.4-61.el7 bind-devel.x86_64 32:9.9.4-61.el7 bind-dyndb-ldap.x86_64 0:11.1-4.el7 bind-libs.x86_64 32:9.9.4-61.el7 bind-lite-devel.x86_64 32:9.9.4-61.el7 bind-pkcs11.x86_64 32:9.9.4-61.el7 bind-pkcs11-devel.x86_64 32:9.9.4-61.el7 bind-pkcs11-libs.x86_64 32:9.9.4-61.el7 bind-pkcs11-utils.x86_64 32:9.9.4-61.el7 bind-sdb.x86_64 32:9.9.4-61.el7 bind-sdb-chroot.x86_64 32:9.9.4-61.el7 bind-utils.x86_64 32:9.9.4-61.el7 Dependency Installed: postgresql-libs.x86_64 0:9.2.23-3.el7_4 Updated: bind-libs-lite.x86_64 32:9.9.4-61.el7 bind-license.noarch 32:9.9.4-61.el7 备份主配置文件： cp /etc/named.conf /etc/name.conf.bak.$(date &#39;+%Y-%m-%d&#39;) 修改主配置文件：vi /etc/named.conf 修改两处为any： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 options &#123; listen-on port 53 &#123; any; &#125;; // 修改此处为any listen-on-v6 port 53 &#123; ::1; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; allow-query &#123; any; &#125;; //修改此处为any //forwarders &#123; //&#125; /* - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. - If your recursive DNS server has a public IP address, you MUST enable access control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification attacks. Implementing BCP38 within your network would greatly reduce such attack surface */ recursion yes; dnssec-enable yes; dnssec-validation no; //修改这里ping通外网 /* Path to ISC DLV key */ bindkeys-file &quot;/etc/named.iscdlv.key&quot;; managed-keys-directory &quot;/var/named/dynamic&quot;; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;&#125;;logging &#123; channel default_debug &#123; file &quot;data/named.run&quot;; severity dynamic; &#125;;&#125;;zone &quot;.&quot; IN &#123; type hint; file &quot;named.ca&quot;;&#125;;include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;; 自定义域名配置现在安装后基本的dns服务器之后，我们就开始自定义个域名来进行解析: vi named.rfc1912.zones 增加一个需要解析的主域名比如：cococzj.com;(公网肯定没有这个域名) 增加下面的文件到文件named.rfc1912.zones的最后1234zone &quot;cococzj.com&quot; IN &#123; type master; file &quot;cococzj.com.zone&quot;;&#125;; 增加域名配置上面定义了cococzj.com.zone的文件，现在增加配置：vi /var/named/cococzj.com.zone 增加下面的内容： 12345678910111213$TTL 86400@ IN SOA ns.cococzj.com. root ( 1 ; serial 1D ; refresh 1H ; retry 1W ; expire 0 ) ; minimum @ IN NS ns.cococzj.com.ns IN A 10.62.12.24www IN A 10.62.14.80ttt IN A 10.62.12.3 ns 对应的ip地址必须为dns服务器搭建的IP地址，也就是dns安装的机器的ip地址。 ns IN A 10.62.12.24 ==&gt; ns IN A your_dns_server_ip www IN A 10.62.14.80 代表将www.cococzj.com解析到10.62.14.80服务器上。 修改zone文件权限chown .named /var/named/cococzj.com.zone 检查配置文件named-checkconf 检查主配置文件是否配置正确，没有输出表明是正确的: named-checkzone 检查zone文件配置： 12345named-checkzone "cococzj.com" /var/named/cococzj.com.zoneresult:zone cococzj.com/IN: loaded serial 1OK 重启服务可以先ping一下服务域名再重启：systemctl restart named.service 测试： 先ping试一下： 12[root@k8s-dev-yw-1 etc]# ping www.cococzj.comping: www.cococzj.com: Name or service not known 增加dns服务地址；vi /etc/resolv.conf，在文件中新建： nameserver 10.62.12.2 再一次ping： 12345678910[root@k8s-dev-yw-1 etc]# vi /etc/resolv.conf[root@k8s-dev-yw-1 etc]# ping www.cococzj.comPING www.cococzj.com (10.62.14.80) 56(84) bytes of data.64 bytes from 10.62.14.80 (10.62.14.80): icmp_seq=1 ttl=63 time=0.501 ms64 bytes from 10.62.14.80 (10.62.14.80): icmp_seq=2 ttl=63 time=0.466 ms64 bytes from 10.62.14.80 (10.62.14.80): icmp_seq=3 ttl=63 time=0.509 ms^C--- www.cococzj.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2001msrtt min/avg/max/mdev = 0.466/0.492/0.509/0.018 ms 新增一个域名 vi named.rfc1912.zones在文件中新增： 1234zone &quot;your_website.com&quot; IN &#123; type master; file &quot;your_website.com.zone&quot;;&#125;; 复制一份之前配置好的配置cp /var/named/cococzj.com.zone /var/named/your_website.com.zone 编辑配置文件 vi /var/named/your_website.com.zone将cococzj改为your_website，然后修改相应的ip和 检查配置文件： 12345[root@k8s-dev-yw-1 etc]# named-checkzone &quot;cmrrancher.com&quot; /var/named/cmrhrancher.com.zonezone cmrrancher.com/IN: loaded serial 1OK[root@k8s-dev-yw-1 etc]# named-checkconf[root@k8s-dev-yw-1 etc]# 重载服务systemctl reload named.service 测试：ping www.cmrhrancher.com 附录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153[root@qianyi-monitor-3 etc]# yum install -y bind*Loaded plugins: fastestmirrorhttp://10.0.0.5/centos/7/cloud/x86_64/openstack-newton/repodata/repomd.xml: [Errno 14] HTTP Error 404 - Not FoundTrying other mirror.To address this issue please refer to the below knowledge base article https://access.redhat.com/articles/1320623If above article doesn&apos;t help to resolve this issue please create a bug on https://bugs.centos.org/http://10.0.0.5/centos/7/docker/x86_64/repodata/repomd.xml: [Errno 14] HTTP Error 404 - Not FoundTrying other mirror.dockerrepo | 2.9 kB 00:00:00 http://10.0.0.5/centos/7/latest/x86_64/repodata/repomd.xml: [Errno 14] HTTP Error 404 - Not FoundTrying other mirror.Loading mirror speeds from cached hostfileResolving Dependencies--&gt; Running transaction check---&gt; Package bind.x86_64 32:9.9.4-61.el7 will be installed--&gt; Processing Dependency: libGeoIP.so.1()(64bit) for package: 32:bind-9.9.4-61.el7.x86_64---&gt; Package bind-chroot.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-devel.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-dyndb-ldap.x86_64 0:11.1-4.el7 will be installed---&gt; Package bind-libs.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-libs-lite.x86_64 32:9.9.4-29.el7 will be updated---&gt; Package bind-libs-lite.x86_64 32:9.9.4-61.el7 will be an update---&gt; Package bind-license.noarch 32:9.9.4-29.el7 will be updated---&gt; Package bind-license.noarch 32:9.9.4-61.el7 will be an update---&gt; Package bind-lite-devel.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-pkcs11.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-pkcs11-devel.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-pkcs11-libs.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-pkcs11-utils.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-sdb.x86_64 32:9.9.4-61.el7 will be installed--&gt; Processing Dependency: libpq.so.5()(64bit) for package: 32:bind-sdb-9.9.4-61.el7.x86_64---&gt; Package bind-sdb-chroot.x86_64 32:9.9.4-61.el7 will be installed---&gt; Package bind-utils.x86_64 32:9.9.4-61.el7 will be installed--&gt; Running transaction check---&gt; Package GeoIP.x86_64 0:1.5.0-11.el7 will be installed---&gt; Package postgresql-libs.x86_64 0:9.2.23-3.el7_4 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved=========================================================================================================================================================================== Package Arch Version Repository Size===========================================================================================================================================================================Installing: bind x86_64 32:9.9.4-61.el7 base 1.8 M bind-chroot x86_64 32:9.9.4-61.el7 base 87 k bind-devel x86_64 32:9.9.4-61.el7 base 399 k bind-dyndb-ldap x86_64 11.1-4.el7 base 122 k bind-libs x86_64 32:9.9.4-61.el7 base 1.0 M bind-lite-devel x86_64 32:9.9.4-61.el7 base 308 k bind-pkcs11 x86_64 32:9.9.4-61.el7 base 298 k bind-pkcs11-devel x86_64 32:9.9.4-61.el7 base 105 k bind-pkcs11-libs x86_64 32:9.9.4-61.el7 base 1.1 M bind-pkcs11-utils x86_64 32:9.9.4-61.el7 base 198 k bind-sdb x86_64 32:9.9.4-61.el7 base 353 k bind-sdb-chroot x86_64 32:9.9.4-61.el7 base 87 k bind-utils x86_64 32:9.9.4-61.el7 base 204 kUpdating: bind-libs-lite x86_64 32:9.9.4-61.el7 base 734 k bind-license noarch 32:9.9.4-61.el7 base 85 kInstalling for dependencies: GeoIP x86_64 1.5.0-11.el7 base 1.1 M postgresql-libs x86_64 9.2.23-3.el7_4 base 234 kTransaction Summary===========================================================================================================================================================================Install 13 Packages (+2 Dependent packages)Upgrade 2 PackagesTotal download size: 8.1 MDownloading packages:Delta RPMs disabled because /usr/bin/applydeltarpm not installed.(1/17): bind-9.9.4-61.el7.x86_64.rpm | 1.8 MB 00:00:00 (2/17): GeoIP-1.5.0-11.el7.x86_64.rpm | 1.1 MB 00:00:00 (3/17): bind-chroot-9.9.4-61.el7.x86_64.rpm | 87 kB 00:00:00 (4/17): bind-devel-9.9.4-61.el7.x86_64.rpm | 399 kB 00:00:00 (5/17): bind-dyndb-ldap-11.1-4.el7.x86_64.rpm | 122 kB 00:00:00 (6/17): bind-libs-lite-9.9.4-61.el7.x86_64.rpm | 734 kB 00:00:00 (7/17): bind-libs-9.9.4-61.el7.x86_64.rpm | 1.0 MB 00:00:00 (8/17): bind-license-9.9.4-61.el7.noarch.rpm | 85 kB 00:00:00 (9/17): bind-lite-devel-9.9.4-61.el7.x86_64.rpm | 308 kB 00:00:00 (10/17): bind-pkcs11-9.9.4-61.el7.x86_64.rpm | 298 kB 00:00:00 (11/17): bind-pkcs11-devel-9.9.4-61.el7.x86_64.rpm | 105 kB 00:00:00 (12/17): bind-pkcs11-utils-9.9.4-61.el7.x86_64.rpm | 198 kB 00:00:00 (13/17): bind-pkcs11-libs-9.9.4-61.el7.x86_64.rpm | 1.1 MB 00:00:00 (14/17): bind-sdb-9.9.4-61.el7.x86_64.rpm | 353 kB 00:00:00 (15/17): bind-sdb-chroot-9.9.4-61.el7.x86_64.rpm | 87 kB 00:00:00 (16/17): bind-utils-9.9.4-61.el7.x86_64.rpm | 204 kB 00:00:00 (17/17): postgresql-libs-9.2.23-3.el7_4.x86_64.rpm | 234 kB 00:00:00 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 12 MB/s | 8.1 MB 00:00:00 Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : GeoIP-1.5.0-11.el7.x86_64 1/19 Updating : 32:bind-license-9.9.4-61.el7.noarch 2/19 Installing : 32:bind-libs-9.9.4-61.el7.x86_64 3/19 Installing : 32:bind-9.9.4-61.el7.x86_64 4/19 Installing : 32:bind-pkcs11-libs-9.9.4-61.el7.x86_64 5/19 Updating : 32:bind-libs-lite-9.9.4-61.el7.x86_64 6/19 Installing : postgresql-libs-9.2.23-3.el7_4.x86_64 7/19 Installing : 32:bind-sdb-9.9.4-61.el7.x86_64 8/19 Installing : 32:bind-sdb-chroot-9.9.4-61.el7.x86_64 9/19 Installing : 32:bind-lite-devel-9.9.4-61.el7.x86_64 10/19 Installing : 32:bind-pkcs11-devel-9.9.4-61.el7.x86_64 11/19 Installing : 32:bind-pkcs11-9.9.4-61.el7.x86_64 12/19 Installing : 32:bind-pkcs11-utils-9.9.4-61.el7.x86_64 13/19 Installing : 32:bind-chroot-9.9.4-61.el7.x86_64 14/19 Installing : bind-dyndb-ldap-11.1-4.el7.x86_64 15/19 Enabling SELinux boolean named_write_master_zonessetsebool: SELinux is disabled. Installing : 32:bind-devel-9.9.4-61.el7.x86_64 16/19 Installing : 32:bind-utils-9.9.4-61.el7.x86_64 17/19 Cleanup : 32:bind-libs-lite-9.9.4-29.el7.x86_64 18/19 Cleanup : 32:bind-license-9.9.4-29.el7.noarch 19/19 Verifying : 32:bind-pkcs11-devel-9.9.4-61.el7.x86_64 1/19 Verifying : 32:bind-pkcs11-9.9.4-61.el7.x86_64 2/19 Verifying : 32:bind-chroot-9.9.4-61.el7.x86_64 3/19 Verifying : 32:bind-sdb-9.9.4-61.el7.x86_64 4/19 Verifying : 32:bind-9.9.4-61.el7.x86_64 5/19 Verifying : 32:bind-devel-9.9.4-61.el7.x86_64 6/19 Verifying : GeoIP-1.5.0-11.el7.x86_64 7/19 Verifying : 32:bind-pkcs11-libs-9.9.4-61.el7.x86_64 8/19 Verifying : 32:bind-lite-devel-9.9.4-61.el7.x86_64 9/19 Verifying : 32:bind-libs-9.9.4-61.el7.x86_64 10/19 Verifying : 32:bind-pkcs11-utils-9.9.4-61.el7.x86_64 11/19 Verifying : 32:bind-libs-lite-9.9.4-61.el7.x86_64 12/19 Verifying : 32:bind-utils-9.9.4-61.el7.x86_64 13/19 Verifying : bind-dyndb-ldap-11.1-4.el7.x86_64 14/19 Verifying : 32:bind-license-9.9.4-61.el7.noarch 15/19 Verifying : postgresql-libs-9.2.23-3.el7_4.x86_64 16/19 Verifying : 32:bind-sdb-chroot-9.9.4-61.el7.x86_64 17/19 Verifying : 32:bind-libs-lite-9.9.4-29.el7.x86_64 18/19 Verifying : 32:bind-license-9.9.4-29.el7.noarch 19/19 Installed: bind.x86_64 32:9.9.4-61.el7 bind-chroot.x86_64 32:9.9.4-61.el7 bind-devel.x86_64 32:9.9.4-61.el7 bind-dyndb-ldap.x86_64 0:11.1-4.el7 bind-libs.x86_64 32:9.9.4-61.el7 bind-lite-devel.x86_64 32:9.9.4-61.el7 bind-pkcs11.x86_64 32:9.9.4-61.el7 bind-pkcs11-devel.x86_64 32:9.9.4-61.el7 bind-pkcs11-libs.x86_64 32:9.9.4-61.el7 bind-pkcs11-utils.x86_64 32:9.9.4-61.el7 bind-sdb.x86_64 32:9.9.4-61.el7 bind-sdb-chroot.x86_64 32:9.9.4-61.el7 bind-utils.x86_64 32:9.9.4-61.el7 Dependency Installed: GeoIP.x86_64 0:1.5.0-11.el7 postgresql-libs.x86_64 0:9.2.23-3.el7_4 Updated: bind-libs-lite.x86_64 32:9.9.4-61.el7 bind-license.noarch 32:9.9.4-61.el7 Complete! 两个重要的配置文件： /etc/named.rfc1912.zones ： 定义域 /var/named ：域的详细配置 之后记得要将文件的权限复制给named这个用户和组。 上面的配置可以连接到本地的定义的域名，但是无法ping通百度等。所以一般需要再配置文件中增加forwarders，当本地没有找到定义的域之后会在forwarders中定义的继续查找。打开forwarders之后记得修改dnssec-validation no;这样之后再ping 外网的域名就可以了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[最近遇到的小问题总结-2018522]]></title>
    <url>%2F2018%2F05%2F22%2Fproblem-cmrh-2018522%2F</url>
    <content type="text"><![CDATA[最近遇到的小问题总结-2018522 Excel将两列合并成一列：=CONCATENATE() linux统计文件,原文链接: 1234567891011121314统计某文件夹下文件的个数ls -l |grep "^-"|wc -l统计某文件夹下目录的个数ls -l |grep "^ｄ"|wc -l统计文件夹下文件的个数，包括子文件夹里的ls -lR|grep "^-"|wc -l如统计/home/han目录(包含子目录)下的所有js文件则：ls -lR /home/han|grep js|wc -l 或 ls -l "/home/han"|grep "js"|wc -l统计文件夹下目录的个数，包括子文件夹里的ls -lR|grep "^d"|wc -l linux sort的一些用法： 1234567891011去除重复行sort file |uniq查找非重复行sort file |uniq -u查找重复行sort file |uniq -d统计sort file | uniq -c 什么是linux vip：https://www.zhihu.com/question/67682565，https://www.novell.com/documentation/bcc/bcc11_admin_nw/data/bq7ucwl.html。 Linux scp命令，上传或下载文件 复制本地文件到远程服务器：scp local_file remote_username@remote_ip:remote_folder；例如：scp /home/chen/file.md chen@192.168.1.1:/home/chen/test.md 复制远程文件到本地：scp remote_username@remote_ip:remote_folder local_file例如：scp chen@192.168.1.1:/home/chen/test.md /home/chen/file.md 修改文件所属用户和组信息 chown username:usergroup /path/to/file 修改文件权限: ugo,auser,group,other,all chmod a+x /path/to/file chmod u+w /path/to/file linux 查看ip： 123ip addrifconfighostname -I 解压缩文件： 123解压文件：tar -zvxf xxx.tar.gz压缩文件： tar -zvcf xxx.tar.gz /path/to/file/tozip vim 一些操作 1234567vim 全文件内替换 ： :%s/old/new/g当前行替换：:s/old/new/gvim 打开多个文件： split/vsplit 文件切换ctrl w w， 获取CTRL w (上下左右或者jkhl)，CTRL F6 6 docker 删除容器状态为exited的容器： docker rm -v $(docker ps -aq -f status=exited) 删除所有none的tag的镜像 docker images |grep none |awk &#39;{print $3}&#39;|xargs -i docker rmi {} idea重置配置文件。 D:\Users\chenzj001.IntelliJIdea2018.1 删除这个文件就好了，全部重置。也可以只删除config。windows找到用户–&gt;.i 也就是在用户目录下,删除.IntelliJIdea2018 windows 查找端口占用进程 123netstat -aon|findstr &quot;端口号&quot;tasklist|findstr “端口号”]]></content>
      <categories>
        <category>problem</category>
      </categories>
      <tags>
        <tag>problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 脚本学习]]></title>
    <url>%2F2018%2F05%2F13%2Fshell-script%2F</url>
    <content type="text"><![CDATA[shell 脚本学习shell 脚本在linux上可以说是非常有用的一个工具，它就是linux命令的一个集合，所以写好shell脚本的关键一部分就是对linux命令比较熟悉。 本博客是在图书馆借阅shell相关脚本学习书记做的一个简单笔记。 shell 脚本的创建与执行一般我们在linux中创建的shell脚本都是以.sh为结尾的，这其实不是说一定要用sh结尾才行，只是大家约定习俗的一个习惯而已。在shell文件的第一行，通常是#!/bin/bash，表示该脚本使用bash语法。#是shell脚本中的注释。通常自定义的脚本我通常习惯放在/usr/local/bin目录下,关于bin,sbin,/usr/bin,/usr/local/bin等的区别 现在我们创建第一个脚本first.sh: 12#!/bin/bashecho "Hello,world" 脚本创建好之后有两种运行方式: 使用bash执行：bash first.sh 直接执行：先让first.sh变成可执行脚本chmod +x first.sh,然后再直接运行./first.sh 脚本运行过程中可以使用bash -x first.sh，这样可以看到脚本的执行过程。 shell 脚本的变量脚本预定义变量shell 预先设置的几个变量值，比如optin.sh： 12#!/bin/bashecho $0 $1 $2 如果执行：./option.sh 1 2,输出为：option.sh 1 2 也就是执行脚本的时候后面的参数有几个，在脚本中直接使用$位置就可以使用这些变量值，0位置是脚本的名称。 自定义变量在脚本中自己定义的变量，变量名=变量值，在脚本中使用的方式为$变量名。 123#!/bin/bashnum=1echo "$num" num=1 中num和=之间不要有空格,不要有空格，不要有空格。 控制台变量值传递控制台交互性，读取控制台输入的数，然后传递给脚本中，如： 12345678910111213#!/bin/bashread -p "Please input a number: " xread -p "Please input another number: " ysum=$[$x+$y]echo "the sum of the two numbers is : $sum"##chen@ubuntu:~/shell-learn$ bash read.sh #Please input a number: 1#Please input another number: 3#the sum of the two numbers is : 4 数值运算简单的加减乘除的运算，使用[]前面需要加上$，如： 12345678910111213141516171819#!/bin/bash#!/bin/basha=1b=2sum=$[$a+$b]echo "$a+$b=$sum"echo ================sum2=$[$a+$b]echo "$a+$b=$sum2"## sh sum.sh 1+2=$[1+2]## bash sum.sh 1+2=3## ./sum.sh 1+2=3## 这三种方式执行结果不一样 if-else 条件逻辑语句对于if-else大家在任何一门语言中都是必备的，shell中的if-else格式如下： 常用的数值判断if-else常用的根据数字来判断的相关实例： 1234567891011121314151617181920# 单独if语句if 判断语句; then commandfi# if-else 语句if 判断语句; then commandelse commandfi#if-elseif-else 语句if 判断语句; then commandelif 判断语句2; then commandelse commandfi 如： 12345678910111213141516171819#!/bin/bashnum=$1if ((num&lt;60));then echo "you don't pass the exam; your score is $num"elif ((num&gt;=60&amp;&amp;num&lt;80));then echo "good job, your score is $num"else echo "great job, your score is $num"fiif ((num==60));then echo "ok"fiif ((num!=50));then echo "not equal 50"fi 执行的时候：./if.sh 89。 数值大小的判断除了有(())之外，还可以使用[]，如果使用[]，那么就不能使用&gt; , &lt; , =这些符号，要使用:小于：-lt，大于：-gt，小于或等于：-le，大于或等于：-ge，等于：-eq，不等于：-ne，如： 1234567#!/bin/basha=10if [ $a -lt 5 ];then echo ok;fiif [ $a -gt 5 ];then echo gt;fiif [ $a -ge 5 ];then echo ge;fiif [ $a -eq 5 ];then echo eq;fiif [ $a -ne 5 ];then echo ne;fi if中常用的跟文档相关的判断if 语句中可以使用一些内置的跟文件判断相关的参数。 12345678910111213141516#!/bin/bash# if判断跟文件相关的一些参数# -e 判断文件或目录是否存在# -d 判断是不是目录，以及是否存在# -f 判断是不是普通文件，以及是否存在# -r 判断是否有读权限# -w 判断是否有写权限# -x 判断是否可执行if [ -e /home/ ]; then echo ok ;fiif [ -f /home/ ]; then echo ok ;fiif [ -f /home/chen/shell-learn/if1.sh ]; then echo ok ;fi case 逻辑判断java中有if之外还有switch，shell中除了if之外还有case,如果总结一下啊，可以发现if中的结束用fi，case中结束用esac，刚好是倒过来的。 123456789101112131415# case语句，value可以任意个，*指代其它值。类似java switch default case 变量 in value1) command ;; value2) command ;; value3) command ;; *) command ;; esac 实例脚本，判断奇偶数字： 123456789101112131415#!/bin/bashnum=$1n=$[$num%2]case $n in1) echo "奇数" ;;0) echo "偶数" ;;*) echo "其它的值" ;;esac 之前跟同事学的一个很有用的基础脚本：地址 shell中的循环：for，while常用到的循环就是for和while了， for的使用方式： 1234567891011121314#!/bin/bash# 脚本循环#for 变量名 in 循环的条件; do# command#donefor i in 1 2 3 4 5 6; do echo $idonefor filename in `ls`; do echo $filenamedone while 的使用方式： 123456789101112#!/bin/bash# while 条件; do# command# donenum=5 while [ $num -ge 1 ]; do echo $num num=$[ $num-1 ]done while 中如果是while :; do那么就是死循环了，有些地方可以这样使用。 shell 函数shell的函数一定要在被调用之前声明，生命函数的关键字是function,函数的参数个数需要在调用的时候指定，如func.sh： 1234567891011121314#!/bin/bashfunction sum()&#123; sum=$[$1+$2] echo $sum echo "第三个变量：" $3&#125;sum $1 $2echo "======================="sum $1 $2 $3 调用的方式./func.sh 1 2 3 输出结果为： 可以看到第一个函数里面的$1 $2 $3跟shell脚本预设的变量是不相关的，函数里面能用的参数都是sum传递进去的。 ps: 如果每次写完脚本都需要使用chmod +x xxx.sh这样也挺烦的，不如写个脚本吧，x.sh。 1chmod +x *.sh 这样就可以使用./x.sh，直接执行脚本就好了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础脚本]]></title>
    <url>%2F2018%2F05%2F10%2Fa-easy-script%2F</url>
    <content type="text"><![CDATA[基础脚本最近在同事那里学到一个小脚本，感觉要是之前我也会这样写，那我省去多少时间啊，技多不压身啊。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/bin/bashecho "ok"buildProcess()&#123; echo "build image"; docker build -t promethues-agent:v0.1.test . echo "start contianer.."; docker run -dit -P --name pro-agent-test promethues-agent:v0.1.test --spring.profiles.active=test echo "finished";&#125;privilege()&#123; if [ "$(whoami)" != "root" ] then echo "should be root to execute this script"; exit 1 fi&#125;stopProecess()&#123; echo "delete container.."; docker rm -f pro-agent-test echo "delete image..."; docker rmi promethues-agent:v0.1.test echo "finished";&#125;showUsage()&#123; cat &lt;&lt;ENDUsage: $0 &lt;build|stop&gt;END&#125;main()&#123; command=$1 # check privilege# privilege case $command in "build") buildProcess;; "stop") stopProecess;; *) showUsage;; esac&#125;main $@ 上面是一个docker镜像的制作脚本，命名为dockerimg.sh。使用方式： 123456./dockerimg.sh build./dockerimg.sh stop./dockerimg.sh *]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[promethues 联邦集群]]></title>
    <url>%2F2018%2F05%2F10%2Fpromethues-federate%2F</url>
    <content type="text"><![CDATA[Prometheus 联邦集群Prometheus的联邦集群我们使用它来作为Prometheus代理。因为我们是在监控rancher平台里面的docker容器里面的应用，那么拿到的就是容器的ip，而我们实际的Prometheus是部署在外部虚拟机上面的。这个时候外部的Prometheus就无法拿到rancher平台内部容器应用的metrics，所以部署一台prometheus到rancher组成联邦机，详细的官网有解释:federate，总体架构图如下 外部连接方式： 12345678910111213141516171819202122232425scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. # - job_name: 'prometheus' # # metrics_path defaults to '/metrics' # # scheme defaults to 'http'. # static_configs: # - targets: ['localhost:9091'] - job_name: 'federate' scrape_interval: 15s # honor_labels: true metrics_path: '/federate' params: 'match[]': - '&#123;job=~"prometheus"&#125;' #- '&#123;job="*"&#125;' # - '&#123;job="prometheus"&#125;' # - '&#123;__name__=~"job:.*"&#125;' # - '&#123;job="rancher_network_monitor"&#125;' - '&#123;job="targets-server"&#125;' static_configs: - targets: - 'localhost:9090' - '10.62.14.129:9090' # - '10.62.12.3:9090' # - '10.0.11.23:80' 额外注意就是这里的job_name下有一个match，这个好像必须要填写，嗯就是这样，只要在内部的Prometheus代理将job那么定义好，在外部再像上面的配置文件一个配置，就能在外部访问内部的prometheus的数据，而且可以保存这些数据在外部的Prometheus。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus 监控 Java 应用]]></title>
    <url>%2F2018%2F05%2F09%2Fprometheus-monitor-tomcat%2F</url>
    <content type="text"><![CDATA[Prometheus 监控 Java 应用Prometheus 监控 Java 应用有两种方式：一种是使用官方提供的jar包，然后嵌入到应用中就可以了。这种方式一般都是新项目。我认为也是最合适的一种。不过这种情况一般是理想而已。而除了这种方式，第二种是prometheus的jmx_exporter。 今天我们讨论的就是第二种。使用jmx_exporter的方式来监控我们的java应用程序。我们的java应用基本上是使用tomcat作为服务器的。这种情况下有两种方式，一种是基于springboot的jar包启动方式，一种是直接下载tomcat软件之后，将应用打成war包部署的方式。jmx_exporter的使用非常简单，但是如果不了解就会非常懵逼。jmx_exporter实际也是基于java的jmx通过暴露Mbean来做为代理，使用http的方式来给Prometheus进行指标采集。 jar 包启动应用如果是jar包启动的方式，那么github上面就已经有示例了。可以参照：java -javaagent:./jmx_prometheus_javaagent-0.3.0.jar=9151:config.yaml -jar yourJar.jar，这种方式启动。这种属于在应用启动的时候就给它加上代理。 这种方式是没有加认证的，如果需要加认证，嗯，有点麻烦，实验过一次，后来发现，还是算了。可能在虚拟机上直接运行程序还好，但是打成docker镜像就真的就有点多余了。 这种方式是监控的内嵌tomcat的启动的应用，在访问http://ip:9151/metrics,/metrics可有可无，这个时候可以看到很多tomcat指标，当然如果你的config.yaml没有改动，那么可能并不会看到，因为官网的config.yaml中rules下的pattern:Catalina*，这里是不适用与内嵌tomcat的。内嵌的tomcat需要修改为Tomcat。 12345678910111213141516171819202122232425262728293031323334353637---startDelaySeconds: 0#hostPort: 192.168.226.128:8999#jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:8999/jmxrmissl: falsewercaseOutputName: truelowercaseOutputLabelNames: truerules:- pattern: 'Tomcat&lt;type=GlobalRequestProcessor, name=\"(\w+-\w+)-(\d+)\"&gt;&lt;&gt;(\w+):' name: tomcat_$3_total labels: port: "$2" protocol: "$1" help: Tomcat global $3 type: COUNTER- pattern: 'Tomcat&lt;j2eeType=Servlet, WebModule=//([-a-zA-Z0-9+&amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+&amp;@#/%=~_|]), name=([-a-zA-Z0-9+/$%~_-|!.]*), J2EEApplication=none, J2EEServer=none&gt;&lt;&gt;(requestCount|maxTime|processingTime|errorCount):' name: tomcat_servlet_$3_total labels: module: "$1" servlet: "$2" help: Tomcat servlet $3 total type: COUNTER- pattern: 'Tomcat&lt;type=ThreadPool, name="(\w+-\w+)-(\d+)"&gt;&lt;&gt;(currentThreadCount|currentThreadsBusy|keepAliveCount|pollerThreadCount|connectionCount):' name: tomcat_threadpool_$3 labels: port: "$2" protocol: "$1" help: Tomcat threadpool $3 type: GAUGE- pattern: 'Tomcat&lt;type=Manager, host=([-a-zA-Z0-9+&amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+&amp;@#/%=~_|]), context=([-a-zA-Z0-9+/$%~_-|!.]*)&gt;&lt;&gt;(processingTime|sessionCounter|rejectedSessions|expiredSessions):' name: tomcat_session_$3_total labels: context: "$2" host: "$1" help: Tomcat session $3 total type: COUNTER- pattern: ".*" 当然，你也看到了我的pattern这里有个：&quot;.*&quot;为啥这样？因为这样可以让所有的jmx metrics全部暴露出来，这方式有点暴力但是很好，很有用。 Tomcat war包应用war包应用部署就不说了，在bin目录启动./startup.sh就好了。但是如果要加上jmx_exporter。那么我们需要加上要给东西，进入bin目录（$TOMCAT_HOME/bin），将jmx_exporter.jar包文件和config.yaml文件复制到这里。然后修改里面的一个catalina.sh的脚本，找到JAVA_OPTS,加上代理： 123JAVA_OPTS="$JAVA_OPTS $JSSE_OPTS"JAVA_OPTS="$JAVA_OPTS -javaagent:$PWD/jmx_prometheus_javaagent-0.3.0.jar=9151:$PWD/config.yaml" 这种启动tomcat之后，你就可以通过访问9151端口来访问metrics了。这里要记得，修改上面的config.yaml中pattern的部分，改为和github上一样就可以了，也就是pattern为Catalina**。 监控远程的 tomcat如果你不修改catalina.sh，可以采用在bin目录新建一个setenv.sh文件，修改为可执行文件。然后加入下面的设置：1234#CATALINA_OPTS='-javaagent:/home/chen/jmx_prometheus_javaagent-0.3.0.jar=9151:/home/chen/config.yaml'CATALINA_OPTS='-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false'#JAVA_OPTS='-javaagent:/home/chen/jmx_prometheus_javaagent-0.10.jar=9151:/home/chen/config.yaml'#CATALINA_OPTS='-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=/home/chen/jmxremote.password' 这样在启动的时候tomcat就会暴露出8999端口并且打开metrics。上面是jmx不带认证的。 然后在另一端下载jmx_exporter，这里需要去下载0.10版本，在github 的release中找一下就好了。如果是远程的tomcat jmx，最新版本的代理反正是用不了，我的测试中0.3.0是用不了的，0.10是可以用的。 1java -javaagent:./jmx_prometheus_javaagent-0.10.jar=9150:config.yaml 相应的config.yaml需要做一些修改： 12345678910111213141516---startDelaySeconds: 0hostPort: 192.168.226.128:8999jmxUrl: service:jmx:rmi:///jndi/rmi://192.168.226.128:8999/jmxrmissl: falsewercaseOutputName: truelowercaseOutputLabelNames: truerules:- pattern: 'Tomcat&lt;type=GlobalRequestProcessor, name=\"(\w+-\w+)-(\d+)\"&gt;&lt;&gt;(\w+):' name: tomcat_$3_total labels: port: "$2" protocol: "$1" help: Tomcat global $3 type: COUNTER- pattern: ".*" 当然如果是springboot，现在有spring-boot-starter-actuator的包，是可以直接暴露metrics的，当然也可以引入prometheus提供的client jar。 实际中，还是的多动手，实践出真知。]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 设置prometheus和grafana的反向代理]]></title>
    <url>%2F2018%2F05%2F02%2Fnginx-proxy-promethues-grafana%2F</url>
    <content type="text"><![CDATA[nginx 设置promethues和grafana的反向代理在配置完promethues，和grafana之后，可能需要上生产环境，这个时候如果有下面两种情况，那么就可能需要用到代理； 端口只开发80,或者8080等特别的几个端口，端口数量有限； 不希望暴露给外部端口号，使用子路经来区分；eg: http://{ip}/prometheus,http://{ip}/grafana nginx 配置，监听server的端口为80，然后通过子路径在内部反向代理出去： 123456789101112131415161718192021222324server &#123; listen 127.0.0.1:80; #server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; # proxy_pass http://127.0.0.1:9090/; &#125; location /grafana/ &#123; proxy_pass http://127.0.0.1:3000/; &#125; location /promethues/ &#123; proxy_pass http://127.0.0.1:9090/prometheus/; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; alertmanager很大的情况等同与prometheus，可以等同配置。 设置promethues的代理，子路径接下来需要的是将prometheus和grafana在启动或者配置文件中做一些更改，prometheus的相对来说比较简单，主要实在启动的时候根据命令行的参数来进行子路径设置。 在启动的时候设置web.external-url使用下面的命令： 1./prometheus --web.external-url=promethues 结果如图： 还可以使用./promethues --help获取更多的命令行参数，alertmanager同样也适用。 设置grafana的代理，子路径grafana的代理需要在default.ini中配置root_url:root_url = %(protocol)s://%(domain)s:/grafana 之后再重启就可以了。记住，再nginx中，proxy_pass 不要带上后缀。添加反向代理后，如果访问使用http://localhost:3000/grafana或者http://localhost:3000页面会显示不全。但是使用nginx代理后的路径：http://localhost/grafana就可以看到全页面了。 参考资料Running Grafana behind a reverse proxy]]></content>
      <categories>
        <category>monitor</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>prometheus</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu 怎么修改源？]]></title>
    <url>%2F2018%2F05%2F02%2Fubuntu-change-source-list%2F</url>
    <content type="text"><![CDATA[ubuntu 怎么修改源修改ubuntu的源为国内的源，修改之前首先备份： 1cp /etc/apt/sources.list /etc/apt/sources.list.backup 修改源列表，选择的是国内的阿里源,vi /etc/apt/sources.list,清空里面内容：dG,使用命令的时候要回到第一行：gg,之后将下面的内容复制到文件中： 12345678910111213141516171819deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb [arch=amd64] https://download.daocloud.io/docker/linux/ubuntu xenial stable# deb-src [arch=amd64] https://download.daocloud.io/docker/linux/ubuntu xenial stabledeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus+Grafana 搭建监控系统]]></title>
    <url>%2F2018%2F04%2F26%2Fprometheus-grafana-monitor%2F</url>
    <content type="text"><![CDATA[Prometheus+Grafana 搭建监控系统今天将第一版监控系统上线，过程整个就是一路坎坷。不过踩坑，填坑，确实也是为自己积攒了一些小经验。 prometheus的服务发现Prometheus的监控使用的是pull的模式，也就是每隔几秒钟去各个target采集一次metric。那么如果是多个target，如果是静态配置的话，那么就得在配置文件里面一个一个添加，尽管可以使用接口去更新配置文件，但如果服务太多，那工作量也很大。而且如果遇到微服务的情况并且容器化部署，那么可能ip地址都是随机改变的，那么就将更麻烦了。所以就有服务发现的模式出来了，有很多种实现的方式，consul，dns等等，针对我们现有的平台，我们选择了file_sd_config: 1234567891011121314151617181920scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'rancher_network_monitor' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 1m static_configs: - targets: ['192.168.7.3:8080','192.168.8.90:8080'] labels: group: 'rancher_network_monitor' metrics_path: /metrics - job_name: 'filediscovery' scrape_interval: 5s file_sd_configs: - files: ['/home/config/*.json'] 基于文件的方式，只需要在/home/config目录下增加json文件就可以了，这也是在网上找的一个方案，所以我也记录下来，万一也能帮助到别人了 12345678910111213141516171819202122[ &#123; "targets": [ "10.10.10.1:65160", "10.10.10.2:65160" ], "labels": &#123; "job":"Center", "service":"qtest" &#125; &#125;, &#123; "targets": [ "10.10.10.3:65110", "10.10.10.4:65110" ], "labels": &#123; "job":"Gateway", "service":"qtest" &#125; &#125;] 监控报警规则 rule_files:12rule_files: - './alert.yml' 规则可以在这里配置多个，这里其实也是支持通配符的。也就是： 12rule_files: - '/path/to/rule/*.yml' 我使用的是yml格式，如果你会的话，也可以是用.rule; 123456789101112131415161718192021groups:- name: goroutines_monitor rules: - alert: go_goroutines_test expr: go_goroutines &gt; 200 for: 5s labels: name: goroutines_test severity: warning annotations: summary: "&#123;&#123; $labels.instance &#125;&#125;，机器goroutines过高。" - name: instance_monitor rules: - alert: instance_down expr: up == 0 for: 1m labels: name: instance severity: critical annotations: summary: "&#123;&#123; $labels.instance &#125;&#125; 挂掉了,&#123;&#123; $labels.job&#125;&#125;" 上面的yaml就是定义了两个规则，go_goroutines&gt;200以及up==0一个是go的协程数，一个是被监控目标是否可以正常采集。在则合理推荐使用labels，这样如果是使用alertmanager做预警，那么可以使用这些label。for的意思指接到一个触发报警rule的收将状态变成active，然后如果在for之内的评估期内如果没有报警就变成pending状态，如果有就变成firing，发送报警。 alertmanager发送报警邮件alertmanager相当于通知中心，它只会在Prometheus发送报警后，通过某些渠道（邮件，即时通讯等）发送通知。当然它在通知这方面做了相当多的事情。 alertmanager 通知分组如果是一个频繁的bug，引起Prometheus一直触发警报，如果不做限制，那么就会引起邮箱轰炸。alertmanager中的route就是做这个的。通过路由器将不同的邮件发送给不同的人。 12345678910111213141516171819202122232425262728293031route: # How long to initially wait to send a notification for a group # of alerts. Allows to wait for an inhibiting alert to arrive or collect # more initial alerts for the same group. (Usually ~0s to few minutes.) # group_wait: 30s # How long to wait before sending a notification about new alerts that # are added to a group of alerts for which an initial notification has # already been sent. (Usually ~5m or more.) group_interval: 30s # If an alert has successfully been sent, wait 'repeat_interval' to resend them. repeat_interval: 10s # A default receiver receiver: monitor-admin group_by: [name, alertname] routes: # All alerts with service=mysql or service=cassandra # are dispatched to the database pager. - receiver: 'go-admin' group_wait: 30s group_by: [name, alertname] match: name: goroutines_test alertname: go_goroutines_test # All alerts with the team=frontend label match this sub-route. # They are grouped by product and environment rather than cluster # and alertname. - receiver: 'instance-admin' group_interval: 1m group_by: [name, alertname] match: name: instance 这里的group_by: [label,...]将通知分组，group_interval: 30s指的是如果在30s内收到同一组的邮件告警，那么将合并他们为一条邮件然后发送。repeat_interval: 10s当第一次发送邮件后下一次发送邮件的间隔。这个值一定要改，尤其是上线之后。routes主要是将邮件进行区分发送，这里我们之前在rule里面配置的label就可以起作用了。使用match来匹配那些label然后做不同的路由。 alertmanager 邮件抑制邮件抑制的意思，我觉得更应该是邮件优先原则。指的是如果有两组告警，一组告警非常紧急，一组不重要，如果两组一起来告警，那么可能会引起，邮箱里面重要的告警被不重要的淹没了，从而导致我们忽略了某些重要告警。所以我们可以进行配置： 12345678inhibit_rules:- source_match: severity: 'critical' target_match: severity: 'warning' # Apply inhibition if the alertname is the same. # equal: ['alertname', 'cluster', 'service'] equal: ['monitor','job'] 上面的规则就是，如果有serverity为critical的标签，那么serverity为warning的告警先不发，只发critical的告警。在Prometheus的rule里面如下配置： 所以，多配置点label还是有点用的，哈哈。 ps：抑制的是某一种高级别的邮件发送，而这个邮件会按照定好的时间间隔一直发送，不会说只发一条就不发了。 alertmanager 邮件静默邮件静默其实就是指在某一段时间内，将某一类型的告警暂时忽略不让它发送告警。（又用到label了。。）创建静默的方式有两种，一个是直接在告警信息上创建；另一个是直接new silence： 需要配置一个label。之后一定一定要配置时间，第一次使用的时候，我以为静默是一直忽略，然后我就下班了。之后回到家中，2h默认时间过后，我的邮箱爆炸了。。。所以静默是有时间限制的，一定要，一定要设置。 alertmanager的邮件模板12345678910templates: - './templates/*.tmpl'receivers: - name: 'monitor-admin' email_configs: - to: 'noreply_czj@163.com' headers: &#123; Subject: "[WARN] &#123;&#123; .CommonLabels.alertname&#125;&#125; 报警邮件" &#125; html: '&#123;&#123; template "email.czj.html" . &#125;&#125;' 1234567891011&#123;&#123; define &quot;email.czj.html&quot; &#125;&#125;&lt;table&gt; &lt;tr&gt;&lt;td&gt;报警名&lt;/td&gt;&lt;td&gt;xxx&lt;/td&gt;&lt;td&gt;xx&lt;/td&gt;&lt;td&gt;xxx&lt;/td&gt;&lt;td&gt;xxx&lt;font color=&apos;red&apos;&gt;[xxx]&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt; &#123;&#123; range $i, $alert := .Alerts &#125;&#125; &lt;tr&gt;&lt;td&gt;&#123;&#123; index $alert.Labels &quot;alertname&quot; &#125;&#125;&lt;/td&gt;&lt;td&gt;&#123;&#123; $alert.StartsAt &#125;&#125;&lt;/td&gt;&lt;td&gt;&#123;&#123; $alert.Labels.from &#125;&#125;&lt;/td&gt;&lt;td&gt;&#123;&#123; $alert.Labels.to &#125;&#125;&lt;/td&gt;&lt;td&gt;&#123;&#123; $alert.Labels.value &#125;&#125;&lt;/td&gt;&lt;/tr&gt; &#123;&#123; end &#125;&#125; &lt;/table&gt;&#123;&#123; end &#125;&#125; 邮件模板这里我是在网上找了一个非常详细的帖子。我觉得写的非常详细，主要注意就是要在模板的一行的define，要和receivers的html中tempate相对应。之后就是在模板中引用go语言的变量用点+大写变量名。详细可以看这边博客alertmanager 邮件模板。从源码解释，很详细。 Data:.Alerts：[{firing map[instance:10.62.14.80:8080 job:example-random monitor:codelab-monitor to:10.62.12.3 alertname:node_connect from:10.62.14.80 group:production] map[summary:机器 10.62.14.80:8080 ，10.62.14.80，10.62.12.3] 2018-04-19 15:44:51.3912519 +0800 CST 0001-01-01 00:00:00 +0000 UTC http://FTSZ-NB0078:9090/graph?g0.expr=host_to_host_http_connect+%3C%3D+0&g0.tab=1} {firing map[monitor:codelab-monitor to:10.62.14.80 alertname:node_connect from:10.62.14.80 group:production instance:10.62.14.80:8080 job:example-random] map[summary:机器 10.62.14.80:8080 ，10.62.14.80，10.62.14.80] 2018-04-19 15:44:51.3912519 +0800 CST 0001-01-01 00:00:00 +0000 UTC http://FTSZ-NB0078:9090/graph?g0.expr=host_to_host_http_connect+%3C%3D+0&g0.tab=1}] status:.status: firing Receiver:.Receiver: team-X-mails GroupLabels:.GroupLabels: map[alertname:node_connect] CommonLabels:.CommonLabels: map[from:10.62.14.80 group:production instance:10.62.14.80:8080 job:example-random monitor:codelab-monitor alertname:node_connect] CommonAnnotations:.CommonAnnotations: map[] ExternalURL:.ExternalURL: http://FTSZ-NB0078:9093 alertmanager 邮件配置注意一个，是否开启ssl。我们用的是http，所以把ssl关闭。我用163配置的： 123456global: smtp_smarthost: 'smtp.163.com:25' smtp_from: '********@163.com' smtp_auth_username: '*******@163.com' smtp_auth_password: '*******' smtp_require_tls: false 一般本地测试的时候：smtp_require_tls这个可以是true，但是一般服务器不通外网可能不行。 prometheus 与 alertmanager 降低日志级别日志调到debug级别，两者都可以用.alertmanager --help或者./promtheus --help来查看所有可选项。 alertmanager:./alertmanager --config.file=&#39;config163.yml&#39; --log.level=debug; prometheus: ./prometheus --web.enable-lifecycle --config.file=&#39;prometheus.yml&#39; --log.level=debug --web.external-url=http://localhost:9090/pro --web.route-prefix=pro 后面Prometheus的--web.external-url=http://localhost:9090/pro，--web.route-prefix=pro。这两个参数就类似加个服务号，如果有子路径做反向代理，Prometheus最好启动的时候就加上这两个。 比如在nginx用子路径做反向代理：1234567891011121314151617181920212223242526272829303132server &#123; listen 127.0.0.1:80; #server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; # proxy_pass http://127.0.0.1:9090/; &#125; location /alertmanager/ &#123; proxy_pass http://127.0.0.1:9093/; &#125; location /pro/ &#123; proxy_pass http://127.0.0.1:9090/pro/; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 暂时就这些吧。 参考链接GETTING STARTED prometheus.ioalertmanager邮件模版alertmanager报警规则详解Prometheus智能化报警流程避免邮件轰炸Custom Alertmanager Templates]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rancher 体验]]></title>
    <url>%2F2018%2F04%2F18%2Francher-experience%2F</url>
    <content type="text"><![CDATA[Rancher 体验Rancher是一个开源的企业级容器管理平台。花了大概3个小时从零到自己搭建一个能运行的服务，真的容易啊。而且文档很丰富，官方还有中文版。哈~，地址：文档。 安装 Rancher安装Rancher其实很简单，因为它本身就提供了image，所以只需要在本地安装docker，然后下载镜像，之后再启动就完事了。整个过程非常顺利。唯一一个需要主要的地方是，它需要两台机器。一个作为rancher主节点，一个作为工作节点。所以，如果想体验一下 Rancher 的能力，最好能有两台ip不一样的机器，并且两台都安装了docker，当然也可以用虚拟机。 docker run -d --name rancher -p 8080:8080 --restart=unless-stopped rancher/server:stable 安装完 Rancher 之后，需要配置一下主机。Rancher 做了非常棒的国际化，支持中文。��[笑哭.png] 主机的注册地址，一般是rancher主节点的ip地址。 之后选择基础架构-&gt;添加主机。 只需要将脚本进行拷贝，在运行的另一台安装了docker的主机上执行它，之后就能在主机中看到了： 应用安装与运行应用分为用户应用和基础设施应用。用户应用： 基础设施应用： 添加应用： 应用是一个服务集，应用下可以有很多服务。一个应用下的网段是在一块的，不同应用之间的网络是不能直接连接的。 添加完应用之后就是添加服务： 添加完服务之后添加负载均衡： 一个发布在rancher中的应用就可以体验了。 参考资料Rancher 文档]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Rancher</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus 初体验]]></title>
    <url>%2F2018%2F04%2F16%2Fprometheus-pre-use%2F</url>
    <content type="text"><![CDATA[prometheus 监控普罗米修斯主要的是从被监控项目中获取metrics。架构全景图： 安装与使用Prometheus 的安装方式很多，我在本地是使用windows的包，Prometheus自打出生就支持docker所以，如果是*inx机器，建议安装docker然后体验。在本地我仅仅只需要执行目录下的prometheus.exe就可以就可以进行体验了。Prometheus服务自身也会暴露出metrics，用来对自身进行指标收集和监控。在根目录最重要的一个配置文件是prometheus.yml，里面有三类大属性：global，rule_files，scrape_configs。具体的配置信息可以看这个：https://prometheus.io/docs/prometheus/latest/configuration/configuration/，也可以看看中文文档：https://songjiayang.gitbooks.io/prometheus/configuration/ 在Prometheus的后台localhost:9090，选择一个metric，然后点击execute，之后就可以在下面的graph和console中看到输出的结果： 在status下选择Configuration然后可以看到prometheus.yml里面的定义。在Target下可以看到监控的目标源的ip地址的信息。 metrics 类型metrics 有四类，并且每一个类都有相应的客户端lib。在目标监控中需要暴露出相应的metrics给prometheus服务器进行收集，之后才能进行有效的信息分析，之后预警和监控。 Prometheus客户端lib提供4种主要的核心metric类型：Counter，Gauge，Histogram，Summary。 Counter：数值类型，只能增加，不能减少。用户计数请求服务，完成任务数，发生错误数。不要用在可能会减少的地方 Gauge：数值类型可以增加可以减少，变化的类型，类似于测量温度，当前内存使用量 Histogram：直方图对样本进行观察（比如请求的持续时间，响应大小）。并将其统计到一个可配置的bucket中。一个Histogram包括： 123&lt;basename&gt;_bucket&#123;le="&lt;upper inclusive bound&gt;"&#125;&lt;basename&gt;_sum&lt;basename&gt;_count：指的是相同的&lt;basename&gt;_bucket&#123;le="+Inf"&#125; Summary：主要用于表示一段时间内数据采样结果，类似Histogram 123&lt;basename&gt;&#123;quantile="&lt;φ&gt;"&#125; （0 ≤ φ ≤ 1）&lt;basename&gt;_sum&lt;basename&gt;_count metrics定义如：&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;, ...} metrics接口需要注意，每行要空行，最后以空行结束。 123456789101112131415161718192021222324252627282930313233343536# HELP http_requests_total The total number of HTTP requests.# TYPE http_requests_total counterhttp_requests_total&#123;method=&quot;post&quot;,code=&quot;200&quot;&#125; 1027 1395066363000http_requests_total&#123;method=&quot;post&quot;,code=&quot;400&quot;&#125; 3 1395066363000# Escaping in label values:msdos_file_access_time_seconds&#123;path=&quot;C:\\DIR\\FILE.TXT&quot;,error=&quot;Cannot find file:\n\&quot;FILE.TXT\&quot;&quot;&#125; 1.458255915e9# Minimalistic line:metric_without_timestamp_and_labels 12.47# A weird metric from before the epoch:something_weird&#123;problem=&quot;division by zero&quot;&#125; +Inf -3982045# A histogram, which has a pretty complex representation in the text format:# HELP http_request_duration_seconds A histogram of the request duration.# TYPE http_request_duration_seconds histogramhttp_request_duration_seconds_bucket&#123;le=&quot;0.05&quot;&#125; 24054http_request_duration_seconds_bucket&#123;le=&quot;0.1&quot;&#125; 33444http_request_duration_seconds_bucket&#123;le=&quot;0.2&quot;&#125; 100392http_request_duration_seconds_bucket&#123;le=&quot;0.5&quot;&#125; 129389http_request_duration_seconds_bucket&#123;le=&quot;1&quot;&#125; 133988http_request_duration_seconds_bucket&#123;le=&quot;+Inf&quot;&#125; 144320http_request_duration_seconds_sum 53423http_request_duration_seconds_count 144320# Finally a summary, which has a complex representation, too:# HELP rpc_duration_seconds A summary of the RPC duration in seconds.# TYPE rpc_duration_seconds summaryrpc_duration_seconds&#123;quantile=&quot;0.01&quot;&#125; 3102rpc_duration_seconds&#123;quantile=&quot;0.05&quot;&#125; 3272rpc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 4773rpc_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; 9001rpc_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; 76656rpc_duration_seconds_sum 1.7560473e+07rpc_duration_seconds_count 2693 Exporter自定义Exporter其实就是暴露一个metrics接口让Prometheus服务器进行收集。所以只需要返回像上面的例子一样的文本数据就可以了。官方推荐首先采用Historam。可以导入相应的客户端metric然后进行导入。 继续学习 Prometheus 的内置函数 Prometheus 自定发现监控目标 报警规则，报警方式 图表 对接granafa]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 logstash 输出到 Elasticsearch 时指定 document_id]]></title>
    <url>%2F2018%2F04%2F02%2Flogstash-output-elasticsearch-document-id%2F</url>
    <content type="text"><![CDATA[今天使用elk搜集日志的时候想到一个事情，是否可以指定es索引中的document_id。查了资料之后发现还真有这个： 123456elasticsearch &#123; hosts =&gt; [ "localhost:9200" ] index =&gt; "%&#123;[fields][service_name]&#125;-%&#123;+YYYY.MM.dd&#125;" document_id =&gt; "%&#123;@timestamp&#125;" &#125; 就是在logstash.conf中的output中，设置elasticsearch里面document_id就可以了。就是这么简单。当然我这里是用timestamp做的id，其实可以自己换成一个其它的。es 对一个 id 添加两次是支持的，里面的字段 version 会加 1。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab 自定义配置 （三）]]></title>
    <url>%2F2018%2F04%2F02%2Fgitlab-customer-settings%2F</url>
    <content type="text"><![CDATA[登陆页面与LOGO自定义 在这里替换之后返回登陆也可以看到： 注册邮箱后缀限制在admin area中settings下限制邮箱后缀，将公司邮箱设为白名单，其它邮箱设为黑名单。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 SVN 迁移到 Git （二）]]></title>
    <url>%2F2018%2F04%2F02%2Fsvn-to-git%2F</url>
    <content type="text"><![CDATA[从 SVN 导入到 GitLab 仓库中我们只需要下载一个 git-svn 的工具，如果是windows版本的git工具，应该是内置了的；Linux 下使用yum install git-svn。一些操作可以参考git官网的两篇文章：Git-与其他系统-Git-与-Subversion和Git-与其他系统-迁移到-Git。 从实际的svn迁移到git中有两种方式: 1) 最简单最直接的方式，从svn拷下代码，然后上传到gitlab仓库中。先将svn上的代码拷下来，git svn clone https://192.168.1.12/svn/trade/App_cn/Src/_CJ208_Noe。 然后使用git将其推送到gitlab仓库。 1git remote add origin http://gitlab.xxx.com/xxxgroup/_CJ208_Noe.git 推送过程记得使用root管理： 可以在项目提交历史看到相应的提交记录。 这种方式提交的话，可以看到提交人的信息跟gilab是没有绑定的，这样可能就不太方便我们去查找某人，尤其是svn账号不规范的情况。 上图就是使用czj这种缩写，有些svn账号又是全名。有可能是因为历史原因，不过如果想直接从svn导入到git，这种方式是最方便的。当然可能在历史的提交记录里面有一些杂的信息。比如：git-svn-id:XXXX这些，进入到项目里面使用git log可以看到commit信息： 2) 将用户名和账号对应起来，然后再上传代码。不过在上传之前我们先过滤一下：a) 先将用户名称过程，可以使用下面两个方式将svn提交的用户名存放到文本文件中： 直接使用svn地址： 12svn log https://192.168.7.2/svn/trade/App_cncsen/Src/CJ325_Integral --xml | grep -P "^&lt;author" | sort -u | \ perl -pe 's/&lt;author&gt;(.*?)&lt;\/author&gt;/$1 = /' &gt; users.txt 将代码从svn拷下来之后，进入到代码根目录里面使用： 12svn log ^/ --xml | grep -P "^&lt;author" | sort -u | \ perl -pe 's/&lt;author&gt;(.*?)&lt;\/author&gt;/$1 = /' &gt; users.txt 在users.txt中我们可以看到提交人的账号信息： 之后我们将其匹配到gitlab中的用户名： 然后使用 git svn clone 将代码拷下来，不过这次拷贝我们加入用户的信息： 1git svn clone https://192.168.7.246/svn/trade/App_cncsen/Src/CJ325_Integral --authors-file=users.txt --no-metadata 之后再进行一些过滤: 过滤svn中tag分支： 1git for-each-ref refs/remotes/tags | cut -d / -f 4- | grep -v @ | while read tagname; do git tag &quot;$tagname&quot; &quot;tags/$tagname&quot;; git branch -r -d &quot;tags/$tagname&quot;; done 然后将 refs/remotes 下面剩下的索引变成本地分支: 1git for-each-ref refs/remotes | cut -d / -f 3- | grep -v @ | while read branchname; do git branch &quot;$branchname&quot; &quot;refs/remotes/$branchname&quot;; git branch -r -d &quot;$branchname&quot;; done 这样过滤之后现在再发布到远端，下图可以看到，现在的历史中干净很多，而且名字也对应上了。 现在就可以开心的使用git了。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab 安装与配置（一）]]></title>
    <url>%2F2018%2F03%2F30%2Fcode-in-gitlab%2F</url>
    <content type="text"><![CDATA[很多公司都是使用的 svn 来管理代码，其实我感觉 git 肯定是未来的潮流。最近闲暇时间，在公司搭建了一个 GitLab ，正好可以记录一下。 gitlab 的搭建环境推荐使用 &gt;=4G 内存 使用 Docker 部署 gitlab首先需要安装 Docker。推荐使用docker官网的安装方式：Centos 安装docker，如果访问docker官网过于太慢，可以使用DaoCloud下载docker：daocloud 安装docker。有时候下载镜像，有一些比较显而易见的原因，所以可以使用docker镜像加速器。 安装docker之后，下载镜像gitlab/gitlab-ce:latest，之后参照安装文档。 123456789sudo docker run --detach \ --hostname gitlab.example.com \ --publish 443:443 --publish 80:80 --publish 22:22 \ --name gitlab \ --restart always \ --volume /srv/gitlab/config:/etc/gitlab \ --volume /srv/gitlab/logs:/var/log/gitlab \ --volume /srv/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce:latest 一个gitlab就搭建好了。默认的帐号为root。 配置邮件服务在gitlab的配置目录有个文件：/etc/gitlab/gitlab.rb，编辑这个文件。 1sudo docker exec -it gitlab vi /etc/gitlab/gitlab.rb 如果你是用的docker方式，看上面的命令，可以编辑本地目录/srv/gitlab/config/gitlab.rb也是可以的，不过需要在docker容器里面进行配置重加载。 安装邮件的文档：配置smtp] 贴一份在实际中使用的邮件配置： 1234567891011121314gitlab_rails[&apos;gitlab_email_enabled&apos;] = truegitlab_rails[&apos;gitlab_email_from&apos;] = &apos;name-gitlab@company.com&apos;gitlab_rails[&apos;gitlab_email_display_name&apos;] = &apos;gitlab邮件通知&apos;gitlab_rails[&apos;smtp_enable&apos;] = truegitlab_rails[&apos;smtp_address&apos;] = &quot;smtp.mxhichina.com&quot;gitlab_rails[&apos;smtp_port&apos;] = 465gitlab_rails[&apos;smtp_user_name&apos;] = &quot;name-gitlab@company.com&quot;gitlab_rails[&apos;smtp_password&apos;] = &quot;email-password&quot;#gitlab_rails[&apos;smtp_domain&apos;] = &quot;ap-ec.cn&quot;gitlab_rails[&apos;smtp_authentication&apos;] = &quot;login&quot; # 认证的方式gitlab_rails[&apos;smtp_enable_starttls_auto&apos;] = truegitlab_rails[&apos;smtp_tls&apos;] = trueuser[&apos;git_user_email&apos;] = &quot;name-gitlab@company.com&quot; 安装完邮件后进入到容器中：docker exec -ti gitlab bash，之后使用gitlab-ctl reconfigure进行配置重加载，加载完成后需要进行测试：gitlab-rails console 1Notify.test_email('test-email-name@company.com', 'Message Subject', 'Message Body').deliver_now 输入上面的命令如果有新邮件收到，那么邮箱smtp就配置成功。 如果smtp配置成功，但是注册的时候还是没有邮件发送通知，那么重启一下容器。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea-linux-vim等常用快捷键]]></title>
    <url>%2F2018%2F03%2F26%2Fidea-linux-vim-command-shortcut%2F</url>
    <content type="text"><![CDATA[最近经常看到一些有意思的快捷键，但是又不是经常用到，平常经常用的肯定都能很熟悉了。 idea 常用快捷键最常用的肯定是 find action :ctrl shift A，这简直就是神器，如果忘记快捷键，尝试使用这个，然后输入快捷键的功能名称就有可能找到相应 功能了。 idea 弹出当前类里面的方法框：ctrl o, 类结构图，file structure：ctrl F12 跳转： navigate -&gt; back/forward; navigate -&gt; last edit location/next edit location 最近的文件：recent file/recent changed file ctrl + E 书签跳转，bookmarks : F11快速打一个书签； ctrl F11定一个数字，之后使用ctrl+数字快速定位； shift F11 弹出书签预览。 收藏： add to favorites, 快捷键: alt shift F，你可以先建立一个favorites列表(add new favorites lists)，然后在代码行处使用add to favorites。之后可以使用alt 2调用面板查看 跳到某个具体类 Ctrl N 找文件：Ctrl shift N 找单个字符：Ctrl shift alt n,可以在文件中寻找一个单词，字符等。 字符串搜索：Edit-&gt;Find-&gt;Find in Path: ctrl shift F,可以改建，我的改成了ctrl shift Y 移动操作：move caret XXXXX，然后选择相应的选项 大小写：edit-&gt; toggle case ，ctrl shift u 相同字符串多列操作：select all occurences –&gt; ctrl shift alt j，比如下图中将=号右边部分全部加上双引号&quot;&quot;，这种情况就可以尝试多列操作。 在上图中如果只是多列操作，那么后面中文的字符长度不一样，那么不能直接移动，所以可以使用第10条移动操作结合起来一起操作。简直神器。 不知道怎么操作时神奇键：show intention action，alt enter 重构：shift F6,ctrl shift alt t 重构方法，签名等：ctrl F6 抽取变量，函数等，refactor-&gt;Extract-&gt;xxx；可以选局部变量，全局变量等。下面是选择的variable 代码最后一次提交人：annotate。 文件修改位置：previous change 版本撤销：revert 本地修改记录：show history 打本地标签：put label，类似 commit 在当前目录新建文件：ctrl alt insert 文件复制：F5 文件移动：F6 复制文件全路径：ctrl shift C 复制文件名：ctrl shift alt C 选择复制的历史数据：ctrl shift V maven 依赖图，右键maven-&gt;show dependencies：ctrl shift alt u 查看当前类的继承关系，hierarchy class ：ctrl H 方法调用，call hierarchy，ctrl alt Hidea 神奇操作： live templates注意live templates 可以使用$END$作为最后结束时，光标的位置： postfixpostfix 是idea预置的，无法增加，使用ctrl shift a输入“postfix”就能看到相应的预置postfix。 比如我们想生成下面的代码： 123if(args != null)&#123; //xxxx&#125; 使用postfix可能只需要输入: 1args.nn 就能出现提示，生成上面的代码 idea debug 操作 断点： toggle line breakpoint：ctrl F8。 debug模式启动：shift F9 单步运行：F8 结束当前断点：resume，F9 关停所有断点：mute breakpoints 条件断点：ctrl shift f8 表达式求值：evaluate expression：alt F8 从断点跳到光标行：run to cursor：alt f9 运行过程中改变值：set value，在Debug视图中选中需要改变的变量按F2 在当前位置运行，适用与单元测试：ctrl shift f9 从历史运行中寻找一个运行：alt shift f9–&gt;Run--&gt;Debug... idea 常用插件key promoter，idea vim：:splombok plugin，maven helper，sonar lint，alibaba java code guideemacsidea: 使用ctrl + J 然后再输入想查找的字符，就可以快速定位了，在keymap中修改acejumpworld。 VScode 快捷键ctrl k 之后按下 v，打开实时预览。]]></content>
      <categories>
        <category>All</category>
      </categories>
      <tags>
        <tag>All</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 多线程随笔]]></title>
    <url>%2F2018%2F03%2F23%2Fjava-thread-guidance%2F</url>
    <content type="text"><![CDATA[Java多线程随笔最近匆匆的看了一下多线程的知识，其实在我们现在的系统中，我只在一个地方用了多线程，异步刷缓存的时候使用到了。其它地方并没有使用。这个可能跟公司局限性业务有关系，毕竟大家都听过一句话，20%的人掌握着80%的财富。而我们服务的就是那20%里面的一部分。 多线程的产生硬件的摩尔定律已经被打破了，所以现在多核处理器下使用多线程那是大大的提高效率啊。毕竟5个人同时赚钱，跟5个人轮流赚钱那可不是差的一点半点。所以为了合理的利用资源，那必须的挤榨一点机器性能啊。 上下文切换多线程未必一定比单线程快，为什么？每个东西都是有正反面的。多线程其实对于计算机来说是一种cpu时间切片。像《java并发编程艺术》里面说的，你在看英文书的时候，如果在某一页遇到不知道的单词，你停下来查字典，查完字典你还得返回去继续读。这个过程中读书，查字典就是两个线程，你就是cpu，你给每个线程一些时间让他们做自己的事情。但是在这个过程中，在你从看书切换到查字典再切换到看书这个过程中，你其实还需要记住看书的位置吧，这种从一件事切换到另一件事再从另一件事切回来，总的有耗费吧？术语称之为上下文切换。 减少上下问切换的方式： 无锁并发编程。使用hash算法进行数据id取模分段，特定的线程处理不同段的数据 使用cas算法，java的atomic包，cas=compare and swap 使用最少线程，避免创建不必要的线程 使用协程，在单线程里面实现多任务调度，并在单线程里面维持多个任务间的切换。 死锁死锁的概念就是有两个线程A，B。线程A，B都需要a,b资源，线程A已经占有了a资源，需要b资源，线程B已经占有了b资源，需要a资源；所以A等待B，B等待A。这样就造成了死锁。这个就是那个著名的哲学家吃饭的问题，5个人，5根筷子，嗯就是这个问题。 线程执行任务 runnable接口，不带返回值 Callable接口，返回Future对象 定义任务：实现runnable，重写run方法。一个线程都需要使用Thread.start()进行启动。线程的运行由线程调度器来选择，线程调度机制非确定性的。 callable接口—-&gt;实现call()—-&gt;executorService.submit()—-&gt;Future&lt;—–访问Future的isDone()是否完成 线程池FixedThreadPool:一次性预先加载线程，限制线程的数量;CachedThreadPool:创建与所需数量相同的线程;SingleThreadPool:线程数量为1的fixedThreadPool; sleep()和wait()的区别sleep()调用的时候不会释放锁，wait()是会阻塞当前线程同时释放锁。 线程优先级优先级别主要是给线程调度器用，如果同一优先级也是随机的。建议使用MAX_PRIORITY,NORM_PRIORITY,MIN_PRIORITY。 线程让步：yield(),然后相同优先级的其它线程；join()，自己会挂起，让join的线程先完成; 后台线程,在调用start方法之前设置daemon。 java并发包java有个juc的并发包。要再仔细看看。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引优化实例]]></title>
    <url>%2F2018%2F03%2F23%2Fmysql-optimization-index%2F</url>
    <content type="text"><![CDATA[数据库索引优化实例索引对于查找小表或者需要处理表中大部分或全部行的大型表的查询不太重要，适合的才是最好的。 数据库索引的操作 快速找到匹配where条件的行结果； 如果有多个索引，MySQL通常会使用能检索除最少结果行的索引； 如果有多列索引，优化器可以使用索引的最左边的前缀来查找行；比如有col1,col2,col3三列索引，那么你就可以使用(col1)，（col1,col2）,(col1,col2,col3)三个索引； 如果有多表连接join的语句，mysql会将varchar(10)和char(10)看成同一个类型并且有效的使用索引；两个表使用的字符要一样比如utf8,和latin，就不行； 在查找最小值和最大值的时候，优化器会先优化你是否在where条件中有常量值； 索引排序会使用最左边的索引列前缀； 在某些情况下，可以优化查询来检索值，而不必咨询数据行 主键与外键优化主键索引的查询优化得益于主键不能为NULL值。表的数据在物理存储上就被组织为基于主键列或多列进行快速查找和排序。如果你的表很大又非常重要，然而却没有声明主键，或者组合主键，那么你需要新建一个新的列使用auto-increment来作为主键。 外键优化，如果有一个大表，有许多列，可以考虑将表的列进行拆分成小表，并且用外键关联。每个小表都有主键，这样可以让查找数据更加快速，而需要数据的时候也可以是使用join关联查询。由于数据是分散的，查询可以有更少的I/O,并且会使用更少的缓存。因为相关的列在磁盘中存储在一块。 组合索引建立一个表： 1234567CREATE TABLE test ( id INT NOT NULL, last_name CHAR(30) NOT NULL, first_name CHAR(30) NOT NULL, PRIMARY KEY (id), INDEX name (last_name,first_name)); 我们有组合索引name，在索引查询中使用最左列查询是会使用索引的。也就是如下面的列子： 123456789101112SELECT * FROM test WHERE last_name='Widenius';SELECT * FROM test WHERE last_name='Widenius' AND first_name='Michael';SELECT * FROM test WHERE last_name='Widenius' AND (first_name='Michael' OR first_name='Monty');SELECT * FROM test WHERE last_name='Widenius' AND first_name &gt;='M' AND first_name &lt; 'N'; 即只要有最左侧的索引列，那么索引就会生效。但是如果是单独使用的first_name，那么索引将不会生效，如下面的示例： 1234SELECT * FROM test WHERE first_name='Michael';SELECT * FROM test WHERE last_name='Widenius' OR first_name='Michael'; 如果是first_name,last_name两个列都建立索引，那么查询的时候如果两列都有建立单独的索引，那么优化器可能会使用能排除最大结果的那个索引。如果是联合索引，那么只要带有最左侧的索引列，那么优化器就会工作。比如如果有索引(col1,col2,col3)，下面的语句只会有前面两个可以用索引下面两个不会用到索引： 12345SELECT * FROM tbl_name WHERE col1=val1;SELECT * FROM tbl_name WHERE col1=val1 AND col2=val2;SELECT * FROM tbl_name WHERE col2=val2;SELECT * FROM tbl_name WHERE col2=val2 AND col3=val3; 索引扩展的使用如果表的主键是一个组合索引，并且还声明了另一个索引。那么innodb会自动建立一个次索引，这个次索引包含了两个组合主键。 1234567CREATE TABLE t1 ( i1 INT NOT NULL DEFAULT 0, i2 INT NOT NULL DEFAULT 0, d DATE DEFAULT NULL, PRIMARY KEY (i1, i2), INDEX k_d (d)) ENGINE = InnoDB; 主键是一个组合主键，并且还有一个索引k_d；数据库还会在内部生成一个次索引（d,i1,i2）；优化器可以扩展次索引为ref,range,index_merge，可以在join和排序优化，和最大最小值优化；The optimizer can use extended secondary indexes for ref, range, and index_merge index access, for loose index scans, for join and sorting optimization, and for MIN()/MAX() optimization. 考虑下面一个示例： 1234567891011121314INSERT INTO t1 VALUES(1, 1, '1998-01-01'), (1, 2, '1999-01-01'),(1, 3, '2000-01-01'), (1, 4, '2001-01-01'),(1, 5, '2002-01-01'), (2, 1, '1998-01-01'),(2, 2, '1999-01-01'), (2, 3, '2000-01-01'),(2, 4, '2001-01-01'), (2, 5, '2002-01-01'),(3, 1, '1998-01-01'), (3, 2, '1999-01-01'),(3, 3, '2000-01-01'), (3, 4, '2001-01-01'),(3, 5, '2002-01-01'), (4, 1, '1998-01-01'),(4, 2, '1999-01-01'), (4, 3, '2000-01-01'),(4, 4, '2001-01-01'), (4, 5, '2002-01-01'),(5, 1, '1998-01-01'), (5, 2, '1999-01-01'),(5, 3, '2000-01-01'), (5, 4, '2001-01-01'),(5, 5, '2002-01-01'); 执行执行计划： 1EXPLAIN SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = '2000-01-01' 如果是多表查询就像下面这样： 如果要关闭优化器： 1SET optimizer_switch = 'use_index_extensions=off'; 执行计划 - EXPLAINEXPLAIN语句提供你MySQL是怎么执行语句的过程。EXPLAIN可以在SELECT,DELETE,INSERT,REPLACE,UPDATE语句上使用，返回他们的执行计划。也就是这些语句是按照何种方式执行会返回表记录。EXPLAIN的执行计划可以使用SHOW WARNINGS来返回更多的扩展信息。 执行计划输出的列信息 列名 意义 id SELECT标识码 select_type SELECT类型 table 表明 partitions 匹配分区 type 获取类型（access_type） possible_keys 可选择的索引 key 实际执行语句时候选择的索引 key_len 选择的key的长度 ref The columns compared to the index rows 预计会有多少行被查询到 filtered Percentage of rows filtered by table condition extra 扩展信息 其中select_type可以是下面这些值： select_type 值 意义 SIMPLE 简单查询（不使用union或者子查询） PRIMARY 最常用的查询类型 UNION 联合查询(Second or later SELECT statement in a UNION) DEPENDENT_UNION 依赖外部的联合查询（Second or later SELECT statement in a UNION, dependent on outer query） UNION RESULT UNION的结果 SUBQUERY 子查询 DEPENDENT SUBQUERY 依赖外部的子查询 DERIVED Derived table SELECT (subquery in FROM clause) MATERIALIZED Materialized subquery UNCACHEABLE SUBQUERY A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) 其中type 可以是下面这些值，如果执行计划是全表扫描，那么最好是进行优化，以下表格从最优到最慢排序： type值 意义 system 表只有一行数据（系统表），特殊的const类型 const 表查询出来的结果最多只有一行，适用于主键查询和唯一索引查询 eq_ref 唯一索引行，常用于join查询中主键查询和非null唯一索引。第二个表只会匹配第一个表中的一行数据，一对一的关系 ref 一对多关系，前一个表的一行数据可以匹配另一个表的多行数据 fulltext 全文本扫描 ref_or_null 类似ref，不过会处理null值 index_merge This join type indicates that the Index Merge optimization is used. In this case, the key column in the output row contains a list of indexes used, and key_len contains a list of the longest key parts for the indexes used. unique_subquery 替换eq_ref在in的子查询中的类型（value IN (SELECT primary_key FROM single_table WHERE some_expr)） index_subquery 类似unique_subquery，替换在in查询中，但是它是使用在非唯一索引的子查询中(value IN (SELECT key_column FROM single_table WHERE some_expr)) range 范围搜索，使用索引查找行结果。在这个查询中key列是指那个索引被用到，key_len是指被用到的最长的key。ref列是NULL index 同ALL一样会扫描全表不过是以索引扫描。在这个类型中只有单索引会出现这种类型。另外注意两点注意:1.If the index is a covering index for the queries and can be used to satisfy all data required from the table, only the index tree is scanned. In this case, the Extra column says Using index. An index-only scan usually is faster than ALL because the size of the index usually is smaller than the table data. 2. A full table scan is performed using reads from the index to look up data rows in index order. Uses index does not appear in the Extra column ALL 全表扫描，需要避免 其中ref的含义为： ref列显示哪些列或常量与key列中命名的索引相比较，以便从表中选择数据。 实例外部银行卡表，除了主键，没有其它的索引： 第一次使用查询语句执行计划： 然后我们增加索引一个索引列： 12ALTER TABLE `cncsen`.`bank_card_info` ADD UNIQUE INDEX `card_no_UNIQUE` (`card_no` ASC); 再次执行刚刚的执行计划： 加入索引后很明显我们由全表扫描变成了范围扫描，试想如果该表数据很大，那么这种优化是有意义的。所以Explain还是很有作用的 参考文档How MySQL Uses Indexes EXPLAIN Output Format 附录摘抄了一个gitchat的文章： MySQL索引优化规则可以通过以下规则对 MySQL 索引进行优化。 前导模糊查询不能使用索引。 例如下面 SQL 语句不能使用索引。select * fromdoc where title like ‘%XX’ 而非前导模糊查询则可以使用索引，如下面的 SQL 语句。select * fromdoc where title like ‘XX%’ 页面搜索严禁左模糊或者全模糊，如果需要可以用搜索引擎来解决。 union、in、or 都能够命中索引，建议使用 in。 union：能够命中索引。示例代码如下：select * fromdoc where status=1 unionall select * fromdoc where status=2 直接告诉 MySQL 怎么做，MySQL 耗费的 CPU 最少，但是一般不这么写 SQL。in：能够命中索引。 示例代码如下：select * fromdoc where status in (1, 2) 查询优化耗费的 CPU 比 union all 多，但可以忽略不计，一般情况下建议使用 inor：新版的 MySQL 能够命中索引。 示例代码如下：select * fromdoc where status = 1 or status = 2 查询优化耗费的 CPU 比 in 多，不建议频繁用 or。 负向条件查询不能使用索引，可以优化为 in 查询。 负向条件有：!=、&lt;&gt;、not in、not exists、not like 等。 例如下面代码：select * fromdoc where status != 1 and status != 2 可以优化为 in 查询：select * fromdoc where status in (0,3,4) 联合索引最左前缀原则（又叫最左侧查询） 如果在(a,b,c)三个字段上建立联合索引，那么它能够加快 a | (a,b) | (a,b,c) 三组查询速度。 例如登录业务需求，代码如下。selectuid, login_time from user where login_name=? andpasswd=? 可以建立(login_name, passwd)的联合索引。 因为业务上几乎没有 passwd 的单条件查询需求，而有很多 login_name 的单条件查询需求，所以可以建立(login_name, passwd)的联合索引，而不是(passwd, login_name)。建联合索引的时候，区分度最高的字段在最左边。如果建立了(a,b)联合索引，就不必再单独建立 a 索引。同理，如果建立了(a,b,c)联合索引，就不必再单独建立 a、(a,b) 索引。存在非等号和等号混合判断条件时，在建索引时，请把等号条件的列前置。如 where a&gt;? and b=?，那么即使 a 的区分度更高，也必须把 b 放在索引的最前列。最左侧查询需求，并不是指 SQL 语句的 where 顺序要和联合索引一致。 下面的 SQL 语句也可以命中 (login_name, passwd) 这个联合索引。selectuid, login_time from user where passwd=? andlogin_name=? 但还是建议 where 后的顺序和联合索引一致，养成好习惯。 范围列可以用到索引（联合索引必须是最左前缀）。 范围条件有：&lt;、&lt;=、&gt;、&gt;=、between等。范围列可以用到索引（联合索引必须是最左前缀），但是范围列后面的列无法用到索引，索引最多用于一个范围列，如果查询条件中有两个范围列则无法全用到索引。 假如有联合索引 (empno、title、fromdate)，那么下面的 SQL 中 emp_no 可以用到索引，而 title 和 from_date 则使用不到索引。select * fromemployees.titles where emp_no &lt; 10010’ and title=’Senior Engineer’and from_date between ‘1986-01-01’ and ‘1986-12-31’ 把计算放到业务层而不是数据库层。 在字段上进行计算不能命中索引。 例如下面的 SQL 语句。select * fromdoc where YEAR(create_time) &lt;= ‘2016’ 即使 date 上建立了索引，也会全表扫描，可优化为值计算，如下：select * fromdoc where create_time &lt;= ‘2016-01-01’ 把计算放到业务层。 这样做不仅可以节省数据库的 CPU，还可以起到查询缓存优化效果。 比如下面的 SQL 语句：select * fromorder where date &lt; = CURDATE() 可以优化为：select * fromorder where date &lt; = ‘2018-01-2412:00:00’ 优化后的 SQL 释放了数据库的 CPU 多次调用，传入的 SQL 相同，才可以利用查询缓存。 强制类型转换会全表扫描 如果 phone 字段是 varchar 类型，则下面的 SQL 不能命中索引。 select * fromuser where phone=13800001234 可以优化为：select * fromuser where phone=’13800001234’ 更新十分频繁、数据区分度不高的字段上不宜建立索引。 更新会变更 B+ 树，更新频繁的字段建立索引会大大降低数据库性能。“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似。一般区分度在80%以上的时候就可以建立索引，区分度可以使用 count(distinct(列名))/count(*) 来计算。 利用覆盖索引来进行查询操作，避免回表。 被查询的列，数据能从索引中取得，而不用通过行定位符 row-locator 再到 row 上获取，即“被查询列要被所建的索引覆盖”，这能够加速查询速度。 例如登录业务需求，代码如下。selectuid, login_time from user where login_name=? andpasswd=? 可以建立(login_name, passwd, login_time)的联合索引，由于 login_time 已经建立在索引中了，被查询的 uid 和 login_time 就不用去 row 上获取数据了，从而加速查询。 如果有 order by、group by 的场景，请注意利用索引的有序性。 order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现 file_sort 的情况，影响查询性能。例如对于语句 where a=? and b=? order by c，可以建立联合索引(a,b,c)。如果索引中有范围查找，那么索引有序性无法利用，如 WHERE a&gt;10 ORDER BY b;，索引(a,b)无法排序。 使用短索引（又叫前缀索引）来优化索引。 前缀索引，就是用列的前缀代替整个列作为索引 key，当前缀长度合适时，可以做到既使得前缀索引的区分度接近全列索引，同时因为索引 key 变短而减少了索引文件的大小和维护开销，可以使用 count(distinct left(列名, 索引长度))/count(*) 来计算前缀索引的区分度。 前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于 ORDER BY 和 GROUP BY 操作，也不能用于覆盖索引（Covering Index，即当索引本身包含查询所需全部数据时，不再访问数据文件本身），很多时候没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 例如对于下面的 SQL 语句：SELEC * FROM employees.employees WHERE first_name=’Eric’AND last_name=’Anido’; 我们可以建立索引：(firstname, lastname(4))。 建立索引的列，不允许为 null。 单列索引不存 null 值，复合索引不存全为 null 的值，如果列允许为 null，可能会得到“不符合预期”的结果集，所以，请使用 not null 约束以及默认值。 利用延迟关联或者子查询优化超多分页场景。 MySQL 并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回 N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行 SQL 改写。 示例如下，先快速定位需要获取的 id 段，然后再关联:selecta.* from 表1 a,(select id from 表1 where 条件 limit100000,20 ) b where a.id=b.id 业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。 不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的。另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产生。 超过三个表最好不要 join。 需要 join 的字段，数据类型必须一致，多表关联查询时，保证被关联的字段需要有索引。 如果明确知道只有一条结果返回，limit 1 能够提高效率。 比如如下 SQL 语句：select * from user where login_name=? 可以优化为：select * from user where login_name=? limit 1 自己明确知道只有一条结果，但数据库并不知道，明确告诉它，让它主动停止游标移动。 SQL 性能优化 explain 中的 type：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts 最好。 consts：单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。ref：使用普通的索引（Normal Index）。range：对索引进行范围检索。当 type=index 时，索引物理文件全扫，速度非常慢。 单表索引建议控制在5个以内。 单索引字段数不允许超过5个。 字段超过5个时，实际已经起不到有效过滤数据的作用了。 创建索引时避免以下错误观念 索引越多越好，认为一个查询就需要建一个索引。宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度。抵制惟一索引，认为业务的惟一性一律需要在应用层通过“先查后插”方式解决。过早优化，在不了解系统的情况下就开始优化。 问题详解这部分，我将列出平时会遇到的一些问题，并给予解答。 请问如下三条 SQL 该如何建立索引？ where a=1and b=1 where b=1 where b=1order by time desc MySQL 的查询优化器会自动调整 where 子句的条件顺序以使用适合的索引吗？回答： 第一问：建议建立两个索引，即 idxab(a,b) 和 idxbtime(b,time)。第二问：MySQL 的查询优化器会自动调整 where 子句的条件顺序以使用适合的索引，对于上面的第一条 SQL，如果建立索引为 idxba(b,a) 也是可以用到索引的，不过建议 where 后的字段顺序和联合索引保持一致，养成好习惯。 假如有联合索引(empno、title、fromdate)，下面的 SQL 是否可以用到索引，如果可以的话，会使用几个列？ select * fromemployees.titles where emp_no between ‘10001’ and’10010’ and title=’Senior Engineer’ and from_date between ‘1986-01-01’and ‘1986-12-31’ 回答：可以使用索引，可以用到索引全部三个列，这个 SQL 看起来是用了两个范围查询，但作用于 empno 上的“between”实际上相当于“in”，也就是说 empno 实际是多值精确匹配，在 MySQL 中要谨慎地区分多值匹配和范围匹配，否则会对 MySQL 的行为产生困惑。 既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？ 回答：不是，因为索引虽然加快了查询速度，但索引也是有代价的。索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担。另外，MySQL 在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，另一种是数据的区分度比较低，可以使用 count(distinct(列名))/count(*) 来计算区分度。 主键和聚集索引的关系？ 回答：在 MySQL 中，InnoDB 引擎表是（聚集）索引组织表（Clustered IndexOrganize Table)，它会先按照主键进行聚集，如果没有定义主键，InnoDB 会试着使用唯一的非空索引来代替，如果没有这种索引，InnoDB 就会定义隐藏的主键然后在上面进行聚集。由此可见，在 InnoDB 表中，主键必然是聚集索引，而聚集索引则未必是主键。MyISAM 引擎表是堆组织表（Heap Organize Table)，它没有聚集索引的概念。 一个6亿的表 a，一个3亿的表 b，通过外键 tid 关联，如何最快的查询出满足条件的第50000到第50200中的这200条数据记录？ 回答：方法一：如果 a 表 tid 是自增长，并且是连续的，b表的id为索引。SQL语句如下。select * froma,b where a.tid = b.id and a.tid&gt;500000 limit200; 方法二：如果 a 表的 tid 不是连续的，那么就需要使用覆盖索引，tid 要么是主键，要么是辅助索引，b 表 id 也需要有索引。SQL语句如下。select * fromb, (select tid from a limit 50000,200) awhere b.id = a.tid; 假如建立联合索引(a,b,c)，下列语句是否可以使用索引，如果可以，使用了那几列？（考察联合索引最左前缀原则） where a= 3 答：是，使用了 a 列。where a= 3 and b = 5 答：是，使用了 a，b 列。where a = 3 and c = 4 and b = 5 答：是，使用了 a，b，c 列。where b= 3 答：否。where a= 3 and c = 4 答：是，使用了 a 列。where a = 3 and b &gt; 10 andc = 7 答：是，使用了 a，b 列。where a = 3 and b like ‘xx%’ andc = 7 答：是，使用了 a，b 列。 文章表的表结构如下： CREATE TABLEIF NOT EXISTS article (idint(10) unsigned NOT NULLAUTO_INCREMENT, author_idint(10) unsignedNOT NULL, category_idint(10) unsigned NOT NULL, viewsint(10) unsignedNOT NULL, commentsint(10) unsignedNOT NULL, titlevarbinary(255) NOT NULL, contenttext NOTNULL, PRIMARY KEY (id) ); 下面语句应该如何建立索引？select author_id, title, content from article where category_id = 1 and comments &gt; 1 order byviews desc limit 1; 回答：没有联合索引时，explain显示，如下图所示： 创建 idxcategoryidcommentsviews(category_id,comments, views) 联合索引时，explain显示，如下图所示： 创建 idxcategoryidviews(categoryid,views) 联合索引，explain 显示，如下图所示：由此可见，可以创建 idxcategoryidviews(categoryid,views) 联合索引。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 枚举和注解总结]]></title>
    <url>%2F2018%2F03%2F22%2Fjava-enum-anotation%2F</url>
    <content type="text"><![CDATA[Java 枚举和注解总结枚举没有枚举前我们基本上常量来定义值：1234567public interface Color&#123; public static final int GREEN = 0; public static final int RED = 1; .... .... ....&#125; 如果有了枚举后我们会怎样了？ 123public enum Color&#123; GREEN,RED,BLACk,.......&#125; 代码是不是就清晰很多了？而且使用的时候输出的值我们是可以使用GREEN，是不是可以很明白的知道是什么颜色？ 枚举的创建枚举是继承自Enum，使用enum关键字。 枚举的使用场景枚举适合在固定的常量下使用，比如四季，月份，星期；这种基本公认的而且不会有改变的场景下使用。 编译器中的枚举编译器默认帮我们实现了很多枚举中的方法，比如equals(),hashCode(),toString,values(),valueOf(String)等。这些都是编译器帮我们做的。 枚举中的注意事项 不能使用static，final修饰枚举，因为它是隐式的final类型的； 因为是final类型，所以我们也就知道它是不能被继承的； 从Enum继承的clone是final类型的，枚举是不能重写clone方法的，并且Enum里面的clone方法直接抛出异常，所以enum是不能被clone的; enum中的ordinal是强依赖于枚举实例的定义顺序的，所以用ordinal来做判断顺序是不推荐的，因为只要在非最后加入实例，那么就会改变整体的顺序;如果是需要顺序可以自定义属性。 注解注解出现前注解的作用我理解为就是用来约定一些数据定义，让我们可以在某个属性或者某个地方做个标记。在注解出现前能做这个的应该是XML，我们通常在xml中定义类或者属性的相关配置。而有了注解我们可以在代码中直接定义了。 注解的分类 定义注解的注解，元注解：@Rentation，@Target，@Document，@Inherited jdk内置注解：@Override，@Deprecated… 自定义的注解 spring等外部注解 作用场景： @Rentation：Source，Class，Runtime 作用目标域： @Target：Construct，Field，Local_variable,method,package,paramter,type 注解的定义注解的定义使用@interface关键字，并且使用元注解进行标注： 12345@Rentation(Rentation.Runtime)@Target(ElementType.Field)public @interface XxAnotation&#123; String values() default "";&#125; 注解属性注解里面的属性只能使用以下6种类型来定义： 1234561. 所有的基本类型；2. String3. Class4. enum5. Annotation6. 以上类型的数组类型 注解不允许使用基本类型的包装类来定义里面的注解属性。注解里面的属性都是使用方法的方式来定义的。有点类似接口方法。注解的属性需要注意一下几点： 要么具有默认值，要么在使用注解的时候提供属性的值； 非基本类型的元素，默认值不能为null； 如果只有一个属性，那么可以设置为value，在使用注解的时候就可以直接赋值；@XxAnotation(&quot;ok&quot;) ps:注解是不能继承的。 注解处理器我们定义了注解，设置了元素值。那么就必须要有一个处理器来进行注解处理。这个可以看看之前的Java 特殊字段脱敏，我们可以使用反射来获取注解定义的值，然后进行业务处理。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先队列和堆排序]]></title>
    <url>%2F2018%2F03%2F21%2Fjava-datastructure-priority-queue-heap%2F</url>
    <content type="text"><![CDATA[优先队列和堆排序很多情况下，我们会对某个任务做优先级，比如手机系统应用肯定要比第三方应用要优先级要高。实现这种方式的数据结构需要满足：一个最优先的任务（最大或最小值），加入新任务（新元素）；这种数据结构可以用优先队列。 优先队列是一种抽象数据类型，表示一组值和对这些值的操作。优先队列最重要的操作就是删除最大元素和插入元素。优先队列里面需要有哪些接口? 1MaxPQ() 未完待续…]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极光推送线下交流分享会]]></title>
    <url>%2F2018%2F03%2F17%2Fshenzhen-xianxia-fenxianghui%2F</url>
    <content type="text"><![CDATA[极光推送线下交流分享会周六参加了异常极光推送组织的线下架构师分享会。当然我只是去学习的。 首先是极光推送的架构师分享极光面对的问题以及架构进化历史。他们呢的架构我就听懂了一个从vm迁移到k8s,docker。另外他们是用c++开发的，采用的是并行处理。实话C++忘记的差不多了。然后他们对于特殊vip客户做了特殊处理。我特别感兴趣一个问题，他说他们有上亿的激活用户，那么怎么给每个用户分标签了？我们在使用机关推送的时候其实是有按男女，按地区，按年龄等来分别推送的。那他们怎么做到在亿级别的用户数上做到很快的处理？他说的方案是请求并行处理；一个请求可以拆分成多个请求，然后结果统一汇总。这个怎么感觉是多线程并发么？ 另外还听到了一个服务化层的概念。比如mq，他说他们所有的请求都是走mq异步的。但是现在并发量大的情况下，mq已经抗不住了。这个时候他们提出mq服务化。就是说mq可能有kafka，rabbitmq，rocketmq。他们可能是在这些mq的上层包装一层，然后底下用这些mq的集群，而对外就是一个mq服务接口层，使用者完全感觉不出来，还是跟以前一样，而这个服务层有路由策略和处理。另外他们的mq是单实例，也就是非集群方式8c16G。 第二位分享的是平安云的架构师，主要讲的是k8s。已经他们的云平台服务搭建，已经在内部的使用。项目先要考虑活下来，才会考虑去优化。这是他分享的一句话。另外会下我还问了一下他关于日志的问题。日志丢包怎么解决，他的回答是切分，然后采用一些策略来做。防止掉大部分。小部分丢包可以自己再采用策略补偿。 第三位分享的是阿里豌豆荚的架构师，很明显，这是一个阿里互联网的气息的人。我第一次听到下级给上级打绩效。这个不得不佩服，他也说了136淘宝的策略，如果两年1，必须淘汰，每年必须有人1。厉害了。然后他分享给我们的是抓好数据结构。熟悉一门新的语言先去看它的api文档，但是如果你熟悉数据结构，你会发现api文档里面其实通用的，因为每个语言用的数据结构还是那些只是实现方式有点不一样罢了。他还分享了一个架构模式进程：mvc-&gt;mvp-&gt;mvvm。性能优化方法面他说先要有个量化下来，他主要是安卓开发推荐我们内存优化的工具是memory，monitor，heap view,aliocation Tracker，MAT。内存泄漏：应用不停的申请新内存；内存溢出：需要一个大内存，实际只有一个小内存;在安卓中使用轻量级的数据结构，不用hashmap的原因是自动装箱，自动拆箱。技术的进化历史：服务业务—&gt; 解决需求反哺业务—&gt; 留存提升业务 作为技术人需要的不但专，而且要博。专而博。 最后的提问环节，不得不说，阿里的影响力那是真的大啊，一堆人提问，一堆人加微信。厉害了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[希尔排序算法]]></title>
    <url>%2F2018%2F03%2F17%2Fshell-sort%2F</url>
    <content type="text"><![CDATA[希尔排序算法这个算法是插入排序的基础上做的优化，它描叙与实现可以看这个插入排序。 希尔排序的思想是使间隔为h之间的元素都是有序的。比如我们说有10个数字，首先我们使用间隔4，那么就是位置为 1，5,2，6,3，7,4，8,5，9；将这些组的相应**位置的值**，我们说的1-9是指的位置，而不是值哦。将这些组进行排序。之后我们再进行第二次分组，也就是将间隔的长度再缩小，假如这里缩小为2，那么第二次的间隔位置组就是：1，3，5，7，9,2，4，6，8`。这样循环比较进行交换之后，我们最终就可以将间隔降为1，这样我们就可以得到一个优化的优化后的插入排序了，也就是常说的希尔排序； 一般来说我们通常将长度设为2的幂除。也就是先用数组长度除2，再除2，再除2；12345678910public static void (int[] arr)&#123; for(int h=arr.length/2;h&gt;0;h=h/2)&#123; for(int i=gap;i&lt;arr.length;i++)&#123; for(int j=i;j&lt;arr.length&amp;&amp;arr[j]&lt;arr[j-h];j-=h)&#123; swap(arr,j,j-h) &#125; &#125; &#125;&#125; 在&lt;&lt;算法&gt;&gt;第四版中看到另一种实现方式： 1234567891011121314151617public static void sort(int[] arr)&#123; int len = arr.length; int h = 1; while(h&lt;arr.length/3)&#123; h=h*3+1; &#125; while(h&gt;=1)&#123; for(int i=h;i&lt;len;i++)&#123; for(int j=i;j&lt;len&amp;&amp;a[j]&lt;a[j-h];j-=h)&#123; swap(arr,j,j-h); &#125; &#125; h=h/3; &#125;&#125; 12345public static void swap(int[] arr,int j,int h)&#123; arr[j] = arr[j]+arr[i]; arr[i] = arr[j]-arr[i]; arr[j] = arr[j]-arr[i];&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 事务处理（包括spring事务管理）]]></title>
    <url>%2F2018%2F03%2F14%2Fjava-transaction%2F</url>
    <content type="text"><![CDATA[Java 事务处理（包括spring事务管理）JDBC 的事务处理Java 中的事务处理有三部分： 自动提交模式 事务隔离级别 保护点 其实实际上java中的事务处理最终依赖的是各数据库的事务处理实现。如果使用的数据库不支持事务，或者提供的数据库驱动程序没有支持事务，那么也是巧妇难为无米之炊。 事务自动提交的方式有：DML（DML），DDL（create），select 查询后结果集关闭，储存过程执行后。 事务隔离级别有：脏读;不可重复读;幻读。 保护点：部分事务回滚;选择性释放。 DriverManager–&gt; Connection –&gt; Statement –&gt; ResultSet –&gt; ResultSetMetaData。 12345678910111213141516171819202122232425Connection connection = null;try &#123; connection = DriverManager.getConnection(""); //将事务的自动提交关系 connection.setAutoCommit(false); /* 业务逻辑（操作数据库）处理 doService(); */ //提交事务 connection.commit();&#125; catch (SQLException e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; //还原自动提交事务 connection.setAutoCommit(true); //关闭connection connection.close(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125;&#125; 保护点，就是我们在处理一段逻辑中，不需要全部回滚回滚当前事务，只需要回滚到当前的保护点就好了： 12345678910111213141516171819202122232425Connection connection = null;Savepoint savepoint = null;try &#123; connection = DriverManager.getConnection(""); savepoint = connection.setSavepoint(); connection.commit();&#125; catch (SQLException e) &#123; if(null!=connection)&#123; try &#123; connection.rollback(savepoint); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; &#125; e.printStackTrace();&#125; finally &#123; try &#123; connection.close(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125;&#125; Spring 的事务实现与原理手受了伤，一个手打字不便。稍等，别忘记了。记录到gtd里面去 今日为2018年3月26日 spring 事务管理有以下一些有点&lt;摘自官网文档16节 Transaction Management&gt;： 接口一致性编程模型，适用于Java Transaction API(JTA)，JDBC，Hibernate，Java Persistence API(JPA),Java Data Objects(JDO) 支持声明式事务管理 编程式事务api接口简单 与Spring的数据访问抽象的完美集成 第一节还有全局事务管理和本地事务管理。还看到了EJB CMT（Container Managed Transaction）.Spring 事务抽象接口为PlatformTransactionManager接口，这个是用到了策略模式，可以看到传递transactionStatus为参数。2018-03-26-20-18-37 很多英文没看懂，明天继续啃。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库查询优化]]></title>
    <url>%2F2018%2F03%2F12%2Fmysql-optimization%2F</url>
    <content type="text"><![CDATA[数据库查询优化通常笛卡尔积的优化很明显，可以用字查询等来做。 查询的优化其实实际上很大部分都是来自索引的优化。我是这么认为的。我们很多时候都会查执行计划的方式来优化语句。而这里面我们看得最多的就是表是否使用了索引。其实如果数量量少的情况下，我们用全表扫描可能会比用索引扫描要好。但是一般情况下还是优化成索引查询比较好。使用的方法就是查执行计划。explain，explain extended。 另外还有一个特别有意思的语句优化： where name like &#39;Abc%&#39;优化成：where name&gt;=&#39;Abc&#39; and name &lt; &#39;Abcd&#39; 本文来自《数据库查询优化器的艺术》-李海翔的读书摘抄。 今天先给自己挖个坑。等之后我再来填。7天之内必填完，今日是3月12日。3月19日填完此坑。 来填坑，今天是3月16日。 性能优化简介谈优化，我们先的知道怎么去测量。我们优化的性能其实实际上指的就是响应时间。主要测量也是看时间花在哪里。响应时间一般指的是执行时间和等待时间。在进行优化之前我们必须先明白什么样的优化才值得我们去优化： 值得优化的查询，如果为了优化1%的查询去浪费20%的人力，那肯定是不合算的； 异常情况要优化，有异常当然要优化了； 丢失时间，执行一些语句，发现花费了莫名奇妙的很长一段时间，而且这段时间也没有任何日志记录；这也有点像异常情况，对待异常一定要明白为什么； 不要相信平均值，毕竟有时候峰值几十秒，然后其它时候几毫秒，这种均值肯定是不能信的 既然要测量，那当然要有可以测量的点了。比如说从慢查询日志，show status，show profile。下面先介绍下几个常用的测量点： 慢查询日志慢查询日志可以通过设置：long_query_time=0来开启。慢查询日志是开销最低，精度最高的测量查询时间的工具。慢查询最大的担心就是消耗大量的磁盘空间，所以建议在只开启某一段时间进行收集即可。推荐一款工具：pt-query-digest。 show profile开始profile的命令为：set profiling=1,开启之后，在服务器上执行的所有语句，都会测量其耗费的时间和其它一些查询执行状态变更相关的数据。当一条查询提交给服务器时候，此工具会记录信息到临时表，然后我们可以使用show profiles;来查询结果。当然你可以使用我下面的这个sql来查询： 1234567891011set @query_id=1;select state,sum(duration) as Total_R, ROUND( 100*SUM(DURATION) /(Select sum(duration) from information_schema.profiling where query_id=@query_id),2) as Pct_R, count(*) as Calls, SUM(duration) /count(*) as 'R/CALL'From information_schema.profilingWhere query_id=@query_idGroup by stateorder by Total_R desc; show status该命令返回一些计数器。既有服务器级别的全局计数器，也有基于某个连接的会话级别的计数器。 另外还有一些命令：show processlist;show global status;这些命令可能需要在实际使用中再查了。不是我想说的重点。 优化过程谈到优化，其实优化的点有很多，从数据库表开始设计，字段类型，字段索引设置都有很多的可优化之处，我们非专业DBA，但是可以多去尝试理解，至少靠自己，是最靠谱的。 数据类型优化选择优化的数据类型比如使用可以正确存储数据的最小数据类型。他们更快，因为更小的数据类型占用更少的磁盘，内存和CPU缓存，并且处理时需要的CPU周期也更少。 使用更简单的数据类型整型比字符操作代价更低，使用内建类型代替字符串存储日期和时间，使用整型存储IP； 避免使用NULL为null的列会使用更多的存储空间；null的列为索引，需要一个额外的存储空间； 另外还有些优化是选择具体数据类型，设计表时候，列不要太多，太多的列需要考虑分表；少关联多个表，多表查询，MySQL最多关联表61个。 索引优化很多时候谈优化，最终都无法避免的就是说到索引优化。索引是存储引擎用来快速找到记录的一种数据结构。在MySQL中也成为key。MySQL中索引是存储在存储引擎中的，不是在服务器中。一般我们说的索引都是B-Tree索引，也就是B-Tree数据结构。B-Tree对索引的列是顺序组织存储的（这里可以看到数据结构的威力了！），索引对值进行排序是按照create table时列的顺序来的。B-Tree 索引适用于全键值，键值范围，键前缀值。 索引的种类有很多，除了上面说的B-tree索引，还有哈希索引，空间数据索引，全文索引。 索引的优点：快速定位到表的位置，减少服务器需要扫描的数量，帮助服务器避免排序和临时表，将随机I/O变成顺序I/O。不过别太迷信索引，在表数量少的情况下，全表扫描更高效。 索引查询： 全值匹配； 匹配最左前缀，多列索引匹配最左边的索引； 匹配列前缀，匹配某一列的值的开头部分； 匹配范围值； 精确匹配某一列并范围匹配其它列； 只访问索引的查询 限制： 如果不按照索引最左列开始查找，则无法使用索引； 不能跳过索引列：比如有三个索引列name,address,age;不能跳过address来查询age的索引。 如果某列为范围查询，其右侧列都无法使用索引查询;如有索引a,b,c;如果在查询中使用了 a between 1 and 15 and b =12,由于a使用了范围查询，a之后的索引b不会生效。 索引优化实例链接地址：http://www.chenzhijun.me/2018/03/23/mysql-optimization-index/]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java性能优化]]></title>
    <url>%2F2018%2F03%2F12%2Fjava%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Java性能优化今日在图书馆看java编程思想，看了一下午，就起身到处走走。没想到走到了《java性能优化》这本书这里，本来不想拿起来的，后来想着为啥不去看看呢。就拿起来读了一些感兴趣的章节。前面一章主要讲了一些优化的大纲，已经四个测试。我记得有一个微基准测试。实在无奈，回到家，具体的是那几个测试也忘记了。当时额外看得只有几章，都是挑的感兴趣的读的。有一章讲的是jdbc，又想起之前别人为我的一个问题：spring的事务是怎么实现的？我当时有点懵逼，其实仔细一想，java中链接数据库不就是通过jdbc来操作的么？那我回答jdbc怎么控制事务的是不是就可以了？可那会脑袋一片空，神之尴尬。java性能优化里面也提到了jpa，Java persistence api。谈到对它进行优化的时候，讲了一些点，预处理语句和语句池。比如使用PreparedStatement而不是直接使用Statement。对一些处理也可以进行批处理。jdbc的事务是基于Connection的。事务的隔离模式有，开销从大到小： 1234Transaction_SerializableTransaction_Repeatable_ReadTransaction_Read_CommitedTransaction_read_uncommited 其实这些就是数据的事务隔离级别，当然java里面还有一个NONE。其实这些事务控制还是根据各数据库提供商来控制的。mysql就提供了上面的四种事务隔离级别。事务也会造成一些问题比如：脏读，幻读，不可重复读。 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted) 是 是 是 不可重复读（read-committed） 否 是 是 可重复读(repeatable-read) 否 否 是 串行化（serializable） 否 否 否 JPA的性能直接受底层jdbc驱动程序的影响，可以尝试减少写入的字段，读取更少的数据或一次读取更多的数据(延时加载)(什么是延时加载？有什么好处？怎么实现？) 事务分为两种：声明式事务管理，编程式事务管理。（spring的事务怎么实现？） JPA 缓存 缓存分为全局缓存和实体管理器缓存，实体管理器中的缓存通常称为一级缓存L1,全局缓存又称为二级缓存。（spring data jpa中是否也是这样实现？） 实体管理器实例都有自己的缓存，它会在本地缓存事务中取得数据，在本地缓存事务中写入的数据。只有在事务提交时，这些缓存的数据才会发送到数据库。 JPA缓存只在通过逐渐去访问实体的时候才有效。实体管理器通过主键查找数据时，会先从L2缓存中找。 任何时候用流都记得用缓存流。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温Java编程思想的一些感悟-20180310]]></title>
    <url>%2F2018%2F03%2F10%2Fthinkinjava-20180310%2F</url>
    <content type="text"><![CDATA[重温Java编程思想的一些感悟-20180310 人的懒惰，是思想上的懒惰 前言今天说个有趣的事情，起床的时间是9点。周末9点起床其实也不算过分，但是吧。拖着拖着在床上又赖到了10点。然后找不到理由再拖下去了。321起床，洗脸刷牙。然后脑袋在思考了：现在要是去大学城吧，10点了。这过去就得半小时吧，然后吃午饭吧，花半小时吧，然后去图书馆路上走个20分钟吧，嗯，好像差不多就12点了。这样下去吧，晚上6点又要吃饭。等会中午说不定还要睡觉。那还去图书馆干嘛。在家里也一样的啊，反正一张大桌子，一个人，很安静。那我先去吃个肠粉当早午饭，吃完回来看书。嗯，出门记得把垃圾扔出去。嗯是的。故事就是这样开始的。当我拿着垃圾扔到楼下垃圾桶。看着太阳这么明媚，天气这么暖和，不知咋地，我竟然生出一种感觉，回家拿书包，去图书馆，而且一分钟也没有思考其它的。直接就又返回来。拿着书包，就出去了。吃完肠粉就去图书馆了。最后得出一个结论：人啊，要想战胜懒惰，先的开始行动。 今日看了《Java编程思想》的多态，接口，内部类。总共花费蕃茄钟5.5个，当然这只是有效的蕃茄中，天知道我分心走神了几次。。。 多态多态，继承，抽象统称为《面向对象三大核心系统》。多态传统上的用法有点像：基类的方法调用，子类的方法实现。它的作用就是消除类型之间的耦合关系。了解多态就得先了解下绑定，绑定分为前期绑定和后期绑定。 前期绑定：默认的绑定方式，方法直接调用，在编译期就能知道。 后期绑定：根据运行时的对象的类型进行绑定。又称为动态绑定或者运行时绑定。 Java中除了static方法和final方法之外其它都是后期绑定。（private方法是final方法）。只有普通方法的调用是多态的，静态方法不能使用多态的。构造器实际上是static方法，不过是隐式的。酒席那个private的方法是final一样，也是隐式的。 动态绑定只有在运行时才知道，因为无法知道它是属于方法所在的那个类，还是属于那个类的子类。 编写构造器的一条准则：尽可能简单的方法是对象进入正常状态，如果可以的话，避免去调用其它方法。 使用继承还是使用组合？推荐使用组合。 123456789101112131415161718192021222324252627282930313233343536class Actor&#123; public void act()&#123;&#125;&#125;class HappyActor extends Actor&#123; public void act()&#123; System.out.print("Happy Actor"); &#125;&#125;class SadActor extends Actor&#123; public void act()&#123; System.out.print("Sad Actor"); &#125;&#125;class Stage&#123; private Actor actor = new HappyActor(); public void change()&#123; actor = new SadActor(); &#125; public void performPlay()&#123; actor.act(); &#125;&#125;public class Demo&#123; public static void main(String[] args)&#123; Stage stage = new Stage(); stage.performPlay(); stage.change(); stage.performPlay(); &#125;&#125; 代码用到了状态模式，使用组合还是继承有一条通用准则：用继承表达行为间的差异，并用字段表达状态上的变化。代码中，通过继承获得了两个不同的类，用于表达act()方法的差异。stage通过使用组合的方式通过change()方法试自己的状态发生改变。状态的改变也产生了行为的改变。 一个子类向上转型成父类，总是安全的。但是父类向子类转型就不一定了，毕竟子类的实现可能有很多，直接强转可能会出错（ClassCastException）。 接口接口的好处有很多，我最大的体会就是，接口中定义方法，子类中各自实现。在项目中我也看过部分代码常量是写在接口中的，工具类是用abstract标明。这种写法我在《effective java》里面看到好像是不推荐这么干的。算了扯回来接口。 谈接口就有点类似抽象类。接口可以看作抽象类的抽象。接口中只有方法定义，新版jdk8支持带默认的方法体了好像要用default。但是实际中，我没用过。。接口中的方法都是public的。接口中声明的域都是static和final的。 策略模式： 12345678910111213141516171819202122232425262728293031323334353637383940class Processor&#123; public String name()&#123; return getClass().getSimpleName(); &#125; Object process(Object input)&#123; return input; &#125;&#125;class Upcase extends Processor&#123; String process(Object input)&#123; return ((String)input).toUpperCase(); &#125;&#125;class Downcase extends Processor&#123; String process(Object input)&#123; return ((String)input).toLowerCase(); &#125;&#125;class Splitter extends Processor&#123; String process(Object input)&#123; return Arrays.toString(((String)input).split(" ")); &#125;&#125;public class Apply&#123; public static void process(Processor p,Object s)&#123; System.out.print("Using processor"+p.name()); System.out.print(p.process(s)); &#125; private static String s = "Disagreement with beliefs is by definition incorrect"; public static void main(String[] args)&#123; process(new Upcase(),s); process(new Downcase(),s); process(new Splitter(),s); &#125;&#125; 根据所传递的参数对象的不同而具有不同行为的方法。Processor对象是一个策略。上面的例子是类版本，改成接口版本： 123456789101112131415161718192021222324252627282930313233343536public interface Processor&#123; String name(); Object process(Object input);&#125;public abstract class StringProcessor implements Processor&#123; public String name()&#123; return getClass().getSimpleName(); &#125; public abstract String process(Object input); public static String s = "Disagreement with beliefs is by definition incorrect"; public static void main(String[] args)&#123; process(new Upcase(),s); process(new Downcase(),s); process(new Splitter(),s); &#125;&#125;class Upcase implements StringProcessor&#123; String process(Object input)&#123; return ((String)input).toUpperCase(); &#125;&#125;class Downcase implements StringProcessor&#123; String process(Object input)&#123; return ((String)input).toLowerCase(); &#125;&#125;class Splitter implements StringProcessor&#123; String process(Object input)&#123; return Arrays.toString(((String)input).split(" ")); &#125;&#125; 适配器模式： 12345678910111213141516171819class FilterAdapter implements Processor&#123; Filter filter; public FilterAdapter(Filter filter)&#123; this.filter = filter; &#125; public String name()&#123;return filter.name():&#125; public Waveform process(Object input)&#123; return filter.process((Waveform)input); &#125;&#125;public class Filter Processor&#123; public static void main(String[] args)&#123; Waveform w = new Waveform(); Apply.process(new FilterAdapter(new LowPass(1.0)),w); Apply.process(new FilterAdapter(new HighPass(2.0)),w); Apply.process(new FilterAdapter(new BandPass(2.0)),w); &#125;&#125; 接口是可以多继承的，使用关键字extends。接口也可以用来实现工厂方法设计模式： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152interface Service&#123; void method1(); void method2();&#125;interface ServiceFactory&#123; Service getService();&#125;class Implementation1 implements Service&#123; Implementation1()&#123;&#125; public void method1()&#123; System.out.print("Implementation1 method1"); &#125; public void method2()&#123; System.out.print("Implementation1 method2"); &#125;&#125;class Implemetation1Factory implements ServiceFactory&#123; public Service getService()&#123; return new Implementation1(); &#125;&#125;class Implementation2 implements Service&#123; Implementation2()&#123;&#125; public void method1()&#123; System.out.print("Implementation2 method1"); &#125; public void method2()&#123; System.out.print("Implementation2 method2"); &#125;&#125;class Implemetation2Factory implements ServiceFactory&#123; public Service getService()&#123; return new Implementation2(); &#125;&#125;public class Factories&#123; public static void serviceConsumer(ServiceFactory fact)&#123; Service s = fact.getService(); s.method1(); s.method2(); &#125; public static void main(String[] args)&#123; serviceConsumer(new Implemention1Factory()); serviceConsumer(new Implemention2Factory()); &#125;&#125; 内部类将一个类的定义放在另一个类的定义内部，这就是内部类。普通内部类，只能放在类的外部层次，不能有static字段。匿名类必须传进必须是final的。 内部类实现工厂方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354interface Service&#123; void method1(); void method2();&#125;interface ServiceFactory&#123; Service getService();&#125;class Implementation1 implements Service&#123; Implementation1()&#123;&#125; public void method1()&#123; System.out.print("Implementation1 method1"); &#125; public void method2()&#123; System.out.print("Implementation1 method2"); &#125; public static ServiceFactory()&#123; new ServiceFactory()&#123; public Service getService()&#123; return new Implementation1(); &#125; &#125; &#125;&#125;class Implementation2 implements Service&#123; Implementation1()&#123;&#125; public void method1()&#123; System.out.print("Implementation2 method1"); &#125; public void method2()&#123; System.out.print("Implementation2 method2"); &#125; public static ServiceFactory()&#123; new ServiceFactory()&#123; public Service getService()&#123; return new Implementation2(); &#125; &#125; &#125;&#125;public class Factories&#123; public static void serviceConsumer(ServiceFactory fact)&#123; Service s = fact.getService(); s.method1(); s.method2(); &#125; public static void main(String[] args)&#123; serviceConsumer(Implemention1.factory); serviceConsumer(Implemention2.facotry); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初级排序算法]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%88%9D%E7%BA%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[初级排序算法约定首先我们约定几个工具方法： 123456789101112131415161718/** 比较a是否小于b，如果a&lt;b返回true*/public static boolean less(int a,int b)&#123; if(a&lt;b)&#123; return true; &#125; return false;&#125;/** 交换数组中i，j的值的位置*/public static void exchange(int[] arr,int i,int j)&#123; int temp = arr[i]; arr[i]=arr[j]; arr[j]=temp;&#125; 选择排序选择排序的思想是在一堆无序数字中，首先找出最小的数字，然后与数组第一个元素交换，然后再在剩下的元素中找最小的数字，与第二个交换。这样以此类推，直到整个排序完成。这个就是选择排序。选择排序的思想就是一直选择最小的那个值。 代码： 123456789101112131415/**选择排序，在传入前请判断arr是否为null*/public static void selectSort(int[] arr)&#123; int len = arr.length; for(int i=0;i&lt;len;i++)&#123; int min = i; for(int j=i+1;j&lt;len;j++)&#123; if(less(arr[j],arr[i]))&#123; min=j; &#125; &#125; exchange(arr,i,min); &#125;&#125; 选择排序的特点，我们可以看到，选择排序每次都是从开始位置，一直到最后的位置，而且每次都需要遍历。所以输入的值关系并没有影响它，比如你输入“1,2,3,4”和“4，1，3,2”所需要的时间是相等的，因为它每次都会起始位置开始比较。数据移动是固定的，跟待比较的值呈线性关系。输入N个值，移动N次。另外有一点，如果一次遍历下来最小值是它本身，那么它还是会和自己做一次交换。选择排序的时间复杂度怎么计算了？我们可以看到比较次数为：(n-1)+(n-2)+(n-3)+(n-4)+…+3+2+1，也就是任意i的位置都需要进行一次交换和（n-i-1）次比较。可以由公式得出为 $ (n-1)+(n-2)+(n-3)+(n-4)+…+3+2+1 ~ N^2/2 $ 插入排序如果你喜欢打牌，那么你肯定就知道插入排序就是怎么样了。比如你抓牌的时候会将抓到的牌按照大小顺序来排列，当你手上的牌是一个已好的顺序时候，下次抓出的牌你会将它插入到相应的位置。这就是插入排序的原理了。在实际中，我们用数组的方式来表示的话，当插入的已有的顺序中间的时间，那么其它位置的牌将需要全部移动一个位置。 1234567891011121314public void insertSort(int[] arr)&#123; int len = arr.length; //为什么从1开始？，第0个位置相当于抓的第一张牌，做基础对比的。 for(int i=1;i&lt;len;i++)&#123; // i之前的数据，可以看作是已经排好序了 for(int j=i;j&gt;0;j--)&#123; if(less(arr[j],arr[j-1]))&#123; exchange(arr,i,j); &#125; &#125; &#125;&#125; 插入排序如果是以下集中情况将会很有效： 数组中每个元素的位置离他们最终元素的位置都不远； 数组中只有几个元素的位置不正确； 一个有序的大数组接一个小数组。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔20180306]]></title>
    <url>%2F2018%2F03%2F07%2Fjust-write-something%2F</url>
    <content type="text"><![CDATA[随笔20180306时间算算是真快啊，确实，转眼毕业快两年了。参加工作已经两年了。很多事情真是感觉像是昨天才发生的一样。这几天静下心来想想自己。每日的计划做的很满，却是非常飘。很多时候我在对各种新技术去了解，去有想法，却忽略了一个很简单的事情，技术的更新特别快，今天有A，明天就有B，如果只是跟着潮流，那么未来的方向是哪里呢？我反思，我这一年里经历了很多，自己也在周末，下班都在弄一些技术相关的新的东西。然而，我却只教会了自己怎么用。没有教会自己为什么。过去的日子已经追不回来，努力的过好当下的日子才是最重要的。2018年立了很多的flag，其实最大的flag就只有一个，那就是-行动。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温Java编程思想的一些感悟-20180305]]></title>
    <url>%2F2018%2F03%2F05%2Fthinkinjava-20180305%2F</url>
    <content type="text"><![CDATA[重温Java编程思想的一些感悟-20180305JIT(just-in-time) 即时编译技术：将程序全部或者部分翻译成本地机器码，程序运行速度因此得到提升。对于hotspot来说，代码每次被执行，都会做一些优化，执行的次数越多，速度也就越快。 java会给所有的默认域做一个初始化，非基本类型对象不初始化值的情况下，默认为null。初始化顺序:static-&gt;{}-&gt;构造器； static的数据指向同一份存储区域，不能用于局部变量。 final对于基本类型，其数值永不改变。final对于引用类型，其引用永不改变。必须在定义final或者在构造器中对final进行赋值。 final类无法继承，方法无法重写或修改。private的方法其实是隐式的final。 compareTo,如果是两个int，最好不要直接返回i-i2，如果是有符号的int类型，一个正数最大，一个负数最大，那么将永远返回负数。所以如果重写compareTo，最好是不要直接返回return i-i2; 集合是fail-fast的，比如： 123list.add(aa);list.iterator();list.add(cc); 需要明确在添加或者修改完容器所有元素之后再获取迭代器。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Java 0228]]></title>
    <url>%2F2018%2F02%2F28%2Feffective-java-0228%2F</url>
    <content type="text"><![CDATA[Effective Java 0228覆盖equals时候请遵守通用规定翻开书，看到了书的第3章-对于所有对象都通用的方法。我们知道，java中一切类都继承自Object，在Object类里面有 两个非常重要的方法：`hashCode()`,`equals()`; 其实Object类里面很多方法的注释非常详细，有一些比较通用的约定。作为我们在重写的时候我们最好是遵守这些约定。 如果我们要覆盖equals方法，我们需要思考几个问题： 我们为什么要覆盖？可不可以不覆盖？各自场景是什么？ 怎么覆盖？ 有什么需要特别注意的？ 覆盖equals看起来非常简单，其实坑有蛮多。最好的避免方式当然就是不覆盖了，这样我们可以看到在Object里面的实现： 123public boolean equals(Object obj) &#123; return (this == obj);&#125; 默认equals比较的就是类只与它自身的相等。在何种情况下我们不需要覆盖呢？书上说有四点： 类的每个实例本质上是唯一的。 不关心类是否提供“逻辑相等”的测试功能。 超类已经覆盖了equals，从超类继承过来的行为对于子类也合适。 类是私有的或是包级私有的，可以确定它的equals方法永远不会被调用。 什么时候需要覆盖，那么肯定就是上面四个的反例或者其它的情况了。一般我们需要比较逻辑上的关系时候，我们可能需要重写equals。这种被称为“值类”。值类也有不需要覆盖的场景:比如单例模式，每个值至多存在一个对象。那么比较值相等的意义就不大了;另一种就是枚举，枚举类型逻辑相同与对象等同是一个意义，因此这两个方式就算不覆盖Object的equals方法也可以。 覆盖equals时候最好也是必须遵守它的通用约定： 自反性，x.equals(x) 返回true 对称性，x.eq(y),y.eq(x) 返回true 传递性，x.eq(y),y.eq(z),x.eq(z) 返回true 一致性，多次调用x.eq(y),都应该返回true 与null进行equals(null)的时候必须返回false 高质量equals方法的建议： 使用==操作符号，查看是否当前比较参数是本身这个对象的引用。如果是返回true。 使用instanceof操作符检查“参数是否是正确的类型”，这个可以帮助我们排序非当前类型的比较，也可以排除null值。 转换instanceof之后的类型为当前this指向的对象的类型。 检查参数中的每个域，是否和该对象中对应的域相等。 对于不是float和double的基本类型可以用“==”比较，另两个调用他们的compare方法(为什么compare方法可以了？)。另外可以将最有可能不一样的域提前比较。 覆盖equals的时候一定要一定要一定要覆盖hashCode()。 那么hashCode()该怎么覆盖了？下次分享。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温Java编程思想的一些感悟-20180223]]></title>
    <url>%2F2018%2F02%2F23%2Fthinkinjava-20180223%2F</url>
    <content type="text"><![CDATA[重温Java编程思想的一些感悟-20180223最近又重新翻开了这本书，翻开第一页，上面的第一句话让我有点感触。 2013年11月25日，购于亚马逊-陈志军 过得真快啊，今天是18年2月23日。岁月绕过谁？莫名的想起一句话：优于别人，并不高贵，真正的高贵应该是优于过去的自己。今天的我，是否又优于过去的我了？ 翻了一下书的目录，上面还有以前写的，哪天哪天要读那几章节。哈，我其实已经忘记我读过的这本书的内容了。 翻了下第一二三章，对象导论，一切都是对象，操作符。 使用java开发，我们通常会说一句话，万事皆对象。什么是对象？对象的好处？继承和组合的区别与联系？java的单继承结构-Object为终结父类。对象的创建和生命期。我们通常说创建一个对象在堆上，然后都是new出来的，那么基本类型的变量的空间又该如何算？也是在堆上么？ 对象的存储：寄存器（无法直接控制），堆栈（RAM），堆（RAM），常量存储，非RAM存储（流对象，持久化对象）。常量存储是将值直接存储在程序代码内部，常量的意思就是表明它是永远不会被改变的。那么基本类型是怎么存放的了？书上说，new的对象存储在堆里，但用new来创建一个小的简单的变量通常不是很搞笑，所以基本类型是特例，基本类型不用new来创建变量，而是创建一个并非是引用的“自动”变量，存储在堆栈中，并且也会将“值”也存在堆栈中。更加具体的解释可以看这篇问答：http://www.iteye.com/problems/75716。堆栈的作用是一个先进后出的结构也就是先分配的内存最后释放，局部变量是存放在堆栈中，所以函数返回之后，局部变量就没有了。 基本类型：boolean，byte(8bit),char(16bit),short(16bit),int(32bit),long(64bit),float(32bit),double(64bit),void 可以确定这些基本类型所占的存储空间的大小是不变的。但是boolean占多少位了？https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html 网上一堆的争论，官网的解释是这个： 1boolean: The boolean data type has only two possible values: true and false. Use this data type for simple flags that track true/false conditions. This data type represents one bit of information, but its &quot;size&quot; isn&apos;t something that&apos;s precisely defined. boolean 只有两个值：true和false。使用它作为真/假表达式的标记，它的数据类型表示的是一个bit的信息，但实际大小确不是精确定义的。 基本类型是有默认值的，但仅仅只限于作为类的成员变量时，java才会给默认值，如果是在一个方法中，如果你直接像下面这样： 1234public void test()&#123; int i; System.out.println(i+1);&#125; 如果像上面这样，i不初始化，就会报编译错误。 static关键字：我们知道，对象的创建需要在执行new之后，才会给对相对分配数据存储空间。但如果我只想给类的某个域或者方法单一分配存储空间或者希望某个方法就算没有创建对象，也能够被调用。这种想法就是，我不想使用new去创建对象(可能对象太大占用太多空)，但是勒，我又需要这个对象里面的某个域或者某个方法。这种时候就可以使用static关键字了。static的意思就是用它来修饰的域或者方法，不与包含它的那个类的任何实例对象关联在一起。所以就算没有创建某个类的任何对象，也可以调用static方法，或者static域。而非static域和方法，必须知道他们一起运作的特定对象。static字段对于每个类来说都只有一份，也就是无论你是创建多个也好，还是只有一个也好，它们用到的那个static域是公有的，都是同一个存储空间。 操作符在java最底层，数据是通过操作符来操作的。java的操作符有：四则运算(算术操作符)，赋值，关系操作符（&lt;,&gt;,&lt;=,==,!=），逻辑操作符号（&amp;&amp;，||，！），自增自减，按位操作（&amp;，|，^,~），移位操作符（&lt;&lt;,&gt;&gt;,&gt;&gt;&gt;（无符号右移）,&lt;&lt;&lt;（无符号左移））。 逻辑操作符号只用于布尔值 按位操作符～，是一元操作符，也就是只需要一个操作数就可以了。所以不能用 ～= 类型转换：高位转地位会失真，所以需要显示强转；地位转高位没事不必显示强转。基本类型中，如果是数据类型小和数据类型大的做运算，结果总是数据类型大的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK-HashMap解析]]></title>
    <url>%2F2018%2F02%2F06%2Fjdk-util-hashmap%2F</url>
    <content type="text"><![CDATA[JDK-HashMap解析jdk 类介绍HashMap 是一个继承Map接口并且是使用哈希表的实现。提供了所有的map操作，并且允许null建和null值。除了非同步方式和允许null键值，它大致上可以等同于Hashtable。影响HashMap表现的有两个原因：初始容量和加载因子。 capacity：容量指的是hash表中的总容量。初始容量在第一次创建hashmap的时候就已经创建好了。load factor: 加载因子，是测量capacity在hash表达到了多满的时候可以自动增长。 当键值对达到了加载因子的量并且超过了当前的capacity，hash表就会重新进行hash（意思是内部数据结构将重建），目的是为了将hash表能够达到差不多两倍的容量。 通常的规则是，默认的加载因子为0.75，它可以在时间和空间上做一个非常好的平衡。如果设置自定义capacity，map里面可能存在多少键值队就需要作为一个考虑因素，以达到最小的rehash操作。如果初始capacity在设置capacity之后还能存下更多的键值对，那么就不会出现rehash操作。 如果事先能大致确认map的容量是多少，那么给map定一个初始化大小是非常可行的，会比让它自己增长更有效率。在hash表中如果出现太多的相同的key拥有相同的hashCode()会降低效率，如果需要改善这种关系，如果键是可以比较的，类可以使用key的比较顺序来进行改善。 HashMap的实现是非线程安全的。在多线程的环境下，必须在外部上进行同步控制。如果外部没有控制，那么需要将map进行包装，也就是使用Collections.synchronizedMap(map)方法，最好是在一开始创建map的时候就进行包装。map的iterators方法是fail-fast的。在iterator创建之后，如果map的结构被修改了，除了iterator的remove方法，iterator会抛出ConcurrentModificationException异常。所以在多线程环境下进行修改，iterator会快速的返回失败。不过也得注意在非同步环境下，迭代器的fail-fast不能得到保证。迭代器抛出这个异常一个非常好的功能，但是不应该依赖它来写程序，如果出现异常那么就应该说明程序有bug。 jdk的源码里面，介绍的非常详细。真的很详细。 分块介绍之前说过map是非线程安全的，这节就单纯的聊一下非线程安全下的map。 一个类我们从哪里来看它了？其实我也不知道，那就干脆从我们使用一个对象开始。我们是先创建对象，给对象设置值，然后使用对象的方法。 创建map通常在创建map的时候我们会使用对象的构造方法，如果有对象有属性，可能还会有带参数的构造方法。所以在创建map之前我们先了解下map有哪些属性？ 通过idea我截取了一个图，在jdk8中是 在jdk7中： 可以看到，map对象里面有一些属性，根据这些属性的定义我们可以猜到大致的意思。 DEFAULT_INITIAL_CAPCITY = 1&lt;&lt;4：默认容量大小，1左移4位，16。记住默认的大小必须是2的幂。MAXIMUM_CAPACITY = 1&lt;&lt;30：最大的容量DEFAULT_LOAD_FACTOR:加载因子，默认0.75Entry&lt;?,?&gt;[] EMPTY_TABLE： 空表Entry&lt;K,V&gt;[] table：长度为2的幂方，并且必要的时候可以改变大小，默认为空表。注意到这个字段是transientsize: 实际map中键值对的数量threshold：下一次表重构的极限值modCount：hash表结构修改一次就增加一次，包括增加减少键值对，内部数据结构重构。主要用来在迭代器中fail-fast 大致的参数就这些了。 然后我们看到构造方法，如下图： 1234567891011public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125; 如果你看构造函数带有Map的构造方法： 1234567public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); inflateTable(threshold); putAllForCreate(m);&#125; 你会发现，同样的它也在里面调用带两个参数的构造方法，所以我认为，这个带两个参数的构造方法可以算作是基本方法，其实想想这算不算代码复用？如果我们可以在平常的开发中开发出了基础方法，那么是否也可以像这样进行包装使用？ 这三个构造方法其实只是包装了一层而已，当然map构造方法进行了一些其它的操作，所以我们需要再看下里面的基础构造方法: 1234567891011121314public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; init();&#125; 这里可以看到，我们的map里面，在使用的时候至少需要进行两个属性设置：initialCapacity初始容量和loadFactor加载因子。 可以看到代码的严谨性：在初始化的时候进行了参数可用性校验，平常我写方法的时候，偶尔会忘记这个，不过最后我会根据sonar扫描一次代码提交。哈哈sonar这点还是可以做到的。 在这个构造方法里面init()是一个空方法。 在map的构造方法里面后面还有两个操作，一个是inflateTable，一个是putAllForCreate。其实可以想到，用map的构造方法是有数据的，不同于其它三个，其它三个都是无数据的。所以我们只需要构建一个map出来就好了。而map的构造方法拥有数据所以在构建map之后我们需要将数据填充到新的map中。一个是扩容的代码： 123456789101112131415161718private static int roundUpToPowerOf2(int number) &#123; // assert number &gt;= 0 : "number must be non-negative"; return number &gt;= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number &gt; 1) ? Integer.highestOneBit((number - 1) &lt;&lt; 1) : 1;&#125;/** * Inflates the table. */private void inflateTable(int toSize) &#123; // Find a power of 2 &gt;= toSize int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity);&#125; 之后的设置初始值就不详细说了。 常用方法常用方法put(key,value)这个方法是从Map接口中继承过来的，我们大致猜测一下，map如果插入了一个键值对，插入之前我们是不是需要先判断一下容量？是不是还需要有个地方存入这个值？在哪里存这个键值对？： 1234567891011121314151617181920212223242526272829303132333435public V put(K key, V value) &#123; //对表容量进行判断 if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; //key的空值处理 if (key == null) return putForNullKey(value); //对key进行hash计算 int hash = hash(key); //hash表中的位置i int i = indexFor(hash, table.length); //在i的位置找到值， for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //条件1：判断当前key的hash与i位置的hash是否一致 //条件2：判断key是否是i位置(entry)的key相等的 //条件3：判断key是否和i位置(entry)的key内容相等 //条件1&amp;&amp;（条件2||条件3） if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; //说明找到了hash的key，将新值替换旧值 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; //map数据结构修改，+1 modCount++; //没有在i的位置找到值，说明这个是新的key，加入到entry addEntry(hash, key, value, i); return null; &#125; 我们再代码中就可以看到之前的问题了，不过代码实现中还做了null值的处理。下面我们一个个来看： 扩容处理：inflateTable(threshold); 这个在上面构造map的时候讲过了。 空值处理：putForNullKey(value); 123456789101112131415private V putForNullKey(V value) &#123; //从table的0位置开始遍历 for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; //找到null的key，替换值 if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null;&#125; 可以看到，这里的的操作是从0位置开始查找的。也就是从一开始遍历，这种情况下可以想到一个O(n)的时间复杂度。所以，如果是1.7，null键最好不要存在。 在代码中可以看到存键值对的地方是Entry，关于Entry我们现在只需要知道它是一个单链表的数据结构就好了，之后我们专门讲解一下Entry和hash。 讲完1.7。我们看看1.8中的put方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //判断是否是空表 if ((tab = table) == null || (n = tab.length) == 0) //空表，重构表 n = (tab = resize()).length; //如果原始表中不存在数据，直接新建一个节点，p为表中key的数据节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; //原始表中存在数据 Node&lt;K,V&gt; e; K k; //找到了key节点hash的值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果该节点是一个TreeNode else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果节点存在，修改节点里面的值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //增加修改次数 ++modCount; //size+1之后的大小大于极限值 if (++size &gt; threshold) resize(); //空方法 afterNodeInsertion(evict); return null;&#125; jdk1.8中的put方法，实现好像有些优化。加入了TreeNode，之后这块我补上，整体的流程是和1.7一致的。 常用方法get(key)该方法在1.7中的实现是：123456789101112131415161718192021222324252627282930public V get(Object key) &#123; //判断null，如果是null遍历循环找出值 if (key == null) return getForNullKey(); //找到存了key的entry Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; //如果是空表，直接返回null if (size == 0) &#123; return null; &#125; // 计算hash值 int hash = (key == null) ? 0 : hash(key); //循环遍历找到值，从hash值开始找 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null;&#125; 1.7中还是很好理解的，这里主要是hash值计算，之后我们重点讲解。 在1.8中，get方法： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //表不为空 //表的内容长度不为0 //在(n-1)&amp;hash的位置不为null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个位置?为什么判断第一个位置，是和node的数据结构有关系么？是不是加入的时候都是加入第一个位置？ if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 注意到8中总是计算判断第一个node，为什么？之后了解清楚了再来详细解析 常用方法containsKey(Object),containsValue(Object)containsKey(Object)的源码中我们可以看到也是调用了getEntry方法： 123public boolean containsKey(Object key) &#123; return getEntry(key) != null;&#125; containsValue(Object)的源码中我们可以看到，基本类似，也是循环链表，然后获取到值之后比较value： 1234567891011public boolean containsValue(Object value) &#123; if (value == null) return containsNullValue(); Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false;&#125; 常用方法keySet(),values(),entrySet()这几个方法是指代的键,值遍历，以及键值对遍历。 常用方法size()，内部方法resize()这里就需要主要注意resize()方法是default的，也就是同一个包下面才可以调用。 size()方法就是为了获取到map里面真实的实例数量，返回的是size属性。 resize()方法在每次达到极限界值的时候会自动调用： 12345678910111213141516171819202122232425262728293031323334void resize(int newCapacity) &#123; //获取原始表的容量值 Entry[] oldTable = table; int oldCapacity = oldTable.length; //原表容量值等于最大值，可以直接返回了。 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新表 Entry[] newTable = new Entry[newCapacity]; //填充新表数据 transfer(newTable, initHashSeedAsNeeded(newCapacity)); //替换旧表内容 table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125;void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; Entry,hash,hashCode(),size,capacity,threshold前面我们讲过一些常用的方法，put，get，size等，这些方法其实都是有一定关联的，put前是不是应该先判断容量是否够？不够该怎样，够该怎样。其实就是这样，就像流程图一样，只不过在具体的实现上面我们会有不同的方式。使用不同的数据结构来做优化。 这里我们需要讲一下Entry，hash，hashCode(),size,capacity,threshold他们代表的意义，以及作用。 如果说map是存储键值对，那么键是怎么存？值又是怎么存？两者怎么关联起来了？回过头我们思考这些问题，最好的方式就是看看put方法是怎么运作的： jdk1.7: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public V put(K key, V value) &#123; // 判断空表，然后扩容 if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; //判断nullkey if (key == null) return putForNullKey(value); // 重点。这里计算了key的hash int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125;/** * Retrieve object hash code and applies a supplemental hash function to the * result hash, which defends against poor quality hash functions. This is * critical because HashMap uses power-of-two length hash tables, that * otherwise encounter collisions for hashCodes that do not differ * in lower bits. Note: Null keys always map to hash 0, thus index 0. */ /** 获取hash 值，对于hash结果进行重写，主要是为了防止低质量的hash计算方法，hashmap使用2的幂方长度的hash表，在低位容易遇到同样的hashcode冲突 Null 为key的hash是0 */final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 实际上它使用的是的是Entry[] table 数组，而Entry是一个链表的形式。 123456static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;&#125; hashCode方法是一个本地方法，hash算法实在key的hashCode上计算的，这里有两篇文章，感觉很好，我就不敢多说了：http://www.cnblogs.com/xiongpq/p/6175702.html，http://blog.csdn.net/fjse51/article/details/53811465； size是指的map实际中存在了多少键值对，也就是实际的实例； capacity是指这个map的总容量； threshold是一个极限值，当达到了size达到了极限值的时候就会自动扩容(2的幂次方)，threshold的计算为capacity*load factor。 在jdk8中感觉变化太大了，下次要单独讲讲jdk8。另外还需要讲解一下在多线程下的map该怎么操作。 疑问待解决1：在put的时候，如果是重复键，再第二次的时候就只是替换了，那么entry链表有什么用？为什么这么设计？ 2：hash的计算方式？hashCode计算方式，什么是hashCode？那些位运算是怎么做的？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate 校验参数]]></title>
    <url>%2F2018%2F01%2F31%2Fhibernater-validate-properties%2F</url>
    <content type="text"><![CDATA[Hibernater Validator 校验参数使用方式如果我们使用spring mvc 那么肯定知道在方法中我们可以使用注解对参数进行校验： 12345@PostMapping("/user)public void addUser(@Valid User user)&#123; User userP = user; return ;&#125; 这个时候通常是在User对象里面使用一些注解来判断如：@NotNull,@NotEmpty。这些bean验证方法是遵循JSR303和JSR380规范的，目前的情况可以去这里查看详情：http://beanvalidation.org/2.0/。 实现规范的这些中有一个包是Hibernate Validator，这个包不是我们的ORM框架，可能是orm太出名了，以致于提到Hibernate大家都会想到SSH的Hibernate ORM，其实Hibernate Validator是一个非常完善的bean Validator。如果我们不想自己去实现一套，其实是可以引入这个包的。开源社区遇到的情况肯定会比我们自己要多。而且它也支持我们自定义注解来进行校验。 在我们的系统中，是无法采用@Valid这种方式的，因为我们有一些特别的操作处理所以不能使用它。如果你想自己使用的话，可能可以像我这样，写一个Validtor工具类，然后在需要的地方调用： 123456789101112public class ValidatorUtil &#123; private static final Validator VALIDATOR = Validation.byProvider(HibernateValidator.class).configure().failFast(true).buildValidatorFactory().getValidator(); public static &lt;T&gt; void validate(T obj) &#123; Set&lt;ConstraintViolation&lt;T&gt;&gt; validateResult = VALIDATOR.validate(obj); if(!validateResult.isEmpty())&#123; System.out.println("message:"+validateResult.iterator().next().getMessage()); System.err.println("messageKey:"+validateResult.iterator().next().getMessageTemplate()); throw new RuntimeException(String.format("参数校验失败:%s", validateResult.iterator().next().getPropertyPath().toString()+validateResult.iterator().next().getMessage())); &#125; &#125;&#125; 之后你可以在属性上面这样定义： 如果需要使用校验的，只需要使用 1ValidatorUtil.validate(obj); 如果有异常它会直接抛出来。 自定义注解如果在某些场景下，提供给我们的注解不够用，那我们是否可以自己进行扩展了？ 好的架构，是允许做扩展的。validator也可以支持自定义注解。自定义注解详解在这里自定义验证规则注解。总的来说就是三步： 创建一个注解，并且定义一些校验规则； 继承Validator； 定义默认的error message 现在我们尝试定义一个自定义注解： 准备，定义一个枚举，接下来用到： 1234567891011package com.chenzhijiun.validator.customer;/** * @author chen * @version V1.0 * @date 2018/1/30 */public enum CaseMode &#123; UPPER, LOWER;&#125; 创建一个注解:CheckCase： 123456789101112131415161718192021222324252627package com.chenzhijiun.validator.customer;import javax.validation.Constraint;import javax.validation.Payload;import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.*;import static java.lang.annotation.RetentionPolicy.RUNTIME;@Target(&#123; FIELD, METHOD, PARAMETER, ANNOTATION_TYPE, TYPE_USE &#125;)@Retention(RUNTIME)@Constraint(validatedBy = CheckCaseValidator.class)@Documentedpublic @interface CheckCase &#123; String message() default "&#123;org.hibernate.validator.referenceguide.chapter06.CheckCase." + "message&#125;"; Class&lt;?&gt;[] groups() default &#123; &#125;; Class&lt;? extends Payload&gt;[] payload() default &#123; &#125;; CaseMode value();&#125; 继承Validator,并且将实现自己的规则逻辑 123456789101112131415161718192021222324252627282930313233package com.chenzhijiun.validator.customer;import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;/** * @author chen * @version V1.0 * @date 2018/1/30 */public class CheckCaseValidator implements ConstraintValidator&lt;CheckCase, String&gt; &#123; private CaseMode caseMode; @Override public void initialize(CheckCase constraintAnnotation) &#123; this.caseMode = constraintAnnotation.value(); &#125; @Override public boolean isValid(String object, ConstraintValidatorContext constraintContext) &#123; if ( object == null ) &#123; return true; &#125; if ( caseMode == CaseMode.UPPER ) &#123; return object.equals( object.toUpperCase() ); &#125; else &#123; return object.equals( object.toLowerCase() ); &#125; &#125;&#125; 可以看到，我们这里的主要逻辑就在isValid中处理的。 定义error message ，这个错误提示在哪里定义了？我们可以看到在org.hibernate.validator.resource下有很多的properties文件，这是因为Hibernate做了国际化处理。 如果我们需要重写，只需要将其中的中文的属性文件进行重写放到resource目录就可以了。在定义的注解中我们可以将message指定为我们复制的properties文中的key，这样就能完成error message的自定义了。 源码：https://gitee.com/chenzhijun/validator]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Validator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写属于自己的springboot-starter]]></title>
    <url>%2F2018%2F01%2F31%2Fhow-to-build-springboot-starter%2F</url>
    <content type="text"><![CDATA[编写属于自己的springboot-starter最近手痒，想实现一下自己的starter，感概于springboot的starter导入方便，要是能有一款自己的starter，那样多么好，比起之前的maven导入方式，好像更有意思。主要可以加重对springboot的自动配置装载的理解。 实现过程starter的简易目录如下，”autoconfiguration”,”domain”,”service”,”META-INF”： 这几个包的作用如下：domain一般放置配置的参数，比如我们通常在application.properties中用到的xxx.yyy=value,其中xxx.yyy就是在这里配置的,前面的xxx我们一般指前缀，用以标识某个类级别，后面的yyy一般会是指代属性，如果是驼峰标识的属性在properties文件中，默认为-连接。其中前缀的定义方式使用注解@ConfigurationProperties service服务的实际配置使用，我们获取到参数之后，总要将参数用起来，那么参数配置才有价值，而这些都是在service中做处理，也就是业务处理。所以可以想象的到，在service类里面，我们肯定需要一个domain的类来接收配置的参数。 autoconfiguration包其实就是我们的自动装载配置了，它需要告诉系统我们要让哪个配置类自动装载，spring的ioc要管理bean，前提就是这个bean要被加载到了容器中，所以我们需要注入配置类(domain)使用@Configuration指明当前类是配置类，使用@EnableConfigurationProperties指明快速注册到spring中的类。 另外就是resource下的META-INF包下的spring.factories文件，这里说的是元信息，也是指明我们的配置启动类是在哪个目录，在其它的spring项目里面也可以看到这个文件。 现在我们看下pom文件我们导入的包： 我在这里加入了configuration-processor包，这种情况下能够方便我们在配置文件中使用我们自定义的前缀，然后进行提示。 注意到我这里有些jar包引入的时候使用optional，这种方式声明的包不会让jar包顺延，也就是在使用方我们仍需导入这个包才能使用。]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迟到的总结]]></title>
    <url>%2F2018%2F01%2F18%2Fconclusion-2017%2F</url>
    <content type="text"><![CDATA[转眼2017已经过去了，甚至2018也已经过去了18天。如果说今天来做一个2017的总结，我想这是扯的。最近需求已经完了，等资金账户在app的ui弄完，今年到年底过年的需求就只剩下维护了。这些天感触挺多，最大的一个感触就是2018，今年注定不是一个平静的简单的一年。 去年的一年时间，我从2月份过完年提交辞职信，其实我心里是没谱的。很大的情况是我不知道我该怎样去做。那个时候的我一部分是因为冲冠一怒为红颜，年少的时候，总是有哪股冲动。就这样我辞职了第一份工作，其实我在易宝的日子还是很平凡吧。没有太多的压力，也没有太大的动力。如果说一个人的成长只能应该跟自己有关，我觉得是扯的。它还跟周边所处的环境有关。在易宝，我很感谢很多人。李旦恒，我的技术总监，他的知识面无论是广度和深度都让我对自己有一些重新的认识，相对于他，我简直就是小学在完过家家。邓昌坤，从京东走出来的我们teamleader。他是一个很棒的人，在他的眼里，从来没有办不到的事。这也在以后告诉我，我也能行。真的，那段时间天天跟他坐一辆车回去，很多问题，心里的想法都是他给我解惑。有时候我想想，我真的是“傻”人有“傻”福。邓辉，我在易宝的第一个导师。当我们大家都拿着mac的时候他用着几年前的thinkpad，总让我觉得有股大神的范，其实实际上就是一个逗逼，但是又活得很真实。我觉得逗逼的人，其实心里有时候也会有很多的委屈。笑对也许是最好的方式。汪阳，汪大师。我的第二个导师，我现在有时候看不进书的时候总会回想起他那句话，从前我在新浪的时候，那群哥们就是变态，只有有时间就是啃书，妈的。而且在我一次次遇到问题，搞不定的时候，他总是那么轻轻一瞥，然后说了个这还不简单，巴拉巴拉就弄完了。还有产品天阳，薛雨婷，很多时候一些冷段子就是从他们哪里来的～～。还记得在北京，易大师，胡根深，还有测试bug的曾英姐。很多时候想想如果最舒服的时候，应该就是那段时间了，我来深圳了，离开了大家，联系也少了，但我不会忘记，曾经，我们是朋友，战友，亲爱的人。祝福大家。 2月辞职，3月进klook，刚进客路的第一个月吧，公司拿到了红杉B轮投资，3000万$。现在听说有拿了C轮了，一个很不错的公司。在这里我体会到的是国外的开放式的工作方式和环境。只不过很可惜，如果是用java我想我会一直待下去。但是公司用go当然它也很好，只是目前的我不合适，因为我刚毕业，根基未稳。这可能就是农村孩子的尿性，无法，也没有胆量和后路去拼一把。在客路我学知识的能力感觉是以前的几倍，为什么了？因为啥也不懂。从java转向go，我根本就是一头蒙，所以只要是非工作时间，非吃饭时间，我基本都在看go。现在想想那段时间竟然也是我最充实的时间。因为实在是没有时间和精力去忙其它的了。在客路我学了go，docker。另外还有部分mysql调优。还维护着一个老的ssh项目。这里的人很棒。我长到这么大，这次算是第一次从家里走出省。对，是真正意义上一个人走出省。在客路，我也见识到了自己的渺小，真的是那么渺小。这里很多人都是留学毕业回来，你无法想象，一个一本大学毕业的学生，竟然第一次有了一种不好意思将大学名字说出来的感觉。在长沙我是很自豪的说出来的。不出来见世面，不知道自己的渺小。心中的舞台竟然是那样的小。在客路我经历了开放式办公，所谓的弹性时间办公。其实我也觉得这是一种变相的压力。因为老大不走，底下开发人员就算没有开发任务也不能走(毕竟你也不好意思走)。然后有一次，我走的比较早，在路上遇到同事，他们调侃了一句，xx工作是不是太饱和了，这么早下班。那会我真的是挺尴尬的。因为我回家其实也是看书，或者做一些其它的事。就是想换个环境提高自己。。所以针对这种情况我还是无法理解～～真的。不过这里的扁平话是真的，我们从来没有叫过老大xx总，而且叫他的英文名。甚至遇到公司C位的人，你都可以很直接的跟他们开玩笑，当然，在没有外人的情况下，总之内部是真的扁平，你可以反驳老大，也可以和leader开玩笑，他可能就是你的朋友，而不是说上下级。作为“艺术家”的我，真的很讨厌国企风格的那种上下级关系。当然，这一切都得有个开明的leader。很幸运，客路是的。所以他们成功我觉得是很正常的，火车头好，火车自然能齐心往下越开越远。当然在这里还是有很多的开发压力，不过对于我来说，很享受，有压力的我，我才感觉到我还活着。 后来因为一些私事兜兜转转回长沙，但是后来又一次来深圳。这其中自己的经历算是丰富了。 来深圳后，进入了现在的公司，半扁平化管理，也是一家正在转型的农业行业公司。中农网。当初选它，一方面，是因为自己想为农业行业做点事。作为农民出生，我知道，被农民贱卖的很多大自然的礼物在城市是有多值钱甚至城市的人还求之不来，而且农村的人太穷了。能让他们多一些收入，我感觉那也是一个值得的牛逼的事情。另外选择中农网，其实就是觉得这名字好，中农啊，中国农业～～～我靠，那会感觉瞬间高大上～～～。不犹豫就来了。现在的感受，说好也好，说不完善的地方也有，毕竟在转型中。很多东西都无法完全的实实在在的落地。但是今年年会，明显感觉到了ceo的压力。也感觉到了，今年公司战略的压力，技术中心，今年注定不会是风平浪静。 2018,不管怎样，我陈志军从来不会退缩。我压抑自己，不是不想爆发，而是我知道，现在的我还不足，太多的不足。3年不鸣，一鸣惊人；3年不飞，一飞冲天。但是蜕变注定不会顺利，前路还很漫长，也很坎坷。加油。]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Logstash发送异常邮件]]></title>
    <url>%2F2018%2F01%2F09%2Flogstash-send-email%2F</url>
    <content type="text"><![CDATA[使用Logstash发送异常邮件前端时间我们讲了如何使用elk搭建日志系统,以及如何使用Docker搭建ELK日志系统。虽然我们可以不用再去日志服务器找日志了，但是这样也有问题，我怎么知道什么时候会出现异常，不出现异常我也没必要去kibana查日志啊。 今天我们就要解决这个问题。当然解决的方式比较简单。如果有大神有更好的方式欢迎一起分享。 使用Logstash发送邮件我们使用的是Logstash来发送邮件，网上我也搜了elastalert,但是感觉又多了一个服务，又要多去维护一个服务。后来发现logstash自带了邮件发送功能，那就直接用logstash就好了。 非常的简单易用，在logstash.conf中增加如下配置： 12345678910111213141516171819202122232425262728293031323334353637input &#123; beats &#123; host =&gt; &quot;localhost&quot; port =&gt; &quot;5043&quot; &#125;&#125;filter &#123; if [fields][doc_type] == &apos;order&apos; &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; %&#123;LOGLEVEL:level&#125; %&#123;JAVALOGMESSAGE:msg&#125;&quot; &#125; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] index =&gt; &quot;%&#123;[fields][doc_type]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125; if &quot;ERROR&quot; == [level] &#123; email &#123; to =&gt; &quot;5228******@qq.com,chen****@163.cn&quot; cc =&gt; &quot;email_chen****@163.com&quot; via =&gt; &quot;smtp&quot; subject =&gt; &quot;标题，ERROR: %&#123;[fields][doc_type]&#125;项目出现异常&quot; htmlbody =&gt; &quot;消息主体：%&#123;message&#125;&quot; body =&gt; &quot;Tags: %&#123;tags&#125;\\n\\Content:\\n%&#123;message&#125;&quot; from =&gt; &quot;email_chen****@163.com&quot; address =&gt; &quot;smtp.163.com&quot; username =&gt; &quot;email_chen****@163.com&quot; password =&gt; &quot;*****&quot; # pop3密码或者登陆密码 &#125; &#125;&#125; 主要是增加了下面这段 1234567891011121314if &quot;ERROR&quot; == [level] &#123; email &#123; to =&gt; &quot;5228******@qq.com,chen****@163.cn&quot; cc =&gt; &quot;email_chen****@163.com&quot; via =&gt; &quot;smtp&quot; subject =&gt; &quot;标题，ERROR: %&#123;[fields][doc_type]&#125;项目出现异常&quot; htmlbody =&gt; &quot;消息主体：%&#123;message&#125;&quot; body =&gt; &quot;Tags: %&#123;tags&#125;\\n\\Content:\\n%&#123;message&#125;&quot; from =&gt; &quot;email_chen****@163.com&quot; address =&gt; &quot;smtp.163.com&quot; username =&gt; &quot;email_chen****@163.com&quot; password =&gt; &quot;*****&quot; # pop3密码或者登陆密码 &#125;&#125; 我们只对ERROR级别的日志进行发送邮件，这里用了if条件语句。如果你看过之前的两篇文章，我想这里你是很容易就能弄懂的。当然这种方式不一定很好，如果你有更好的想法，欢迎交流。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>Logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用CURL发送http请求]]></title>
    <url>%2F2018%2F01%2F09%2Fuse-curl-to-http-request%2F</url>
    <content type="text"><![CDATA[使用CURL发送http请求之前都是postman发送数据，但是在服务器上有时候没法用postman连接，其实挂VPN是可以的，但是为了显示b格(好吧，其实是被逼而已)使用curl发送http请求。 其实curl可以做的事情非常多，我们这里仅仅只是用到了一小一小小部分。 发送get请求：curl localhost:8888/path/to/api 发送post请求带json数据： 1curl "http://localhost:8888/path/to/api" -H "Content-Type: application/json" --data '&#123;"pageNumber":1,"pageSize":10&#125;' 当然还有一招，你可以使用火狐的控制台，查看网络，选中接口，复制为CURL。嘿嘿~~~]]></content>
      <categories>
        <category>CURL</category>
      </categories>
      <tags>
        <tag>CURL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Swarm 实践的一些经验总结]]></title>
    <url>%2F2018%2F01%2F08%2Fdocker-swarm-practice%2F</url>
    <content type="text"><![CDATA[Docker Swarm 实践的一些经验总结最近弄了一下Docker Swarm。在公司已经把我拉入了黑名单，因为只要我已启动docker公司的mysql就得挂。我也是好奇，正想找问题原因。服务器的密码就已经改了，我是没有机会了。不过在改密码之前，我还是获得一些宝贵的经验，这个我觉得很重要，可以记录下来，自己以后再踩坑的时候可以自己参照下。 环境准备ubuntu机器一台，centos一台，都安装了docker。docker的版本是Docker version 17.12.0-ce, build c97c6d6。4个spring boot项目：eureka，warehouse，web，booksrv。MySQL使用的是另外一台机器上的安装在hosts(非容器化)。eureka做为注册中心，启动了两个示例。部署方式使用的docker stack。 也有直接使用了docker service。使用了网络overlay，看了网上很多文章，对于网络不太了解就用了原生的，这样方式至少容易理解。创建了overlay网络springcloud-overlay。还是用了java:8-jre-alpine镜像 做了些什么事情实现了两台机器上的docker容器连通，U机器上的容器，能够访问C机器上的容器。这说明服务容器化是可以实现的。当然我说的是废话，google以及一系列大公司已经完成了k8s的线上实践了。不过我们还是比较小，经验不丰富，所以采用最简单的方式，先跑一下，看能否跑通，之后再说下一步。 实际过程1. 踩的坑一开始我是想通过在两台机器上各自暴露eureka端口，然后让eureka互相注册，而在每台机器上都启动容器服务并且注册到每台机器eureka上，这种方式理论模型如下图： 我们在eureka上是看到了这些服务都有注册到了，而且可以明显看到服务的ip地址是：172.17.0.0/16 区间的。貌似一切很正常，然后我们尝试访问服务，在每个服务日志中基本都是NO instance XXX Exception,不是在eureka已经注册了么，为什么还没有了？eureka应该是根据ip地址加端口来找服务的，这种情况下，如果A服务找到D服务，发现注册中心有注册地址ip，找到了ip地址后（不清楚这里是eureka转发，还是eureka将地址发给A，A再请求），但是根据找到的ip去请求服务，也就是在A服务上去找D的ip地址，因为D在Centos服务器上，所以根本不可能到达。那么请求又怎么可能知道了。明显两者应该在不同的。(突然感觉不对劲，如果两个eureka不能相互注册，注册中心又怎么可能启动多个。。。这里待定，感觉有问题。)再回首想一下，嗯，没问题，我们使用bridge网络的时候，是内网。没错，是内网，我们一般使用eureka都是公网，至少也是两台hosts能够相互连通，在同一个网络下。但是使用docker默认的网络是内网，A机器是无法访问B机器的172.xxx的网络的。 2. swarm方式在网上找了一圈资料，有在阿里云找到的springcloud项目，并且使用eureka的注册方式，但是他们基于阿里云网络vpc，对于我们不合适，我们是自建机房，尽管没有运维人员去管理。阿里云的方式行不通之后，后来找到swarm，因为我想总有人有这方面（跨主机容器通信）的需求的。刚开始找到了一些是k8s,觉得学习成本有点大，我只是个小java开发，还没强大到那个地步。。。。后来发现swarm好像也可以。一想，swarm毕竟是docker自带的，学习成本应该不大，本且之前也有接触，果断就用swarm做测试。 2.1 创建overlay网络swarm 其实管理的就是docker容器，一开始我们使用docker swarm init.然后让它称为manager，其它的docker容器只要join就可以了。这点还是很方便。 12345docker swarm initdocker swarm join-token managerdocker swarm join-token worker 这种方式就简单的构建了一个swarm了。然后我们首先创建网络。 1docker network create -d overlay springcloud-overlay 2.2 准备镜像创建网络后，就开始准备相应的镜像和Dockerfile。根据每个人的环境不同可以准备相应的Dockerfile： eureka.Dockerfile: 主要作为注册中心用 12345FROM java:8-jre-alpineVOLUME /tmpADD ./*.jar app.jarRUN sh -c 'touch /app.jar'ENTRYPOINT ["java","-jar","/app.jar"] booksrv.Dockerfile: 做一个无需连接数据库的服务提供者 12345FROM java:8-jre-alpineVOLUME /tmpADD ./*.jar app.jarRUN sh -c 'touch /app.jar'ENTRYPOINT ["java","-jar","/app.jar"] warehouse.Dockerfile：连接数据库的服务提供者 12345FROM java:8-jre-alpineVOLUME /tmpADD ./*.jar app.jarRUN sh -c 'touch /app.jar'ENTRYPOINT ["java","-jar","/app.jar","--cacheType=single"] web.Dockerfile：客户端调用booksrv和warehouse服务的接口 12345FROM java:8-jre-alpineVOLUME /tmpADD ./*.jar app.jarRUN sh -c 'touch /app.jar'ENTRYPOINT ["java","-jar","/app.jar"] 在不同的文件夹下将dockerfile文件准备好之后，我们可以使用docker build -t chenzhijun/imagename .；（chenzhijun/iamgename可以自定义） 2.3 准备yml文件我们准备好镜像文件之后，可以使用docker stack的方式部署服务。 eureka.yml 123456789101112131415161718192021222324252627version: '3'services: eureka1: image: chenzhijun/eureka networks: springcloud-overlay: aliases: - eureka ports: - "1111:1111" environment: - ADDITIONAL_EUREKA_SERVER_LIST=http://eureka2:1111/eureka/ eureka2: image: chenzhijun/eureka networks: springcloud-overlay: aliases: - eureka ports: - "1112:1111" environment: - ADDITIONAL_EUREKA_SERVER_LIST=http://eureka1:1111/eureka/networks: springcloud-overlay: external: name: springcloud-overlay service.yml 1234567891011121314151617181920212223242526272829version: '3'services: web: image: chenzhijun/web networks: - springcloud-overlay environment: - EUREKA_SERVER_ADDRESS=http://eureka1:1111/eureka/,http://eureka2:1111/eureka/ ports: - "8888" bookservice: image: chenzhijun/booksrv networks: - springcloud-overlay environment: - EUREKA_SERVER_ADDRESS=http://eureka1:1111/eureka/,http://eureka2:1111/eureka/ warehouse: image: chenzhijun/warehouse networks: - springcloud-overlay environment: - EUREKA_SERVER_ADDRESS=http://eureka1:1111/eureka/,http://eureka2:1111/eureka/ networks: springcloud-overlay: external: name: springcloud-overlay 我们可以使用docker stack deploy -c eureka.yml(service.yml) eureka(service)。之后可以使用docker stack ls查看启动了哪些服务，使用docker stack services eureka(service)来查看具体的信息。 这个时候，你可以使用docker ps来看机器上启动了哪些服务，然后使用docker logs -f containerid来看输出的日志记录。 嗯基本上就可以看到相应的输出。。 docker swarm的另一种服务创建方式： 1234567docker service create --name warehouse1 \ --network springcloud-overlay \ --mount type=bind,source=/home/user/jar/notice/warehouse-server.jar,destination=/user/share/jar/app.jar \ --env EUREKA_SERVER_ADDRESS=http://eureka1:8761/eureka/,http://eureka2:8761/eureka/ \ --env MYSQL_SERVER_ADDRESS=mysql:3306 \ -p :30021 \ java:8-jre-alpine java -jar /user/share/jar/app.jar --cacheType=single 一些常见的命令: 12345678910111213141516171819docker stack --hlep;docker stack ls;docker stack deploy -c xxx.yml xx-name;docker stack services xx-name;docker service --help;docker network --help;docker run ;docker --help; docker service create --name warehouse1 --network springcloud-overlay --mount type=bind,source=/home/user/jar/notice/warehouse-server.jar,destination=/user/share/jar/app.jar --env EUREKA_SERVER_ADDRESS=http://eureka1:8761/eureka/,http://eureka2:8761/eureka/ --env MYSQL_SERVER_ADDRESS=mysql:3306 -p :30021 java:8-jre-alpine java -jar /user/share/jar/app.jar --cacheType=single]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从命令行参数获取Shell脚本参数]]></title>
    <url>%2F2018%2F01%2F08%2Fshell-get-param-from-console%2F</url>
    <content type="text"><![CDATA[从命令行参数获取Shell脚本参数很多时候我们写完脚本都需要指定一些变量，这些变量可能根据实际的环境而值不同。所以如何在 shell 脚本中接收到不同的变量值了？ 获取参数shell 脚本获取参数的方式很多，我只记录我用过的一种：filebeat.sh 12345678910111213141516171819202122#!/bin/bashif [ ! -n "$1" ] ;then echo "请输入日志文件目录,比如/path/to/xxx.log,输入/path/to:" exit 1;fiecho "日志文件目录：$1" #log_pathif [ ! -n "$2" ] ;then echo "请输入项目名称,比如customer,notice ...:" exit 1;fiecho "项目名称:$2" #log_typecd $(pwd)/filebeat# copy a filebeat.yml to filebeat_log_type.ymlcp config/filebeat.yml config/filebeat_$2.ymlsed -i "s#PATH#$1#g" config/filebeat_$2.ymlsed -i "s#DOCTYPE#$2#g" config/filebeat_$2.ymlnohup ./filebeat -e -c config/filebeat_$2.yml &gt;&gt; logs/$2.log &amp;tail -f logs/$2.log shell 文件都是用#!/bin/bash作为第一行，我们在 filebeat.sh 中获取两个参数，如何使用这个脚本了？sh filebeat.sh param1 param2 param1 我们在脚本中使用$1获取，param2 我们在脚本中使用$2获取，以此类推。 如果要在文件中替换某个特殊文字或者字符如果值中带有/path/to/file,这种带有/的字符，在脚本中可以使用#来转义。之前用的是sed -i &quot;s/path/$1/g&quot; config/file.yml,后来改成sed -i &quot;s#path#$1#g config/file.yml。 参考文档：http://www.jb51.net/article/56549.htm https://www.jianshu.com/p/d3cd36c97abc http://blog.51cto.com/w55554/1223870]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker搭建ELK日志系统]]></title>
    <url>%2F2017%2F12%2F27%2Felk-docker%2F</url>
    <content type="text"><![CDATA[使用Docker搭建ELK日志系统之前用本地版本安装了ELK之后，就没有再去弄它了。年底没那么忙，心里一直惦记，所以最近又开始折腾了。去elastic官网看一下，果然版本帝就是版本帝，一周一版本。现在我用的版本是基于6.1.1版本的。 目标 收集Java日志文件，并且根据文件的不同将日志分类，比如：订单日志，客户日志等。 日志文件多行处理 总体架构图 准备镜像6.0之后官方开始自己维护镜像版本:https://www.docker.elastic.co/。找到需要的ELK镜像地址，pull下来就好了。官方pull下来之后镜像名太长了，所以我将镜像全部重新打了tag，命令：docker tag docker.elastic.co/elasticsearch/elasticsearch:6.1.1 elasticsearch:latest。使用docker images查看： 安装docker版本ElasticSearch在elasticsearch的docker版本文档中，官方提到了vm.max_map_count的值在生产环境最少要设置成262144。设置的方式有两种 永久性的修改,在/etc/sysctl.conf文件中添加一行： 123grep vm.max_map_count /etc/sysctl.conf # 查找当前的值。vm.max_map_count=262144 # 修改或者新增 正在运行的机器： 12sysctl -w vm.max_map_count=262144 之后我们执行命令，暴露容器的9200，9300端口，方便我们在其它集器上可以通过类似head插件去做es索引的操作等。执行命令为： 1docker run -p 9200:9200 -p 9300:9300 --name elasticsearch -e "discovery.type=single-node" elasticsearch 如果实际使用中，可能需要设置集群等操作。因实际情况而定。如果你需要存储历史数据，那么就可能需要将data目录保存到本地，使用-v，或者mount参数挂载本地一个目录。 安装docker版本kibanakibana的作用主要是帮助我们将日志文件可视化。便于我们操作，统计等。它需要ES服务，所以我们将部署好的es和kibana关联起来，主要用到的参数是--link: 1docker run -d -p 5601:5601 --link elasticsearch -e ELASTICSEARCH_URL=http://elasticsearch:9200 kibana 使用link参数，会在kibana容器hosts文件中加入elasticsearch ip地址，这样我们就直接通过定义的name来访问es服务了。 安装logstash和filebeat前面的kibana和ES的安装，如果我们在开发环境中并不需要太多的关注他们的详细配置。但是logstash和filebeat我们需要注意下它的配置，因为这两者是我们完成需求的重要点。 logstash我们只让它进行日志处理，处理完之后将其输出到elasticsearch。 filebeat是一个轻量级收集器，我们使用它来收集Java日志，将不同文件夹下的日志进行tag，处理多行日志行为(主要针对Java异常信息)，之后发送给logstash。 日志的文件格式大概就是：DATE LOG-LEVEL LOG-MESSAGE，格式是在log4j.properties中定义的。你也可以自己定义输出格式。 现在我们定义logstash.conf,主要在logstash中使用grok filter插件。 logstash.conf: 123456789101112131415161718192021222324252627input &#123; beats &#123; #host =&gt; &quot;localhost&quot; port =&gt; &quot;5043&quot; &#125;&#125;filter &#123; if [fields][doc_type] == &apos;order&apos; &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; %&#123;LOGLEVEL:level&#125; %&#123;JAVALOGMESSAGE:msg&#125;&quot; &#125; &#125; &#125; if [fields][doc_type] == &apos;customer&apos; &#123; # 这里写两个一样的grok，实际上可能出现多种不同的日志格式，这里做个提示而已,当然如果是相同的格式，这里可以不写的 grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; %&#123;LOGLEVEL:level&#125; %&#123;JAVALOGMESSAGE:msg&#125;&quot; &#125; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [ &quot;localhost:9200&quot; ] index =&gt; &quot;%&#123;[fields][doc_type]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125; 在logstash.conf中，我们主要使用[fields][doc_type]来标明日志的类型，这个值实在filebeat中定义的。 现在我们假定需要收集两个目录下的日志文件：/home/user/elk/customer/*.log，/home/user/elk/order/*.log： customer.log: 12345678910112017-12-26 10:05:56,476 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:07:23,529 INFO WarehouseController:271 - findWarehouseList,json&#123;&quot;formJSON&quot;:&#123;&quot;userId&quot;:&quot;885769620971720708&quot;&#125;,&quot;requestParameterMap&quot;:&#123;&#125;,&quot;requestAttrMap&quot;:&#123;&quot;name&quot;:&quot;asdf&quot;,&quot;user&quot;:&quot;8857696&quot;,&quot;ip&quot;:&quot;183.63.112.1&quot;,&quot;source&quot;:&quot;asdfa&quot;,&quot;customerId&quot;:&quot;885768861337128965&quot;,&quot;IMEI&quot;:&quot;863267033748196&quot;,&quot;sessionId&quot;:&quot;xm1cile2bcmb15wtqmjno7tgz&quot;,&quot;sfUSCSsadDDD&quot;:&quot;asdf/10069&amp;ADR&amp;1080&amp;1920&amp;OPPO R9s Plus&amp;Android6.0.1&quot;,&quot;URI&quot;:&quot;/warehouse-service/appWarehouse/findByCustomerId.apec&quot;,&quot;encryptType&quot;:&quot;2&quot;,&quot;requestStartTime&quot;:3450671468321405&#125;&#125;2017-12-26 10:07:23,650 INFO WarehouseServiceImpl:325 - warehouse list:8,warehouse str:[&#123;&quot;addressDetail&quot;:&quot;nnnnnnnn&quot;,&quot;areaId&quot;:&quot;210624&quot;,&quot;areaNa&quot;:&quot;&quot;&#125;]2017-12-26 10:10:56,477 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:15:56,477 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:20:56,478 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:05:56,476 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:07:23,529 INFO WarehouseController:271 - findWarehouseList,json&#123;&quot;formJSON&quot;:&#123;&quot;userId&quot;:&quot;885769620971720708&quot;&#125;&#125;]2017-12-26 10:10:56,477 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:15:56,477 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration2017-12-26 10:20:56,478 INFO ConfigClusterResolver:43 - Resolving eureka endpoints via configuration order.log: 12345678910111213141516171819202122232425262728292017-12-26 11:29:19,374 INFO WebLogAspect:53 -- 请求:18,SPEND TIME:02017-12-26 11:38:20,404 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:41:07,754 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 12:38:58,683 INFO RedisClusterConfig:107 -- //// --- 启动单点Redis ---2017-12-26 12:39:00,325 DEBUG ApplicationContextRegister:26 -- 2017-12-26 12:39:06,961 INFO NoticeServiceApplication:57 -- Started NoticeServiceApplication in 17.667 seconds (JVM running for 18.377)2017-12-26 11:27:56,577 INFO WebLogAspect:51 -- 请求:19,RESPONSE:&quot;&#123;\&quot;data\&quot;:null,\&quot;errorCode\&quot;:\&quot;\&quot;,\&quot;errorMsg\&quot;:\&quot;\&quot;,\&quot;repeatAct\&quot;:\&quot;\&quot;,\&quot;succeed\&quot;:true&#125;&quot;2017-12-26 11:27:56,577 INFO WebLogAspect:53 -- 请求:19,SPEND TIME:12017-12-26 11:28:09,829 INFO WebLogAspect:42 -- 请求:20,URL:http://192.168.7.203:30004/sr/flushCache2017-12-26 11:28:09,830 INFO WebLogAspect:43 -- 请求:20,HTTP_METHOD:POST2017-12-26 11:28:09,830 INFO WebLogAspect:44 -- 请求:20,IP:192.168.7.982017-12-26 11:28:09,830 INFO WebLogAspect:45 -- 请求:20,CLASS_METHOD:com.notice.web.estrictController2017-12-26 11:28:09,830 INFO WebLogAspect:46 -- 请求:20,METHOD:flushRestrict2017-12-26 11:28:09,830 INFO WebLogAspect:47 -- 请求:20,ARGS:[&quot;&#123;\n&#125;&quot;]2017-12-26 11:28:09,830 DEBUG SystemRestrictController:231 -- 刷新权限限制链2017-12-26 11:38:20,404 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:41:07,754 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:41:40,664 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:43:38,224 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:47:49,141 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:51:02,525 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:52:28,726 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:53:55,301 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:54:26,717 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 11:58:48,834 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 12:38:51,126 INFO NoticeServiceApplication:664 -- The following profiles are active: test2017-12-26 12:38:58,683 INFO RedisClusterConfig:107 -- //// --- 启动单点Redis ---2017-12-26 12:39:00,325 DEBUG ApplicationContextRegister:26 -- ApplicationContextRegister.setApplicationContext:applicationContextorg.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5f150435: startup date [Tue Dec 26 12:38:51 CST 2017]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@63c12fb02017-12-26 12:39:06,961 INFO NoticeServiceApplication:57 -- Started NoticeServiceApplication in 17.667 seconds (JVM running for 18.377) 日志的文件格式大概就是：DATE LOG-LEVEL LOG-MESSAGE，格式我们是在log4j.properties中定义的。你可以自己定义，自定义注意修改logstash.conf中的grok就好。 之后解决我们的filebeat要解决的问题：收集日志，处理多行日志，给日志打标签。在filebeat.yml中,如下定义： filebeat.yml 12345678910111213141516171819filebeat.prospectors:- paths: - /home/user/elk/logs/order/*.log multiline: pattern: ^\d&#123;4&#125; negate: true match: after fields: doc_type: order- paths: - /home/user/elk/logs/customer/*.log multiline: pattern: ^\d&#123;4&#125; negate: true match: after fields: doc_type: customeroutput.logstash: # 输出地址 hosts: [&quot;logstash:5043&quot;] 收集日志：直接使用prospector定位并且处理日志文件。 多行日志: 根据日志格式，我们开头都是yyyy，类似与纯4个数字，所以我们使用multile插件，做配置就好。官方的文档挺详细的，主要就是实践：filebeat multiline 打标签：这个是最重要的，主要的目的是让logstash知道filebeat发送给它的消息是那个类型，然后logstash发送到es的时候，我们可以建立相关索引。这里的fields是内置的，doc_type是自定义的。 之前的document_type 在5.5.0中就已经废弃了。https://www.elastic.co/guide/en/beats/libbeat/6.1/release-notes-5.5.0.html#_deprecated_6 了解这些之后，我们启动我们的logstash和filebeat。 启动docker版本的logstash： 1docker run -it --name logstash --link elasticsearch -d -v ~/elk/yaml/logstash.conf:/usr/share/logstash/pipeline/logstash.conf logstash 启动filebeat，将文件挂载到容器中，这里也可以有其它的处理方法，你可以根据自己的需求来。 1docker run --name filebeat -d --link logstash -v ~/elk/yaml/filebeat.yml:/usr/share/filebeat/filebeat.yml -v ~/elk/logs/:/home/logs/ filebeat 最后记得在kibana里面建立索引(create index)的时候，默认使用的是logstash，而我们是自定义的doc_type,所以你需要输入order*,customer*这样就可以建立两个索引了。 之后就可以在kibana的Discovery里面看到你配置的了 如果你直接用我的log，请将时间稍微改一下，2017-12-26改为当天实验年月。 上面的命令我都自己实践过，是可以用的，注意下-v参数挂载的几个本地盘的地址。还有filebeat收集的地址。 配置文件地址仓库：使用Docker搭建ELK日志系统 ，仓库配有docker-compose.yml文件在根目录下直接运行docker-compose up 就可以看到实际效果了(记得改下日志时间)。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux如何查看当前占用CPU或内存最多的K个进程]]></title>
    <url>%2F2017%2F12%2F27%2Ftop-k-pid-memory-cpu-linux%2F</url>
    <content type="text"><![CDATA[Linux如何查看当前占用CPU或内存最多的K个进程一、可以使用以下命令查使用内存最多的K个进程 方法1： 1ps -aux | sort -k4nr | head -K 如果是10个进程，K=10，如果是最高的三个，K=3 说明：ps -aux中（a指代all——所有的进程，u指代userid——执行该进程的用户id，x指代显示所有程序，不以终端机来区分） ​ ps -aux的输出格式如下： 1234USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 19352 1308 ? Ss Jul29 0:00 /sbin/initroot 2 0.0 0.0 0 0 ? S Jul29 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S Jul29 0:11 [migration/0] ​ sort -k4nr中（k代表从第几个位置开始，后面的数字4即是其开始位置，结束位置如果没有，则默认到最后；n指代numberic sort，根据其数值排序；r指代reverse，这里是指反向比较结果，输出时默认从小到大，反向后从大到小。）。本例中，可以看到%MEM在第4个位置，根据%MEM的数值进行由大到小的排序。 ​ head -K（K指代行数，即输出前几位的结果） ​ |为管道符号，将查询出的结果导到下面的命令中进行下一步的操作。 方法2：top （然后按下M，注意大写） 二、可以使用下面命令查使用CPU最多的K个进程 方法1： 1ps -aux | sort -k3nr | head -K 方法2：top （然后按下P，注意大写） 原文转自:博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 搜索基础]]></title>
    <url>%2F2017%2F12%2F21%2Felasticsearch-search-base%2F</url>
    <content type="text"><![CDATA[Elasticsearch 搜索基础三大概念 映射（Mapping）：描叙数据在每个字段中如何存储； 分析（Analysis）：全文是如何处理使之可以被搜索； 领域特定查询语言（QueryDSL）：Elasticsearch 查询语言，json格式。]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式文档存储]]></title>
    <url>%2F2017%2F12%2F21%2Felasticsearch-routing-data-to-shards%2F</url>
    <content type="text"><![CDATA[ES 分布式文档存储路由文档到分片创建索引的时候通常我们确定了几个主分片，主分片的数量已经确定，但是输入是存在那个分片？是怎么确定的？是通过一个公司算出来的。 1shard = hash(routing)%number_of_primary_shards 其中routing默认是文档的id，当然也可以自定义routing的值。 集群中操作我们可以往任意节点发送请求，每个节点都有能力处理任意请求。每个节点知道任意文档的位置，并且可以直接将需求转发到响应的节点上。这种节点称为协调节点。比如向node1发送数据请求，node1找到了0分片，然后发现0分片在node1，node2，node3个节点上，然后node1，通过轮询所有分片的来发送请求到哪一个节点做到负载均衡。 在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的 任何文档的CRUD操作都必须现在主分片上进行，然后复制到相关的副本分片。这个操作过程ES允许进行配置。 在集群中更新一个文档的步骤：]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Travis-CI持续构建Hexo博客]]></title>
    <url>%2F2017%2F12%2F19%2Fhexo-travis-ci%2F</url>
    <content type="text"><![CDATA[使用Travis-CI持续构建Hexo博客我是用的 Hexo+Github Page 来构建自己的博客。在 Github 创建your_github_name.github.io仓库的时候，可以直接使用your_github_name.github.io作为你的博客域名站点。详细的话，可以自己去 google 下使用 Hexo搭建个人博客。 写本文的原因是因为，每次我在写完一篇总结，提交了push之后。如果要更新到博客上，需要经历的过程就是： 123456789# 提交源文件到仓库git add .git commit -m "xxx"git push origin master# 发布到博客站点hexo cleanhexo d -g# 这里还需要输入github账号，密码。 我觉得如果提交源文件push之后能够直接发布到博客站点就好了。在网上搜了一圈，很多人的资料还是有些缺陷，自己踩了一路坑，所以有了本篇记录。 环境介绍先介绍下我的博客环境，我用了两个repo:一个是存博客源码的blog.git,另一个也就是用来做站点发布的chenzhijun.github.com.git。 网上很多人喜欢用一个库，然后切换分支的做法。之前我也弄过，不过后来发现我经常弄错分支。反正repo不要钱，就无所谓，分开吧。其实就是懒。总之，自己爽就好。 实际操作其实就是将生成的目录public下的所有文件当作了chenzhijun.github.com.git库下面的文件。 生成一个Personal access tokens.Token记得勾选如下权限，然后copy保存： 注册Travis-CI账号：https://travis-ci.org。我是直接使用github登陆的，方便。 激活需要进行CI的仓库： 点击仓库进去做配置，找到设置的地址： 配置环境变量等： 之后在博客根目录创建.travis.yml文件，我的文件内容为： 12345678910111213141516171819202122232425262728293031323334language: node_js # 设置语言node_js: stable # 设置相应版本cache: apt: true directories: - node_modules # 设置缓存，传说会在构建的时候快一些before_install: - npm install hexo-cli -ginstall: - npm install # 安装hexo及插件script: - hexo clean # 清除 - hexo g # 生成after_script: - git clone $&#123;GH_REF&#125; pub_web # 因为我有两个仓库，先将发布服务的仓库clone下来， - cp -rf public/* pub_web/ # 将源博客仓库(blog.git)目录下的public文件夹下的文件复制到发布服务的仓库(chenzhijun.github.com.git)中 - cd pub_web # 进入到git仓库 - git config user.name "username" - git config user.email "email@address.com" - git add . - git commit -am "Travis CI Auto Builder :$(date '+%Y-%m-%d %H:%M:%S' -d '+8 hour')" # 零时区，+8小时 - git push origin master branches: only: - master #只监测master分支,这是我自己的博客，所以就用的master分支了。env: global: - GH_REF: https://yourname:$&#123;GITHUB_TOKEN&#125;@github.com/yourname/your.blog.address.git #设置GH_REF，注意更改yourname,GITHUB_TOKEN:就是我们在travis-ci仓库中配置的环境变量 在_config.yml中加入(这里如果是用hexo，应该一开始就会弄好了)： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/yourname/yourname.github.com.git branch: master 到这里，我们一个简单的ci就弄好了,你可以试着提交一个commit，然后push到你的仓库之后在travis-ci里面就能看到build日志了。 其实简单的原理就是： 向仓库blog.git提交commit； travis-ci 自动构建blog.git,根据.travis.yml的配置执行； 运行hexo g之后，public目录下文件更新； 克隆chenzhijun.github.com.git仓库，将其命名为别名pub_web，将public下的文件复制到pub_web； 将pub_web目录下的文件提交commit； push最新的文件到github。 特别注意 personal token只在仓库的https协议下有效，官网说的：Creating a personal access token for the command line 这样站点就更新了，如果你是在一个仓库下多个分支，我想应该也容易了。自己动手，丰衣足食。 相关参考： 使用Travis CI自动部署Hexo博客 使用 Travis-CI 来自动化部署 Hexo 基于 Hexo 的全自动博客构建部署系统]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo - Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-document-data]]></title>
    <url>%2F2017%2F12%2F19%2Felasticsearch-document-data%2F</url>
    <content type="text"><![CDATA[什么是文档通常情况下，文档类似与对象。在es的术语中文档指的是根对象，或者称为最顶层对象。它被序列化成json存储在es中，并且有一个唯一ID。 字段可以是任何字符，但不能包含时间段 文档的元数据索引:_indexES中文档有三个特定的属性：index,type,id;(type可能会在以后的版本中去掉，现在的版本6.0中依然存在)。在es分片中我们提到，数据是存储在索引中的，而索引指的是单个，或者多个分片的逻辑命名空间。索引广义上来说，有点类似我们的数据库database名称。一个库名就对应着一个索引。 索引名字必须小写，不能以下划线开头，不能有逗号 类型:_typetype的含义是将具有相同属性或相似属性的文档集合在一起。它是索引中对数据的逻辑分区。有点类似于我们在数据库中的表。 类型名可以是大小写，不能以下划线开头，不能包含逗号，长度限制在256字符。 唯一ID:_id如果说空间中的一个点由xyz三轴表示，那么es中的文档位置就由索引，类型，id来确定。Id是一个字符串，你可以自己生成，也可以让ES帮你生成。id类似与表中的主键。 文档还有其它的元数据，但是总的来说，最最重要的就是这三个。毕竟你最重要的还是数据(文档)。 自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串。 这些 GUID 字符串由可修改的 FlakeID 模式生成，这种模式允许多个节点并行生成唯一 ID ，且互相之间的冲突概率几乎为零。 你可以这样理解，图书馆有各种书，管理员将书分成了很多类，每一个类下面有个唯一的书编号，知道这些，你就能找到那本特定的书。其中图书馆就是ElasticSearch，各种书就是index，分类就是type，唯一id就是书编号。实际的那本书就是一个文档。 创建索引文档 带id创建索引文档,id是你自己生成的唯一id，使用put方法： 12345PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;&#123; "field": "value", ...&#125; 如果是自动生成的，请使用POST方法： 12345POST /&#123;index&#125;/&#123;type&#125;&#123; "field": "value", ...&#125; 获取文档前面我们说过，如果确定index,type,documentId，我们就能获取一个文档值。对于文档的获取方式我们可以获取到文档的全部值，或者是文档的部分值。 根据三元素获取文档全部值： 1GET /website/blog/123?pretty 返回结果： 123456789101112&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "found" : true, "_source" : &#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01" &#125;&#125; 其中需要注意字段found:true,如果找到了值，这里就是true，如果没有找到，这里回事false，并且响应的HTTP状态码会是404。 返回文档的一部分： 1GET /website/blog/123?_source=title,text 这里就是只需要:title,text。返回结果： 1234567891011&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "found" : true, "_source" : &#123; "title": "My first blog entry" , "text": "Just trying this out..." &#125;&#125; 如果只需要_source字段不需要任何元数据： 1GET /website/blog/123/_source 返回结果： 12345&#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01"&#125; 检查文档是否存在如果不需要数据，只需要检查在某一个时刻某个文档是否存在，因为有可能这一秒钟结果返回不存在，下一秒钟就创建了，所以用检查的时候主要注意这个时效性。不需要数据的时候我们可以使用HEAD方法来进行请求，格式为：curl -i -XHEAD http://ip:port/index/type/id;比如: 1curl -i -XHEAD http://localhost:9200/website/blog/123 更新文档在ES中文档是不可修改的。如果需要更新现有的文档，需要重建索引或者进行替换。也就是说，你可以再调用一次创建索引的接口。只不过在返回的响应中会看到元数据域有部分改变： 1234567&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 2, "created": false &#125; 可以看到，version已经变成了2，created是false。created为false，是因为相同的索引，type，id已经存在。在内部ElasticSearch会标记旧文档为已删除，并增加一个全新的文档。虽然旧文档不能再访问，但是es不会立即删除它。当索引的数据越来越多的时候，es会在后台删除？(什么时候删除，学完记得看下文档) 确保只能创建文档，非更新上面更新的方式我们知道，创建的时候有可能也是更新文档。那么如果保证这次的创建请求,是创建了一个全新的文档而非更新一个文档了？我们可以采取一些手段来保证。 使用post方式进行增加，让es自动生成文档id 如果采用自己的id，在末尾可以加参数： a) 使用param的方式创建： 1PUT /website/blog/123?op_type=create b) 使用restful风格： 1PUT /website/blog/123/_create 如果创建过程中有错误，会返回409错误状态码： 1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "document_already_exists_exception", "reason": "[blog][123]: document already exists", "shard": "0", "index": "website" &#125; ], "type": "document_already_exists_exception", "reason": "[blog][123]: document already exists", "shard": "0", "index": "website" &#125;, "status": 409&#125; 删除文档删除的规则也遵循restful风格，所以类推如下： 1DELETE /website/blog/123 如果有值，那么返回200，并且返回json： 1234567&#123; "found" : true, "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 3&#125; 如果没有找到，将会返回404: 1234567&#123; "found" : false, "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 4&#125; 删除跟更新一眼哥也是做一个标记操作，不会立即将文档删除，随着索引的数据越来越多，es会在后台进行删除。]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 集群主分片和副本分片]]></title>
    <url>%2F2017%2F12%2F18%2Felasaticearch-shard-replicas%2F</url>
    <content type="text"><![CDATA[集群的健康状态查询： 1curl -XGET 'localhost:9200/_cluster/health?pretty' 123456789101112&#123; "cluster_name": "elasticsearch", "status": "green", "timed_out": false, "number_of_nodes": 1, "number_of_data_nodes": 1, "active_primary_shards": 0, "active_shards": 0, "relocating_shards": 0, "initializing_shards": 0, "unassigned_shards": 0&#125; status字段是最为重要的，有三个值：green，yellow，red。三者的含义： green：所有主分片和副分片都能正常运行； yellow: 所有主分片都能正常运行，但不是所有副分片都能正常运行； red：有主分片没能正常运行。 在es中存数据需要先建立索引，而索引实际上是一个或多个物理分片的逻辑命令空间。 一个分片是一个底层的工作单元，存储全部数据中的一部分。它是一个Lucene的实例，本身它就是完整的搜索引擎。 文档被存储和索引在分片中，应用程序直接与索引交互不是与分片交互。 理论尚分片可以存储Integer.max_value-128个文档。实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。 副本分片是主分片的拷贝，也就是指一个备份。索引在创建的时候就已经确定了确定的分片数量(默认为5个)，一旦索引创建成功，主分片数就不能再改变，但是副分片数可以被随时改变。创建一个确定主分片的数的索引：使用3个主分片和6个主分片分别创建blogs和accounts索引: 3个blogs主分片，1个备份分片123456789curl -XPUT 'localhost:9200/blogs?pretty' -H 'Content-Type: application/json' -d'&#123; "settings" : &#123; "number_of_shards" : 3, "number_of_replicas" : 1 &#125;&#125;' 6个accounts主分片，1个备份分片： 123456789curl -XPUT 'localhost:9200/accounts?pretty' -H 'Content-Type: application/json' -d'&#123; "settings" : &#123; "number_of_shards" : 6, "number_of_replicas" : 1 &#125;&#125;' 之后结果如图： 如果我们觉得备份分片不够，比如我想将accounts调整为2个备份分片： 123456curl -XPUT 'localhost:9200/accounts/_setting' -H 'Content-Type: application/json' -d'&#123; "number_of_replicas" : 2&#125;' 之后结果如图: 如果我们的备份分片没有分配到节点上，集群的健康值就会变成yellow： 我们可以为每一个分片(包括主分片，副本分片)分配一个节点。这样会提高我们的搜索效率，节点数/主分片数。 上图的副本分片都没有分配到节点上，全部副本分片都是unassigned，在一个原始数据节点存储副本分片，想想也是知道没有任何意义。该节点挂掉之后，副本数据肯定也会挂掉。 还有一种就是集群健康状态为红：red。这种情况下指的是主分片中有一个或多个主分片为不可用状态。]]></content>
  </entry>
  <entry>
    <title><![CDATA[基于ELK+Filebeat搭建日志中心]]></title>
    <url>%2F2017%2F12%2F12%2Felasticsearch-logstash-kibana-part%2F</url>
    <content type="text"><![CDATA[基于ELK+Filebeat搭建日志中心实验环境 ubuntu 16 jdk 1.8 elasticesearch 6.0, kibana 6.0, logstash 6.0, filebeat 6.0 我也在windows10下安装过，win10下只需要修改filebeat的文件路径配置就可以了。 概念介绍elastic提供了非常多的工具，官方称为Elastic Stack。它提供了一些解决方案。我们用到的就是其中Stack中的部分工具 Elasticsearch Elasticsearch 是一个分布式、可扩展开源全文搜索和分析引擎。它能够进行存储，并且能以很快的速度(接近实时)来进行搜索和分析大量的数据。它使用Java编写，底层基于Apache Lucene。 Logstash Logstash 是一个开源的，服务端数据处理管道。 它可同时从多个源来搜集数据，对数据进行过滤等操作，并且将数据传送到你想要传送的地方进行存储。 Kibana Kibana 被设计成一个和Elasticsearch共同使用的开源分析和可视化平台，它支持将ES中存储的数据生成多种维度的图等。 Filebeat Filebeat 是一个轻量级的日志采集器，用于将源数据采集后发送给Logstash，并且Filebeat使用背压敏感协议，以考虑更多的数据量。如果Logstash正在忙于处理数据，则可以让Filebeat知道减慢读取速度。还有很多Beat可以提供选择，有监控网络的Packbeat，指标Metricbeat。各种beat详情：https://www.elastic.co/cn/products/beats 系统架构简图如下： 环境搭建将ELK和filebeat下载后解压到一个目录下，四个产品都是开箱即用的。 Logstash安装与配置Logstash的作用主要是用来处理数据，当然它也可以直接从日志文件读取记录然后进行处理。在Logstash的根目录，我们需要建立一个新文件logstash.conf: 1234567891011121314151617# 配置logstash输入源input &#123; beats &#123; host =&gt; &quot;localhost&quot; port =&gt; &quot;5043&quot; #注意要和filebeat的输出端口一致 &#125;&#125;# 配置输出的地方output &#123; # 控制台 stdout &#123; codec =&gt; rubydebug &#125; # es elasticsearch &#123; hosts =&gt; [ &quot;localhost:9200&quot; ] &#125;&#125; Filebeat安装与配置Filebeat作为日志搜集器，肯定是需要指定输入源和输出地，所以我们需要先配置它。在Filebeat的根目录下我们需要在filebeat.yml中: 123456789101112# 输入源filebeat.prospectors:- type: log paths: - /var/log/*.log # 日志文件以log结尾并且需要放在/var/log目录下 # 如果是windows,如下 # - C:\Users\chen\Desktop\elastic\elkjava\log\*.log# 输出地output.logstash: # The Logstash hosts hosts: ["localhost:5043"] Elasticsearch和Kibana安装与配置这次演示我们对es和kibana不做特别的配置，就默认就好了。 启动顺序其实没有特别的顺序，我一般的启动顺序是elasticsearch-&gt;logsatsh-&gt;kibana-&gt;filebeat。其实差别都不大，特别注意一个就是，如果你想要测试的话，logstash支持配置自动更新，如果是日志文件更新，想让filebeat重新再搜索一次，删除掉filebeat根目录下data/registry文件。其实类推，如果要删除其它的软件(elk)的的数据，删掉data目录,很直接很暴力，但不建议在正式场景直接这样弄。 启动es: 1./bin/elasticsearch 启动logstash： 12345# 测试conf文件是否正确配置./bin/logstash -f logstash.conf --config.test_and_exit# 启动logstash，如果有修改conf会自动加载./bin/logstash -f logstash.conf --config.reload.automatic 启动kibana： 1./bin/kibana 启动filebeat: 1./filebeat -e -c filebeat.yml -d "publish" 现在在浏览器中输入：http://localhost:5601打开kibana， 看到下面的图： 点击create，之后再点击左侧导航栏Discover： 就能看到值了。 不管怎么样，一定要动手实验才知道能不能行。 进阶实战：收集Java日志上面搭建的elk是可以用的，但是在实际中我发现一些需要处理的地方，比如Java的日志文件经常将堆栈打印出来，这个时候如果还是按照上面的配置，我们无法正确的显示在kibana中，因为我们上面实际配置的是按行读取的。但是堆栈信息应该是一条。所以我们需要对日志进行一些多行处理。可以在filebeat中处理，也可以在logstash中处理。我们今天直接在filebeat中处理，让logstash只做日志的过滤。 首先准备一份Java的日志文件,大致的日志文件都会像下面这样格式：date log-level log-message=》时间 日志级别 日志内容，下面是一个抛出空指针的文件(有删减)： 123456789102017-12-06 16:59:38,927 WARN ExceptionHandlerExceptionResolver:391 Failed to invoke @ExceptionHandler method: public com.framework.common.model.ResultData com.elasticsearch.exception.IntegralExceptionHandler.exceptionHandler(java.lang.Exception)java.lang.NullPointerException at com.framework.common.util.SpringUtils.getMessage(SpringUtils.java:152) ~[classes/:?] at com.framework.common.util.SpringUtils.getMessage(SpringUtils.java:138) ~[classes/:?] at com.framework.common.util.SpringUtils.getMessage(SpringUtils.java:57) ~[classes/:?] at com.elasticsearch.exception.IntegralExceptionHandler.exceptionHandler(IntegralExceptionHandler.java:33) ~[classes/:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121] 在filebeat中主要是将堆栈信息的内容合并到一行中，也就是说发送给logstash的时候将下面的异常堆栈当作log-message，让filebeat读取到堆栈的时候将空行转义成字符串然后将这行的信息补充到第一行后面。我们可以使用filebeat的multiline配置,详情：https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html filebeat.yml: 1234567891011121314filebeat.prospectors:- type: log paths: - /var/log/*.log # 日志文件以log结尾并且需要放在/var/log目录下 # 如果是windows,如下 # - C:\Users\chen\Desktop\elastic\elkjava\log\*.log multiline: # 多行处理，正则表示如果前面几个数字不是4个数字开头，那么就会合并到一行 pattern: ^\d&#123;4&#125; negate: true # 正则是否开启，默认false不开启 match: after # 不匹配的正则的行是放在上面一行的前面还是后面output.logstash: # The Logstash hosts hosts: ["localhost:5043"] 同样的这次我们对Java日志做一些信息提炼，也就是使用过滤规则，将date，log-level,log-message等提取出来： logstash.conf: 1234567891011121314151617input &#123; beats &#123; host =&gt; &quot;localhost&quot; port =&gt; &quot;5043&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125; %&#123;LOGLEVEL:level&#125; %&#123;JAVALOGMESSAGE:msg&#125;&quot; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [ &quot;localhost:9200&quot; ] &#125;&#125; 值得注意的就是我们使用filter插件，在其中我们使用了grok(基于正则表达式)过滤我们的日志内容。grok可以将无规则的日志数据通过正则匹配将之转换成有结构的数据，以此我们能够根据相应结构进行查询。grok的使用方式是：%{规则:自定义字段名称}grok详情Logstash提供了很多grok表达式，详情可以看：https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns 为了效果，我们可以删除filebeat，es，logstash，kibana等根目录的data文件，然后按照之前的步骤重启这些工具，现在可以在kibana中看到相应的结果了： 先看到我们自定义的字段： 在Discover中过滤字段： 结果： 目前就告一段落了，最主要的是自己动手时间。接下准备做的事情是将这些能一个Dockerfile，然后只需要自己配置下filebeat的收集目录和logstash的过滤规则，我们就能使用了。不管怎样只有自己动手去做了，才会知道有很多坑需要填。我也在google搜了很多资料，本来想在文后加个参考链接的，后来发现太多了。算了，还是不贴链接了。总之，谢谢哪些我在网上搜资料给了我灵感，帮我跨过一些坑的人。谢谢。 纸上得来终觉浅，绝知此事要躬行。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK - Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Apache POI操作生成Execl]]></title>
    <url>%2F2017%2F12%2F11%2Fjava-poi-operation%2F</url>
    <content type="text"><![CDATA[使用Apache POI操作生成Execl使用Java操作数据生成Excel表格，在网上能搜到的方式有很多，比如jxl，还有今天用到POI。POI是Apache开源的一个项目，该工具使用简单，方便。 准备与一些概念约定需要在poi官网下载相应的jar包，或者使用maven来引入包。在开始之前我们需要知道一些关于excel文档的概念。一个excel文件通常称为[workbook]，excel里面的一个工作页面通常叫做[sheet]，sheet里面的格子称为[单元格cell]，单元格由坐标确定唯一位置，相应为[行row]，[列col]。 了解这些我们可以开始看看poi的一些类和接口。 POIPOI 中主要的几个类为：HSSFWorkbook，HSSFSheet，Row，Cell。就像我们创建一个最简单的报表文件一样，先创建excel文件(HSSFWorkbook)，然后创建一个工作页(HSSFSheet)，然后找到哪一行(Row)，在哪一列上创建一个单元格(Cell)。你在excel中操作的最小单位都是Cell，所以我们进行读取和操作的最小单位也是Cell。 嗯，如果你还喜欢猜测，为什么猜测了？因为这几个类都是HSSF开头，那么是不是后面的就是他们的父类，而这些HSSF开头的都是他们的实现了。答案是的。 开始编程前，请记住，你再操作excel的一个步骤，写代码的时候流程也是这样的。 123456789101112131415161718192021222324//创建 excel workbookHSSFWorkbook hssfWorkbook = new HSSFWorkbook();//创建工作簿 sheetHSSFSheet hssfSheet = hssfWorkbook.createSheet();//找到需要操作的行row,从第0行开始。我们在excel中看到的行是从1开始数，但是在poi中是从0开始。int rowNum = 0;Row row1 = hssfSheet.createRow(rowNum);//创建单元格cellint colNum = 0;Cell row1Col1 = row1.createCell(colNum);//操作单元格内容row1Col1.setCellValue("test");// 导出excel文件FileOutputStream fout = null;try&#123; fout = new FileOutputStream("test.xsl"); hssfWorkbook.write(fout); fout.close();&#125;catch (Exception e)&#123; e.printStackTrace();&#125; 这样一个简单的单元格内容操作就完成了。 多变的需求很多时候我们会有一些特殊的要求，比如文字居中，多行合并，多列合并，加底色，文字颜色等等。。 用的最多的就是文字居中了，这类需求往往都是对于一个单元格有相应要求，POI针对这种需求有一个类：Style。用style来设置单元格的相关样式，下面来看一个文本居中的样式代码： 123HSSFCellStyle style = excel.createCellStyle();style.setAlignment(CellStyle.ALIGN_CENTER);row1Col1.setCellStyle(style);//内容居中显示 我们在进行合并的时候，其实实际上合并的还是单元格。所以我们在创建单元格的时候进行一些单元格范围参数设置： 12345678//单元格范围 参数（int firstRow, int lastRow, int firstCol, int lastCol)CellRangeAddress cellRangeAddress = new CellRangeAddress(0, 1, 0, 1);//在sheet里增加合并单元格hssfSheet.addMergedRegion(cellRangeAddress);//生成第一行Row row = hssfSheet.createRow(0);Cell first = row.createCell(0);first.setCellValue("表头"); 此时被合并的行的其中的单元格都将无效，也就是说再操作这些被合并的单元格都已经不再称为单元格。 123456789101112//单元格范围 参数（int firstRow, int lastRow, int firstCol, int lastCol)CellRangeAddress cellRangeAddress = new CellRangeAddress(0, 1, 0, 1);//在sheet里增加合并单元格hssfSheet.addMergedRegion(cellRangeAddress);//生成第一行Row row = hssfSheet.createRow(0);Cell first = row.createCell(0);first.setCellValue("first");//操作被合并的单元格Row row2 = hssfSheet.createRow(1);Cell second = row2.createCell(0);second.setCellValue("second"); 像上述代码，second不会输出到excel，因为它的所在单元格(1,0)已经被合并了。 通常我们都会自己写一些工具类，然后进行操作excel操作，你也可以试着自己写一个。我这里就不贴我的代码了。 纸上得来终觉浅，绝知此事要躬行]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 简介与使用]]></title>
    <url>%2F2017%2F12%2F03%2Felastic-search-introduce%2F</url>
    <content type="text"><![CDATA[ElasticSearch 简介与使用Elasticsearch 是一个分布式、可扩展、实时的搜索与数据分析引擎。Elastic 的底层用的是 Lucene。如果你想用 Lucene的话必须自己去写接口，而Elastic将这些进行了一层封装，并且提供restful接口，让使用者达到开箱即用。 基本概念Node 与 ClusterElastic 实际上是一个分布式数据库，它可以存储数据，能够让多台服务器协同工作，每个服务器可以运行多个Elastic服务。每一个Elastic服务实例都可以称作一个节点（Node），一组节点就构成了集群（Cluster）。 Index，Type，DocumentElastic中通过索引(Index)，类型（Type），文档（Document）三个值来定义了Elastic中存储的数据结构。索引相当于我们在数据库中的库名字，类型相当于表，文档就是实际存储的数据内容。比如一堆书，这个就是“书”索引（Index），按照“武侠”，“技术”等进行分类（Type),每一本书就是实际上存储的数据(Document)。在Elastic中多个文档组成了一个索引，而文档可以通过分类来方便查询。这里的Type实际上是逻辑上的分组。 ES的增删改ES的增删改查遵循restful的风格，所以在使用在非常方便： 1234567891011121314151617181920// 增加POST localhost:9200/accounts/person/1&#123; "name":"Jack", "lastname":"Mic", "description":"A very handsome man"&#125;//删除数据DELETE localhost:9200/accounts/person/1POST accounts/persion/1/_update&#123; "doc":&#123; "description":"this is change description" &#125;&#125; ES的查询为什么要单独讲讲查询？ElasticSearch，从名字中就可以直接看出，search占据了elastic的很大一部分。很多时候我们使用ES也是主要因为它方便的查询功能，在ES中查询的方式有以下几种： 不带条件返回所有索引下的所有文档： 123GET localhost:9200/_search?pretty // pretty是将返回的json进行格式化GET /_search?size=5 // size 是记录条数GET /_search?size=5&amp;from=5 // from是只从哪页开始，类似SQL分页查询 根据索引，类型，文档id获取到唯一值： 1GET localhost:9200/accounts/person/1 不带body的查询，轻量搜索 12345// 返回所有文档中有‘jack’的数据GET localhost:9200/accounts/person/_search?q=Jack// 返回所有类型中为tweet的tweet字段带有‘elasticsearch’的数据GET /_all/tweet/_search?q=tweet:elasticsearch 带body的条件查询：Query DSL 1234567891011GET localhost:9200/accounts/person/_search&#123; "query":&#123; "term":&#123; "name":&#123; "value":"Jack" &#125; &#125; &#125;&#125; 官方文档提供了一些实例：查询表达式]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch - Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 安装 (单机单节点/伪集群)]]></title>
    <url>%2F2017%2F12%2F01%2Felasticsearch-install%2F</url>
    <content type="text"><![CDATA[ElasticSearch 安装 (单机单节点/单机多节点)ElasticSearch 简介ElasticSearch(ES) 现在已经随着技术发展越来越火爆了。它基于Lucence搜索引擎，实现RestFul风格，开箱即用。广泛用于在网站上做站内搜索。 下载这个忒简单了，会上网的人应该都会。 安装ES 下载解压后，配置文件主要在config目录下，包含文件：elasticsearch.yml,jvm.options,log4j2.properties。这三个文件分别对应ES配置，JVM配置，ES日志配置。我们这里只讨论elasticsearch.yml的配置，其他的暂时不论。 单机单节点单机单节点最爽了，为啥？因为简单啊。进入到解压后文件夹的bin目录，然后window平台双击elasticsearch.bat,*nix平台使用sh elasticsearch,之后再在控制台中看到如下，有个started： 因为我们什么配置都没改，所以ES使用默认配置，http端口为9200，TCP端口为9300。这个时候我们访问下接口：curl -XGET localhost:9200,或者浏览器打开localhsot:9200,就会看到下面的输出： 单机很简单，真的很简单。 单机多节点(伪集群)部署完单机，下面就是集群了。集群，什么是集群了？一个服务在多台机器上部署，并且这些服务之间彼此之间内部高度紧密协作拥有某种联系，我们可以当作是这个服务的集群。在某种含义上，可以认为是一台服务器。 ES 伪集群：es服务在同一台机器上根据不同的端口启动服务，构成在本机上的一个集群模式。 以此为基础，我们来看看怎么配置。 主要用到的配置属性有这些， 我的本地ip地址为：192.168.11.21, master 的 elasticsearch.yml: 12345678cluster.name: notice-applicationnode.name: masternode.master: truenetwork.host: 192.168.11.21# network.bind_host: 192.168.11.21http.port: 9200transport.tcp.port: 9300discovery.zen.ping.unicast.hosts: ["192.168.11.21:9300","192.168.11.21:9310","192.168.11.21:9320"] slave1 的 elasticsearch.yml: 12345678cluster.name: notice-applicationnode.name: slave1# network.publish_host: 192.168.11.21# network.bind_host: 192.168.11.21network.host: 192.168.11.21http.port: 9210transport.tcp.port: 9310discovery.zen.ping.unicast.hosts: ["192.168.11.21:9300","192.168.11.21:9310","192.168.11.21:9320"] slave2 的 elasticsearch.yml:12345678cluster.name: notice-applicationnode.name: slave2# network.publish_host: 192.168.11.21# network.bind_host: 192.168.11.21network.host: 192.168.11.21http.port: 9220transport.tcp.port: 9320discovery.zen.ping.unicast.hosts: ["192.168.11.21:9300","192.168.11.21:9310","192.168.11.21:9320"] 上面的配置，如果要你要体验下可以拷贝到你自己的ES中，将IP改成你的本地ip就可以看到了。 推荐一个图形化工具：elasticsearh-head,这货尽然还推出了Chrome 插件。简直完美。 安装之后你就可以head插件看到集群配置了，下面是我的集群启动，电脑配置不太够，只启动了两台服务。 现在说正题，我们说下配置： cluster.name: 它指代的是集群的名字，一个集群的名字必须唯一，节点根据集群名字加入到集群中 node.name: 节点名称，可以是自定义的方便分辨的名字，记住master也是一个节点。eg:master,slave node.master: true/false 是否是集群中的主节点。 network.host: 设置network.bind_host 和 publish_host的默认值，这里设置成127.0.0.1和主机ip是有区别的，你可以使用curl -XGET “http://network.host/9200&quot;看到结果 network.bind_host: 绑定服务器ip地址 network.publish_host: 绑定发布的地址 http.port: HttpRest 的接口，这个接口可以让你在浏览器访问 transport.tcp.port: 给Java或者其它节点的服务端口，代码里面用这个。 discovery.zen.ping.unicast.hosts: 这里是一组IP,我一般是使用ip:port这种书写方式，还有很多种方式，详情：zen的介绍 安装中文分词插件ElasticSearch 默认的分词器对于中文的分词不是特别友好，英文的词使用空格隔开的，但是中文就不一样了。默认的分词器会将中文的字一个一个拆分，比如“中国”，默认的分词器就是“中”，“国”，然后去匹配。所以我们需要安装一个中文分词器，这里我选择的是IK插件，它提供了一些友好的中文分词器，并且支持热更新分词热更新，注意根据自己的ES版本来选择IK的版本。github的readme上有两种安装方式，一种是用命令行模式：./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.0.0/elasticsearch-analysis-ik-6.0.0.zip。 另一种就是解压缩包安装方式，去https://github.com/medcl/elasticsearch-analysis-ik/releases下载合适的release版本，然后解压到ES根目录下的plugins目录。 IK 提供了两种分词器：ik_max_word和ik_smart_word。]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch - Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8中的日期时间]]></title>
    <url>%2F2017%2F11%2F29%2Fjava-8-date%2F</url>
    <content type="text"><![CDATA[Java8中的日期时间最近尝鲜，之前了解过Java8的新日期API，但是一直没有真正的去尝试使用，这次有一个新的项目而且不是特别重要，所以就开始了自己的一次尝试。 简介新的日期API是位于java.time.*包下。大致用到的类如下： 1234567ZoneId: 时区ID，用来确定Instant和LocalDateTime互相转换的规则Instant: 用来表示时间线上的一个点LocalDate: 表示没有时区的日期, LocalDate是不可变并且线程安全的LocalTime: 表示没有时区的时间, LocalTime是不可变并且线程安全的LocalDateTime: 表示没有时区的日期时间, LocalDateTime是不可变并且线程安全的Clock: 用于访问当前时刻、日期、时间，用到时区Duration: 用秒和纳秒表示时间的数量 其中跟日期和时间相关的类为：LocalDate,LocalDateTime,LocalTime;根据他们的名字也可以看出来LocalDate主要针对日期操作，LocalDateTime主要针对日期+时间进行操作，LocalTime主要为针对时间进行操作。其实对于西方来说，有Date和Time区分的，分为日期类型和时钟类型。 另外跟日期和时间相关的就是时区了：TimeZone,ZoneId;8中内置了很多时区，可以根据需要选择。 另外一个就是Instant，这个指的时间线上的一个点，比如你看成坐标轴的横坐标。 LocalDate,LocalDateTime,LocalTime新的日期API核心三大块。LocalDate是指的:yyyy-MM-dd，这种类型的日期，比如：2017-11-21。所以你可以猜一下，如果是一个日期工具类，平常我们肯定要用到LocalDate， 它作为工具类应该提供静态方法来获取一个对象所以有LocalDate.now()来获取一个LocalDate对象; 然后我们有日期了，要获取一个LocalDate对象，那么必须有解析的静态方法：LocalDate.parse(string)， 如果想要获取到某一个具体日期的LocalDate对象：LocalDate.of(int year, Month month, int dayOfMonth); 以前的日期api不能很方便的操作日期，如果我们要加一天或者减一天都很麻烦，java8 既然是后来者，肯定要有优化所以有了：plus()和minus(); 加一年或者一个月呢？plusYear(),plushMonth(),plusWeeks(),plushDays(); 有加必有减，所以就可以知道大部分api内容了。然后这三个API的内容基本一样。 推荐使用LocalDateTime,因为它将日期和时钟都获取了，可以很方便你进行各种日期时间操作 时区时区这个操作就好说了，直接看代码，因为java提供了一个默认时区，它会根据当前运行时环境自动判断： 123456789101112131415161718192021@Testpublic void testDate() &#123; System.out.println(ZoneId.systemDefault());// 跟操作系统时区相关 ZoneId zoneId = ZoneId.of("Asia/Shanghai");//CTT System.out.println(zoneId); TimeZone timeZone = TimeZone.getTimeZone("CTT"); System.out.println(timeZone.toZoneId()); System.out.println("===================="); for (String id : TimeZone.getAvailableIDs()) &#123; System.out.print(id + ","); &#125; System.out.println("\n===================="); TimeZone timeZone1 = TimeZone.getTimeZone("Asia/Samarkand"); System.out.println(timeZone1.toZoneId());&#125; InstantInstant，表示的是时间线上的一点，我个人认为主要就是将LocalDateTime和Date新旧API连接起来，他们之间可以如下装换： LocalDateTime 转成 Date 12Instant instant = LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant();Date date = Date.from(instant); Date 转成LocalDateTime 12LocalDateTime localDateTime = LocalDateTime.from(new Date());System.out.println(localDateTime); LocalDate 转成 Date 12Date date = Date.from(LocalDate.now().atStartOfDay().atZone(ZoneId.systemDefault()).toInstant 小技巧日期格式化字符串格式： 123456LocalDateTime now = LocalDateTime.now();DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");System.out.println("默认格式化: " + now);System.out.println("自定义格式化: " + now.format(dateTimeFormatter));LocalDateTime localDateTime = LocalDateTime.parse("2017-08-20 15:26:12", dateTimeFormatter);System.out.println("字符串转LocalDateTime: " + localDateTime); 日期周期Period类用于修改给定日期或获得的两个日期之间的区别。 给初始化的日期添加5天: 1234LocalDate initialDate = LocalDate.parse("2017-07-20");LocalDate finalDate = initialDate.plus(Period.ofDays(5));System.out.println("初始化日期: " + initialDate);System.out.println("加日期之后: " + finalDate); 周期API中提供给我们可以比较两个日期的差别，像下面这样获取差距天数: 12long between = ChronoUnit.DAYS.between(initialDate, finalDate);System.out.println("天数差: " + between); 是否润年12boolean leapYear = LocalDate.now().isLeapYear();System.out.println("是否闰年: " + leapYear); 减去一个月1LocalDate prevMonth = LocalDate.now().minus(1, ChronoUnit.MONTHS); 一周的第几天1234DayOfWeek dayOfWeek = LocalDate.now().getDayOfWeek();System.out.println("周几: " + dayOfWeek);int dayOfMonth = LocalDate.now().getDayOfMonth();System.out.println("第几天？: " + dayOfMonth); 获取这个月第一天12345LocalDate firstDayOfMonth = LocalDate.now() .with(TemporalAdjusters.firstDayOfMonth());System.out.println("这个月的第一天: " + firstDayOfMonth);firstDayOfMonth = firstDayOfMonth.withDayOfMonth(1);System.out.println("这个月的第一天: " + firstDayOfMonth); 判断是否是我的生日判断今天是否是我的生日，例如我的生日是 1995-03-19 1234LocalDate birthday = LocalDate.of(1995, 03, 19);MonthDay birthdayMd = MonthDay.of(birthday.getMonth(), birthday.getDayOfMonth());MonthDay today = MonthDay.from(LocalDate.now());System.out.println("今天是否是我的生日: " + today.equals(birthdayMd)); 判断是否之前，之后12345boolean notBefore = LocalDate.parse("2017-07-20") .isBefore(LocalDate.parse("2017-07-22"));System.out.println("notBefore: " + notBefore);boolean isAfter = LocalDate.parse("2017-07-20").isAfter(LocalDate.parse("2017-07-22"));System.out.println("isAfter: " + isAfter);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker swarm 入门]]></title>
    <url>%2F2017%2F11%2F29%2Fdocker-swarm-guide%2F</url>
    <content type="text"><![CDATA[Swarm 在 Docker 1.12 版本之前属于一个独立的项目，在 Docker 1.12 版本发布之后，该项目合并到了 Docker 中，成为 Docker 的一个子命令。目前，Swarm 是 Docker 社区提供的唯一一个原生支持 Docker 集群管理的工具。它可以把多个 Docker 主机组成的系统转换为单一的虚拟 Docker 主机，使得容器可以组成跨主机的子网网络。 1. Swarm 认识Swarm 是目前 Docker 官方唯一指定（绑定）的集群管理工具。Docker 1.12 内嵌了 swarm mode 集群管理模式。 为了方便演示跨主机网络，我们需要用到一个工具——Docker Machine，这个工具与 Docker Compose、Docker Swarm 并称 Docker 三剑客，下面我们来看看如何安装 Docker Machine： 123$ curl -L https://github.com/docker/machine/releases/download/v0.9.0-rc2/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 安装过程和 Docker Compose 非常类似。现在 Docker 三剑客已经全部到齐了。在开始之前，我们需要了解一些基本概念，有关集群的 Docker 命令如下： docker swarm：集群管理，子命令有 init, join,join-token, leave, update docker node：节点管理，子命令有 demote, inspect,ls, promote, rm, ps, update docker service：服务管理，子命令有 create, inspect, ps, ls ,rm , scale, update docker stack/deploy：试验特性，用于多应用部署，等正式版加进来再说。 2. 创建集群首先使用 Docker Machine 创建一个虚拟机作为 manger 节点。 1234567891011121314151617181920212223$ docker-machine create --driver virtualbox manager1 Running pre-create checks...(manager1) Unable to get the latest Boot2Docker ISO release version: Get https://api.github.com/repos/boot2docker/boot2docker/releases/latest: dial tcp: lookup api.github.com on [::1]:53: server misbehavingCreating machine...(manager1) Unable to get the latest Boot2Docker ISO release version: Get https://api.github.com/repos/boot2docker/boot2docker/releases/latest: dial tcp: lookup api.github.com on [::1]:53: server misbehaving(manager1) Copying /home/zuolan/.docker/machine/cache/boot2docker.iso to /home/zuolan/.docker/machine/machines/manager1/boot2docker.iso...(manager1) Creating VirtualBox VM...(manager1) Creating SSH key...(manager1) Starting the VM...(manager1) Check network to re-create if needed...(manager1) Found a new host-only adapter: &quot;vboxnet0&quot;(manager1) Waiting for an IP...Waiting for machine to be running, this may take a few minutes...Detecting operating system of created instance...Waiting for SSH to be available...Detecting the provisioner...Provisioning with boot2docker...Copying certs to the local machine directory...Copying certs to the remote machine...Setting Docker configuration on the remote daemon...Checking connection to Docker...Docker is up and running!To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env manager1 查看虚拟机的环境变量等信息，包括虚拟机的 IP 地址： 1234567$ docker-machine env manager1export DOCKER_TLS_VERIFY=&quot;1&quot;export DOCKER_HOST=&quot;tcp://192.168.99.100:2376&quot;export DOCKER_CERT_PATH=&quot;/home/zuolan/.docker/machine/machines/manager1&quot;export DOCKER_MACHINE_NAME=&quot;manager1&quot;# Run this command to configure your shell: # eval $(docker-machine env manager1) 然后再创建一个节点作为 work 节点。 1$ docker-machine create --driver virtualbox worker1 现在我们有了两个虚拟主机，使用 Machine 的命令可以查看： 1234$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmanager1 - virtualbox Running tcp://192.168.99.100:2376 v1.12.3 worker1 - virtualbox Running tcp://192.168.99.101:2376 v1.12.3 但是目前这两台虚拟主机并没有什么联系，为了把它们联系起来，我们需要 Swarm 登场了。因为我们使用的是 Docker Machine 创建的虚拟机，因此可以使用 docker-machine ssh 命令来操作虚拟机，在实际生产环境中，并不需要像下面那样操作，只需要执行 docker swarm 即可。 把 manager1 加入集群： 12345678910$ docker-machine ssh manager1 docker swarm init --listen-addr 192.168.99.100:2377 --advertise-addr 192.168.99.100Swarm initialized: current node (23lkbq7uovqsg550qfzup59t6) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 用 –listen-addr 指定监听的 ip 与端口，实际的 Swarm 命令格式如下，本例使用 Docker Machine 来连接虚拟机而已： 1$ docker swarm init --listen-addr &lt;MANAGER-IP&gt;:&lt;PORT&gt; 接下来，再把 work1 加入集群中： 1234$ docker-machine ssh worker1 docker swarm join --token \ SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker. 上面 join 命令中可以添加 –listen-addr $WORKER1_IP:2377 作为监听准备，因为有时候可能会遇到把一个 work 节点提升为 manger 节点的可能，当然本例子没有这个打算就不添加这个参数了。 注意：如果你在新建集群时遇到双网卡情况，可以指定使用哪个 IP，例如上面的例子会有可能遇到下面的错误。 123$ docker-machine ssh manager1 docker swarm init --listen-addr $MANAGER1_IP:2377Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 192.168.99.100 on eth1) - specify one with --advertise-addrexit status 1 发生错误的原因是因为有两个 IP 地址，而 Swarm 不知道用户想使用哪个，因此要指定 IP。 12345678910$ docker-machine ssh manager1 docker swarm init --advertise-addr 192.168.99.100 --listen-addr 192.168.99.100:2377 Swarm initialized: current node (ahvwxicunjd0z8g0eeosjztjx) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 集群初始化成功。 现在我们新建了一个有两个节点的“集群”，现在进入其中一个管理节点使用 docker node 命令来查看节点信息： 1234$ docker-machine ssh manager1 docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS23lkbq7uovqsg550qfzup59t6 * manager1 Ready Active Leaderdqb3fim8zvcob8sycri3hy98a worker1 Ready Active 现在每个节点都归属于 Swarm，并都处在了待机状态。Manager1 是领导者，work1 是工人。 现在，我们继续新建虚拟机 manger2、worker2、worker3，现在已经有五个虚拟机了，使用 docker-machine ls 来查看虚拟机： 123456NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmanager1 - virtualbox Running tcp://192.168.99.100:2376 v1.12.3 manager2 - virtualbox Running tcp://192.168.99.105:2376 v1.12.3 worker1 - virtualbox Running tcp://192.168.99.102:2376 v1.12.3 worker2 - virtualbox Running tcp://192.168.99.103:2376 v1.12.3 worker3 - virtualbox Running tcp://192.168.99.104:2376 v1.12.3 然后我们把剩余的虚拟机也加到集群中。 添加 worker2 到集群中： 1234$ docker-machine ssh worker2 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker. 添加 worker3 到集群中： 1234$ docker-machine ssh worker3 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-c036gwrakjejql06klrfc585r \ 192.168.99.100:2377This node joined a swarm as a worker. 添加 manager2 到集群中：先从 manager1 中获取 manager 的 token： 123456$ docker-machine ssh manager1 docker swarm join-token managerTo add a manager to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \192.168.99.100:2377 然后添加 manager2 到集群中： 1234$ docker-machine ssh manager2 docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \ 192.168.99.100:2377This node joined a swarm as a manager. 现在再来查看集群信息： 1234567$ docker-machine ssh manager2 docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS16w80jnqy2k30yez4wbbaz1l8 worker1 Ready Active 2gkwhzakejj72n5xoxruet71z worker2 Ready Active 35kutfyn1ratch55fn7j3fs4x worker3 Ready Active a9r21g5iq1u6h31myprfwl8ln * manager2 Ready Active Reachabledpo7snxbz2a0dxvx6mf19p35z manager1 Ready Active Leader 3. 建立跨主机网络为了演示更清晰，下面我们把宿主机也加入到集群之中，这样我们使用 Docker 命令操作会清晰很多。直接在本地执行加入集群命令： 1234$ docker swarm join \ --token SWMTKN-1-3z5rzoey0u6onkvvm58f7vgkser5d7z8sfshlu7s4oz2gztlvj-8tn855hkjdb6usrblo9iu700o \ 192.168.99.100:2377This node joined a swarm as a manager. 现在我们有三台 manager，三台 worker。其中一台是宿主机，五台虚拟机。 12345678$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS6z2rpk1t4xucffzlr2rpqb8u3 worker3 Ready Active 7qbr0xd747qena4awx8bx101s * user-pc Ready Active Reachable9v93sav79jqrg0c7051rcxxev manager2 Ready Active Reachablea1ner3zxj3ubsiw4l3p28wrkj worker1 Ready Active a5w7h8j83i11qqi4vlu948mad worker2 Ready Active d4h7vuekklpd6189fcudpfy18 manager1 Ready Active Leader 查看网络状态： 123456$ docker network lsNETWORK ID NAME DRIVER SCOPE764ff31881e5 bridge bridge local fbd9a977aa03 host host local 6p6xlousvsy2 ingress overlay swarm e81af24d643d none null local 可以看到在 swarm 上默认已有一个名为 ingress 的 overlay 网络, 默认在 swarm 里使用，本例子中会创建一个新的 overlay 网络。 123456789$ docker network create --driver overlay swarm_test4dm8cy9y5delvs5vd0ghdd89s$ docker network lsNETWORK ID NAME DRIVER SCOPE764ff31881e5 bridge bridge localfbd9a977aa03 host host local6p6xlousvsy2 ingress overlay swarme81af24d643d none null local4dm8cy9y5del swarm_test overlay swarm 这样一个跨主机网络就搭建好了，但是现在这个网络只是处于待机状态，下一小节我们会在这个网络上部署应用。 4. 在跨主机网络上部署应用首先我们上面创建的节点都是没有镜像的，因此我们要逐一 pull 镜像到节点中，这里我们使用前面搭建的私有仓库。 1234567891011121314151617181920212223242526272829303132333435$ docker-machine ssh manager1 docker pull reg.example.com/library/nginx:alpine alpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh manager2 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker1 docker pull reg.example.com/library/nginx:alpine alpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker2 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine$ docker-machine ssh worker3 docker pull reg.example.com/library/nginx:alpinealpine: Pulling from library/nginxe110a4a17941: Pulling fs layer... ...7648f5d87006: Pull completeDigest: sha256:65063cb82bf508fd5a731318e795b2abbfb0c22222f02ff5c6b30df7f23292feStatus: Downloaded newer image for reg.example.com/library/nginx:alpine 上面使用 docker pull 分别在五个虚拟机节点拉取 nginx:alpine 镜像。接下来我们要在五个节点部署一组 Nginx 服务。 部署的服务使用 swarm_test 跨主机网络。 12$ docker service create --replicas 2 --name helloworld --network=swarm_test nginx:alpine5gz0h2s5agh2d2libvzq6bhgs 查看服务状态： 123$ docker service lsID NAME REPLICAS IMAGE COMMAND5gz0h2s5agh2 helloworld 0/2 nginx:alpine 查看 helloworld 服务详情（为了方便阅读，已调整输出内容）： 1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORay081uome3 helloworld.1 nginx:alpine manager1 Running Preparing 2 seconds ago 16cvore0c96 helloworld.2 nginx:alpine worker2 Running Preparing 2 seconds ago 可以看到两个实例分别运行在两个节点上。 进入两个节点，查看服务状态（为了方便阅读，已调整输出内容）： 123456$ docker-machine ssh manager1 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES119f787622c2 nginx:alpine &quot;nginx -g ...&quot; 4 minutes ago Up 4 minutes 80/tcp, 443/tcp hello ...$ docker-machine ssh worker2 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5db707401a06 nginx:alpine &quot;nginx -g ...&quot; 4 minutes ago Up 4 minutes 80/tcp, 443/tcp hello ... 上面输出做了调整，实际的 NAMES 值为： 12helloworld.1.ay081uome3eejeg4mspa8pdlxhelloworld.2.16cvore0c96rby1vp0sny3mvt 记住上面这两个实例的名称。现在我们来看这两个跨主机的容器是否能互通：首先使用 Machine 进入 manager1 节点，然后使用 docker exec -i 命令进入 helloworld.1 容器中 ping 运行在 worker2 节点的 helloworld.2 容器。 12345678$ docker-machine ssh manager1 docker exec -i helloworld.1.ay081uome3eejeg4mspa8pdlx \ ping helloworld.2.16cvore0c96rby1vp0sny3mvtPING helloworld.2.16cvore0c96rby1vp0sny3mvt (10.0.0.4): 56 data bytes64 bytes from 10.0.0.4: seq=0 ttl=64 time=0.591 ms64 bytes from 10.0.0.4: seq=1 ttl=64 time=0.594 ms64 bytes from 10.0.0.4: seq=2 ttl=64 time=0.624 ms64 bytes from 10.0.0.4: seq=3 ttl=64 time=0.612 ms^C 然后使用 Machine 进入 worker2 节点，然后使用 docker exec -i 命令进入 helloworld.2 容器中 ping 运行在 manager1 节点的 helloworld.1 容器。 12345678$ docker-machine ssh worker2 docker exec -i helloworld.2.16cvore0c96rby1vp0sny3mvt \ ping helloworld.1.ay081uome3eejeg4mspa8pdlx PING helloworld.1.ay081uome3eejeg4mspa8pdlx (10.0.0.3): 56 data bytes64 bytes from 10.0.0.3: seq=0 ttl=64 time=0.466 ms64 bytes from 10.0.0.3: seq=1 ttl=64 time=0.465 ms64 bytes from 10.0.0.3: seq=2 ttl=64 time=0.548 ms64 bytes from 10.0.0.3: seq=3 ttl=64 time=0.689 ms^C 可以看到这两个跨主机的服务集群里面各个容器是可以互相连接的。 为了体现 Swarm 集群的优势，我们可以使用虚拟机的 ping 命令来测试对方虚拟机内的容器。 1234567891011121314$ docker-machine ssh worker2 ping helloworld.1.ay081uome3eejeg4mspa8pdlxPING helloworld.1.ay081uome3eejeg4mspa8pdlx (221.179.46.190): 56 data bytes64 bytes from 221.179.46.190: seq=0 ttl=63 time=48.651 ms64 bytes from 221.179.46.190: seq=1 ttl=63 time=63.239 ms64 bytes from 221.179.46.190: seq=2 ttl=63 time=47.686 ms64 bytes from 221.179.46.190: seq=3 ttl=63 time=61.232 ms^C$ docker-machine ssh manager1 ping helloworld.2.16cvore0c96rby1vp0sny3mvtPING helloworld.2.16cvore0c96rby1vp0sny3mvt (221.179.46.194): 56 data bytes64 bytes from 221.179.46.194: seq=0 ttl=63 time=30.150 ms64 bytes from 221.179.46.194: seq=1 ttl=63 time=54.455 ms64 bytes from 221.179.46.194: seq=2 ttl=63 time=73.862 ms64 bytes from 221.179.46.194: seq=3 ttl=63 time=53.171 ms^C 上面我们使用了虚拟机内部的 ping 去测试容器的延迟，可以看到延迟明显比集群内部的 ping 值要高。 5. Swarm 集群负载现在我们已经学会了 Swarm 集群的部署方法，现在来搭建一个可访问的 Nginx 集群吧。体验最新版的 Swarm 所提供的自动服务发现与集群负载功能。首先删掉上一节我们启动的 helloworld 服务： 12$ docker service rm helloworld helloworld 然后在新建一个服务，提供端口映射参数，使得外界可以访问这些 Nginx 服务： 12$ docker service create --replicas 2 --name helloworld -p 7080:80 --network=swarm_test nginx:alpine9gfziifbii7a6zdqt56kocyun 查看服务运行状态： 123$ docker service ls ID NAME REPLICAS IMAGE COMMAND9gfziifbii7a helloworld 2/2 nginx:alpine 不知你有没有发现，虽然我们使用 –replicas 参数的值都是一样的，但是上一节中获取服务状态时，REPLICAS 返回的是 0/2，现在的 REPLICAS 返回的是 2/2。同样使用 docker service ps 查看服务详细状态时（下面输出已经手动调整为更易读的格式），可以看到实例的 CURRENT STATE 中是 Running 状态的，而上一节中的 CURRENT STATE 中全部是处于 Preparing 状态。 1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine user-pc Running Running 13 seconds ago 7acmhj0u... helloworld.2 nginx:alpine worker2 Running Running 6 seconds ago 这就涉及到 Swarm 内置的发现机制了，目前 Docker 1.12 中 Swarm 已经内置了服务发现工具，我们不再需要像以前使用 Etcd 或者 Consul 这些工具来配置服务发现。对于一个容器来说如果没有外部通信但又是运行中的状态会被服务发现工具认为是 Preparing 状态，本小节例子中因为映射了端口，因此有了 Running 状态。现在我们来看 Swarm 另一个有趣的功能，当我们杀死其中一个节点时，会发生什么。首先 kill 掉 worker2 的实例： 12$ docker-machine ssh worker2 docker kill helloworld.2.7acmhj0udzusv1d7lu2tbuhu4helloworld.2.7acmhj0udzusv1d7lu2tbuhu4 稍等几秒，再来看服务状态： 12345678$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine zuolan-pc Running Running 19 minutes ago 8f866igpl... helloworld.2 nginx:alpine manager1 Running Running 4 seconds ago 7acmhj0u... \_ helloworld.2 nginx:alpine worker2 Shutdown Failed 11 seconds ago ...exit...$ docker service ls ID NAME REPLICAS IMAGE COMMAND9gfziifbii7a helloworld 2/2 nginx:alpine 可以看到即使我们 kill 掉其中一个实例，Swarm 也会迅速把停止的容器撤下来，同时在节点中启动一个新的实例顶上来。这样服务依旧还是两个实例在运行。此时如果你想添加更多实例可以使用 scale 命令： 12$ docker service scale helloworld=3helloworld scaled to 3 查看服务详情，可以看到有三个实例启动了： 1234$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9ikr3agyi... helloworld.1 nginx:alpine user-pc Running Running 30 minutes ago 8f866igpl... helloworld.2 nginx:alpine manager1 Running Running 11 minutes ago 7acmhj0u... \_ helloworld.2 nginx:alpine worker2 Shutdown Failed 11 minutes ago exit1371vexr1jm... helloworld.3 nginx:alpine worker2 Running Running 4 seconds ago 现在如果想减少实例数量，一样可以使用 scale 命令： 12$ docker service scale helloworld=2helloworld scaled to 2 至此，Swarm的主要用法都已经介绍完了，主要讲述了 Swarm 集群网络的创建与部署。介绍了 Swarm 的常规应用，包括 Swarm 的服务发现、负载均衡等，然后使用 Swarm 来配置跨主机容器网络，并在上面部署应用。 简书上看到这篇文章，简直太棒了。原文：左蓝 - Docker Swarm 入门]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 存储过程]]></title>
    <url>%2F2017%2F11%2F21%2Fmysql-stored-procedure%2F</url>
    <content type="text"><![CDATA[MySQL 存储过程MySQL存储过程是一个存储在MySQL数据库中的一段SQL代码，类似于我们平常在程序语言中的一个自定义函数。其实说到底，SQL也是一种语言，它也可以定义函数，定时器等等。只不过它是直接操作的数据库中的数据。 平常能用到存储过程的机会不多，以前在支付公司的时候更别说直接操作存储过程了，这根本不可能。任何需要做的处理逻辑都需要在代码中处理，所以这次接到写存储过程的任务，我还是有点忐忑的，一个是自己根本不会。O(∩_∩)O哈！完成的时间还比较紧。不过有挑战就有机遇，这个是没有错的。整个一周下来，也算是对存储过程的时候有了一点小小的心得，所以做次记录，如果下次遇到，能提醒自己。 定义存储过程定义一个存储过程是比较简单的，类似与我们写一个Java方法，定义的语法为： 12345678DELIMITER // CREATE PROCEDURE proc4(IN param1 INT,OUT param2 varchar(50),INOUT param3 varchar(50))BEGINSELECT max(order_type) INTO param1 FROM cncsen.order_info;select @param1; select * from order_info orderInfo where orderInfo.ORDER_TYPE=param1; END //DELIMITER ; 现在我们逐句来解析下： DELIMITER : 这个是声明一个结束符，可以这样认为，解析器找到之前默认是找到;就认为这句话结束，而我们用DELIMITER将原来的;改为//,这样解析器只有遇到//才会认为当前句子结束； CREATE : 这个和创建表，创建数据库是一个意思； PROCEDURE : 这个是指创建存储过程，类似于TABLE,DATABASE； proc4 ：这个的意思是存储过程名字； OUT / IN / INOUT : 相当于权限定义，OUT 是指该参数可以当做出参，不能作为入参；IN 是指该参数为可以作为入参，不能作为出参；INOUT 是指该参数既可以作为出参，也可以作为入参；（入参指作为参数传进来；出参是指作为返回值，传给其他人用)； INT / VARCHAR : 这个是基本类型，相当于Java里面的预置基本类型。 BEGIN ** END : begin和end定义的是一个语句块，从哪里开始，到哪里结束。可以看到后面还有//，而它刚好就是我们之前定义的结束符号，到这里我们的定义也就完成了。最后当然还要将结束符号切换回;。 这里我们就定义好了一个存储过程，其实我们主要关注的是begin和end之间的事情，我们可以在这里写各种SQL语句。当然我们也可以在这里建立临时表，然后将表中的数据插入到其它表中。 存储过程实例首先创建两个表，一个是user信息表，一个是user日志表。下面是两个表的建表语句： TBL_USER 12345678910CREATE TABLE `tbl_user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(45) DEFAULT NULL, `age` varchar(45) DEFAULT NULL, `address` varchar(45) DEFAULT NULL, `stu_id` int(11) DEFAULT NULL, `create_time` datetime DEFAULT NULL, `last_update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 TBL_USER_LOG 1234567891011121314CREATE TABLE `tbl_user_log` ( `user_id` int(11) DEFAULT NULL, `user_name` varchar(45) DEFAULT NULL, `age` int(11) DEFAULT NULL, `address` varchar(52) DEFAULT NULL, `stu_id` int(11) DEFAULT NULL, `create_time` datetime DEFAULT NULL, `last_update_time` datetime DEFAULT NULL, `now_time` datetime DEFAULT NULL, `param_address` varchar(25) CHARACTER SET big5 DEFAULT NULL COMMENT ' ', `id` int(11) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`), UNIQUE KEY `id_UNIQUE` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 下面我们开始写一个存储过程，该存储过程很简单就是将tbl_user表的数据备份一份到tbl_user_log中比如： 123456789101112DELIMITER //CREATE PROCEDURE proc(IN start_date varchar(32),IN end_date varchar(32), IN name varchar(50),INOUT address varchar(50))BEGIN INSERT INTO tbl_user_log(user_id,user_name,age,address,stu_id,create_time,last_update_time,now_time) SELECT id,name,age,address,stu_id,create_time,last_update_time,current_time FROM tbl_user WHERE create_time&gt;=@start_date AND create_time&lt;@end_date AND name=@name;END //DELIMITER 如果你要执行这个存储过程： 12345set @start_date='2017-11-16 12:12:12';set @end_date='2017-11-17 12:12:12';set @name='';set @address='';CALL proc(@start_date,@end_date,@name,@address); 然后在tbl_user_log表中你就可以看到符合你的需求的数据了。 Event 事件我们经常需要做一些定时任务，在数据库中我们如果需要做一些定时任务，这个时候就需要用到Event了。 创建Event的语法为： 123456789101112131415161718192021CREATE [DEFINER = &#123; user | CURRENT_USER &#125;] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT 'string'] DO event_body;schedule: AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...]interval: quantity &#123;YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND&#125; 具体的字段详情就不一一解释了，如果需要详细了解可以查看Event官网信息。 事件的执行需要打开数据库的配置： 1234SET GLOBAL event_scheduler = ON;SET @@global.event_scheduler = ON;SET GLOBAL event_scheduler = 1;SET @@global.event_scheduler = 1; 具体详情可以查看Event官方文档 游标通常我们需要将查出的数据做一些处理，比如先对一个表进行select，然后再通过某个字段汇总，或者进行一些汇总后的总处理，这个时候如果我们需要遍历记录做处理，那么就需要用到游标了。说白了，游标就是用来遍历select之后的数据的。游标的使用非常简单，我们看一个实例： 12345678910111213141516171819202122232425262728CREATE PROCEDURE curdemo()BEGIN DECLARE done INT DEFAULT FALSE; DECLARE a CHAR(16); DECLARE b, c INT; DECLARE cur1 CURSOR FOR SELECT id,data FROM test.t1; DECLARE cur2 CURSOR FOR SELECT i FROM test.t2; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE; OPEN cur1; OPEN cur2; read_loop: LOOP FETCH cur1 INTO a, b; FETCH cur2 INTO c; IF done THEN LEAVE read_loop; END IF; IF b &lt; c THEN INSERT INTO test.t3 VALUES (a,b); ELSE INSERT INTO test.t3 VALUES (a,c); END IF; END LOOP; CLOSE cur1; CLOSE cur2;END; 主要注意的无非就是DECLARE cur1,先声明;然后open，定义一个LOOP循环，read_loop 这里是循环名称。当然这里也定义了一个退出状态条件-done，最后CLOSE。确实很简单。 定义个test表 123CREATE TABLE `test` ( `name` varchar(50) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 之后用游标先查出tbl_user的数据，然后插入到test表中： 1234567891011121314151617181920DELIMITER |CREATE EVENT event_sale_data ON SCHEDULE EVERY 1 HOUR DO BEGIN DECLARE done INT DEFAULT FALSE; DECLARE CITY_NAME VARCHAR(50); DECLARE cursor_user CURSOR FOR SELECT NAME FROM tbl_user; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE; OPEN cursor_user; read_loop: LOOP FETCH cursor_user INTO user_name; IF done THEN LEAVE read_loop; END IF; INSERT INTO demo.test VALUE(user_name); END LOOP; END |DELIMITER ; 存储过程，游标，事件的基本使用就是这些了，非常简单，一开始做的时候很痛苦，也不知道该怎么下手。冷静下来，每一次不会，都是一个机会。 另外，我在event里面如果将select的数据作为变量传递到存储过程当中的时候，存储过程总是获取值失败，也就是参数没法传递到存储过程中，这个问题很奇怪，还在解决中。 还有一个特别有意思的，如果定义了查询-插入的存储过程，也就是将一个表的数据查询后插入到另一个表中，如果你在mysqlworkbench中直接调用call proc()，那么总是会将上一次查询的数据重复的插入的新表中，也就是重复插入，但是第一次的时候没有，只有手动调用两次以上的时候才会出现重复数据。如果是事件触发，却又没有重复数据插入。这个问题，我怀疑是缓存的问题。嗯，踩过的坑就这两个了。作为备忘，时刻提醒自己。]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 函数式编程-前言]]></title>
    <url>%2F2017%2F11%2F21%2Fjava-8-functional-programming%2F</url>
    <content type="text"><![CDATA[Java 8 函数式编程-前言 从 Java 8 发布到现在已经过去很久了，现在 Java 9 也都已经发布了。国人的习惯总是你发布 8 的时候，我用 7 。你发布 9 的时候，我想 Java 8 应该是普及的时候了。那么正当时，还不来普及下 Java 8 的一个大特性，那就有点说不过去了。 引言 - 什么是函数式编程函数式编程（Funtional Programming，以下简称 fp ），我想从 Java 8 发布之后大家对这个名词都不陌生，它有很多好处，减少代码，增加可读性等等。但是什么是函数式编程了？它和我们普通的 OOP 又有什么不同？ 先来看一份常规打招呼的代码，在 Java7 之前我们大多是这么玩的：1234567891011/*** 打招呼*/ public static String greetJdk7(List&lt;String&gt; names) &#123; String greeting = "Hello "; for(String name : names) &#123; greeting += name + ","; &#125; greeting += "!"; return greeting; &#125; Java8 之后如果用函数式编程的方式改写代码： 12345678public static String greetJdk8(List&lt;String&gt; names) &#123; String greeting = names .stream() .map(name -&gt; name + ",") .reduce("Hello ", (hello, name) -&gt; hello + name); return greeting + "!";&#125; 两者对比起来我们会发现，干净利落，但是看不懂，没关系，之后我会告诉你上面用 Java8 写的代码的含义，并且你也能自己写 Functinal Program。 在开始之前，我想提前和你分享一些知识： 在fp中使用的所有变量都是final的，final的含义意味着，在fp中你不能对它进行改变，总之final的使用，我想你是知道的。 非法实例： 1234567String str = "test";List&lt;String&gt; aList =new ArrayList();aList.forEach(obj-&gt;&#123; str="bbb";// 这样是不允许的，因为str在这里相当于final&#125;) 使用局部参数代替全局变量,比如说： 123456789public class Utils &#123; private static Time time; public static String currTime() &#123; return time.getTime().toString(); &#125;&#125; 替换成 1234567public class Utils &#123; public static String currTime(Time time) &#123; return fixedTime.now().toString(); &#125;&#125; 将方法作为参数使用： 123456789public List&lt;Integer&gt; addOne(List&lt;Integer&gt; numbers) &#123; List&lt;Integer&gt; plusOne = new LinkedList&lt;&gt;(); for(Integer number : numbers) &#123; plusOne.add(number + 1); &#125; return plusOne;&#125; 转换成 123456public List&lt;Integer&gt; addOne(List&lt;Integer&gt; numbers) &#123; return numbers .stream() .map(number -&gt; number + 1) .collect(Collectors.toList());&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>-Java -Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 应用可视化监控]]></title>
    <url>%2F2017%2F11%2F02%2Fspring-boot-actuator-prometheus-grafana%2F</url>
    <content type="text"><![CDATA[Spring Boot 应用可视化监控 使用spring-actuator 并且使用prometheus, grafana 做可视化视图展示 总体过程图： 监控SpringBoot 应用监控SpringBoot 其实也整合了 ops 的功能，也就是运维的部分能力。通过引入包spring-boot-starter-actuator来监控相关的指标信息,详情文档：Actuator 介绍。另外在新版本的actuator中已经有了加密信息，所以对于一些信息的获取可能需要授权，因此我们还需要引入spring-security,pom 文件如下：12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 当然既然引入了spring-security,我们就需要对其做一些配置，我的完整配置是这样的： 1234567891011121314151617181920212223242526272829303132333435363738# 应用的api端口server.port=8818# 启用基础认证security.basic.enabled = true# 安全路径列表，逗号分隔，此处只针对/admin路径进行认证security.basic.path = /admin# 认证使用的用户名security.user.name = admin# 认证使用的密码。 默认情况下，启动时会记录随机密码。security.user.password = 123456# 可以访问管理端点的用户角色列表，逗号分隔management.security.roles = SUPERUSER# actuator暴露接口使用的端口，为了和api接口使用的端口进行分离management.port = 8099# actuator暴露接口的前缀management.context-path = /admin# actuator是否需要安全保证management.security.enabled = true# actuator的metrics接口是否需要安全保证endpoints.metrics.sensitive = false# actuator的metrics接口是否开启endpoints.metrics.enabled=true# actuator的health接口是否需要安全保证endpoints.health.sensitive=false# actuator的health接口是否开启endpoints.health.enabled=true 指标采集采集应用的指标信息，我们使用的是prometheus,相应的我们引入包: 12345&lt;dependency&gt; &lt;groupId&gt;io.prometheus&lt;/groupId&gt; &lt;artifactId&gt;simpleclient_spring_boot&lt;/artifactId&gt; &lt;version&gt;0.0.26&lt;/version&gt;&lt;/dependency&gt; 之后在程序中开启相应的配置： 123456789@SpringBootApplication@EnablePrometheusEndpoint@EnableSpringBootMetricsCollectorpublic class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 这个时候我们可以开始启动我们的应用程序，并且访问相关接口:http://localhost:8099/admin/prometheus 输入 properties 文件中的账号密码，就能看到下图： 数据收集我们采集了指标信息之后就可以开始数据收集了，这个时候我们需要用到Prometheus工具，注意这里是工具，不再是 jar 包了。我使用的是 prometheus 的 docker 镜像，当然你也可以根据需要自己选择,先准备一份 Promethus 的配置文件,更多的配置文档请查看：配置文档：12345678910111213141516global: scrape_interval: 10s scrape_timeout: 10s evaluation_interval: 10mscrape_configs: - job_name: spring-boot scrape_interval: 5s scrape_timeout: 5s metrics_path: /admin/prometheus scheme: http basic_auth: username: admin password: 123456 static_configs: - targets: - 192.168.11.54:8099 之后我们准备服务： 1docker run -d --name prometheus -p 9090:9090 -v D:\chenzhijun\test\actuator\prometheus\prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus 请注意，D:\chenzhijun\test\actuator\prometheus\prometheus.yml ，是我的配置文件存放地址，我们需要将它放到容器里面去，所以用了-v来做文件映射。/etc/prometheus/prometheus.yml这个是容器启动的时候去取的默认配置，这里我是直接覆盖掉了它。prom/prometheus这是镜像，如果本地没有，就回去你设置好的镜像仓库去取。 启动完成后用docker ps看下是否已经启动成功，之后打开浏览器输入：http://localhost:9090/targets,如果看到下图就是成功了： ps: 这里需要注意一点，我们在prometheums.yml中使用的IP地址一定要准确，因为我是docker访问的，所以我使用的是宿主机的地址 数据可视化展示同样的我也是使用 docker ： 1docker run --name grafana -d -p 3000:3000 grafana/grafana 成功之后访问：http:localhost:3000，输入账号密码：admin/admin。之后就开始配置 grafna。 先配置数据源,这里稍微注意下 ip 地址 新建 dashboard 配置图形数据 选择指标,这里的指标数据只能是promethues采集到了的数据http://localhost:9090/graph: 4.1. prometh采集的数据http://localhost:9090/graph 最终结果 源码actuator 源码 参考资料SpringBoot 应用监控踩坑集锦 Spring Boot 应用可视化监控 prometheus_started Grafana]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数-异常统一打印]]></title>
    <url>%2F2017%2F11%2F02%2Funit-error-and-param-log%2F</url>
    <content type="text"><![CDATA[先说明我们业务开发基础框架使用的是 SpringBoot 简述在项目开发中总是需要知道一些常用信息打印，比如出现异常了你可能需要打印日志，为了便于分析，你可能也需要打印埋点数据，或者请求参数之类的。 这类操作可能在任何地方都有，如果分别取处理，感觉上不是特别合适。需要写大量代码并且维护量大。我的做法就是使用@Aspect,@ControllerAdvice利用切面来做统一的参数和异常处理。 处理异常使用ControllerAdvice注解来监听全局异常。这样的好处是，所有的出现异常的点我都是直接往上抛，而不用在每个业务里面去处理。在里面我处理了是否为我自己业务逻辑抛出的异常，如果是的话根据错误码返回响应的错误提示。如果不是业务异常，那么我就转换成系统异常，最后给用户的也是“系统出现异常”等提示语，而不会出现异常堆栈代码。 123456789101112131415161718192021222324@ControllerAdvicepublic class ExceptionHandler &#123; private Logger logger = LoggerFactory.getLogger(ExceptionHandler.class); @ExceptionHandler(value = Exception.class) @ResponseBody public ResultData exceptionHandler(Exception e) &#123; logger.error("系统异常：&#123;&#125;", e); ResultData ret = new ResultData&lt;&gt;(); ret.setData(null); ret.setSucceed(false); if (e instanceof BusinessException) &#123; BusinessException serverException = (BusinessException) e; ret.setErrorCode(serverException.getErrorCode()); ret.setErrorMsg(SpringUtil.getMessage(serverException.getErrorCode(), serverException.getMessage())); &#125; else &#123; ret.setErrorCode("100002"); ret.setErrorMsg(SpringUtil.getMessage("100002")); &#125; return ret; &#125; 处理参数利用@Aspect，我将请求的参数，和响应的返回值都做了相应的日志打印。这种方式可能并不是特别好，因为可能响应的信息很多，那么可能系统很快就会磁盘爆满，但是在前期我想还是很有必要的，后期我们可能根据日志的级别来做响应的控制。 下面的代码主要是监控在 controller 层的参数和响应： 1234567891011121314151617181920212223242526272829303132/** * @author chen * @version V1.0 * @date 2017/10/27 */@Aspect@Componentpublic class ParamLogAspect &#123; private Logger logger = LoggerFactory.getLogger(ParamLogAspect.class); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;&gt;(); @Before(value = "execution(public * com.web..*.*(..))") public void doBefore(JoinPoint joinPoint) &#123; startTime.set(System.currentTimeMillis()); ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); logger.info("URL : &#123;&#125;", request.getRequestURL().toString()); logger.info("HTTP_METHOD : &#123;&#125; ", request.getMethod()); logger.info("IP : &#123;&#125; ", IPUtil.getIpAddress(request)); logger.info("CLASS_METHOD : &#123;&#125;.&#123;&#125;", joinPoint.getSignature().getDeclaringTypeName(), joinPoint.getSignature().getName()); logger.info("REQUEST_PARAM : &#123;&#125;", Arrays.toString(joinPoint.getArgs())); &#125; @AfterReturning(returning = "resp", value = "execution(public * com.web..*.*(..))") public void doAfterReturning(Object resp) throws Exception &#123; logger.info("RESPONSE : &#123;&#125;", JSON.toJSONString(resp)); logger.info("SPEND TIME : &#123;&#125;", (System.currentTimeMillis() - startTime.get())); &#125;&#125; 总结总之，方式有很多，我也见过通过实现继承的接口方式来做异常处理的，不管哪种我们的目的都是一个，尽量不给用户不好的提示信息，毕竟我们的目标是：代码帅，运行快]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常堆栈信息打印]]></title>
    <url>%2F2017%2F11%2F02%2Fhow-to-print-error-stack%2F</url>
    <content type="text"><![CDATA[异常堆栈信息打印最近在开发积分和优惠券，忙的焦头烂额，基本上每天都在想着代码怎么写，功能怎么实现。整个架构是怎样。没有产品经理来梳理需求，基本上靠自己写。唉~ 。比较痛苦的是，我开发完功能，然后交互自己又给我随意乱加功能，导致我有些代码的重写，当然直接接口的封装而已，但是这种感觉很不爽，我都开发完了，你也不知会我一下，就乱来。不扯了。今天要记录下一个功能，就是将异常信息，通过邮件发送给我。过程中遇到一个问题，怎样获取到异常堆栈信息并将它输出为字符串，来通过邮件的形式发送出来。 其中最为主要的就是获取异常堆栈，并将之输出为字符串； 下面是一个实例： 123456789101112131415161718192021222324252627282930@org.junit.Testpublic void testNull() &#123; try &#123; testNullException(); &#125; catch (Exception e) &#123; StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw); System.out.println("e.getCause() : " + e.getCause()); System.out.println("e.getSuppressed() ： " + e.getSuppressed()); System.out.println("e.getStackTrace() ： " + e.getStackTrace()); System.out.println("e.getLocalizedMessage() ： " + e.getLocalizedMessage()); System.out.println("e.getMessage() ： " + e.getMessage()); System.out.println("e.getClass() ： " + e.getClass()); /** * e.getCause() : null e.getSuppressed() ： [Ljava.lang.Throwable;@5b464ce8 e.getStackTrace() ： [Ljava.lang.StackTraceElement;@57829d67 e.getLocalizedMessage() ： null e.getMessage() ： null e.getClass() ： class java.lang.NullPointerException */ System.out.println("=================="); e.printStackTrace(pw); System.out.println("e " + sw.toString()); &#125;&#125;private void testNullException() &#123; List&lt;String&gt; list = null; System.out.println(list.get(0).toString());&#125; 其中的主要原理就是通过throwable的printStackTrace(pw)将输出输出到流中，然后通过字符流将其中的数据转换成字符。所以我们可以进行一些封装： 1234567891011public static String getStackTrace(Throwable throwable) &#123; StringWriter sw = new StringWriter(); PrintWriter printWriter = new PrintWriter(sw); try &#123; throwable.printStackTrace(printWriter); return sw.toString(); &#125; finally &#123; printWriter.close(); &#125;&#125; 可能你会想说，为啥StringWriter没有在finally关闭，其实不是不关闭，而是sw.close()它就是一个空实现，调用它的close方法，其实也没做操作。 有问题欢迎留言交流，或者联系。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-模板方法模式]]></title>
    <url>%2F2017%2F10%2F09%2Fjava-design-pattern-template-method%2F</url>
    <content type="text"><![CDATA[模板方法设计模式简述模板方法模式,它其实肯定是有一个模板的.模板是什么?就是假定的标准.而这个标准由我们定义.模板方法指什么了?它就是封装了一系列操作到一个方法里面.为啥这么说?看生活中的一个实例: 生活例子 泡咖啡: 如果我们想要喝咖啡,我们会怎么做? 烧水; 放咖啡; 选个马克杯子; 加点牛奶或糖. 泡一杯咖啡,我们大概就是这样操作.让我们再看看有些人不喜欢咖啡,喜欢喝茶.那么应该是怎样了? 泡茶: 如果喜欢喝茶,泡一杯茶怎么做? 烧水; 放茶叶; 选个保温杯?; 加奶或者加柠檬?.加点奶变奶茶,或者加点柠檬变成柠檬茶? 泡茶的步骤,我们大概就是这样了. 看看泡茶和泡咖啡的流程,它是不是可以抽象出来?比如说 第一步: 烧水,我统一在一个地方烧水不就行了?要烧两次浪费电么(电烧水壶). 第二步: 放咖啡/放茶叶,简单来说,这不是就放饮料而已么. 第三步: 选杯子,马克杯还是保温杯不就是选一个杯子啊. 第四步: 加奶,加糖,加柠檬.总的来说就是加调料. 如果仔细想想看这几个步骤,总体来说他们都是一样的.所以我们抽象出来就是: 烧水 放饮料 选杯子 放调料 如果我们在代码里面实现就是: 饮料模板类:BeverageTemplate.java,饮料模板类定义了一系列的模板方法,其中如果是共用的,比如boilwater(),那么子类继承之后就无需再单独实现了,另外如果是其它的方法,那么就交由具体的子类单独去实现. 12345678910111213141516171819202122232425262728/** * 饮料模板 * Beverage:饮料 */public abstract class BeverageTemplate &#123; /** *准备饮料 * 将一系列方法按照一个固定的模板排布 */ public void prepareBeverage()&#123; boilWater();//烧水 putBeverage();//放饮料 packageCup();//选杯子装杯 addCondiment();//放调料 &#125; private void boilWater() &#123; System.out.println("烧开水..."); &#125; protected abstract void addCondiment(); protected abstract void packageCup(); protected abstract void putBeverage();&#125; 咖啡类,继承自饮料模板类,并且重写了几个模板方法: 1234567891011121314151617public class CoffeeBeverage extends BeverageTemplate &#123; @Override protected void addCondiment() &#123; System.out.println("加糖,咖啡太苦..."); &#125; @Override protected void packageCup() &#123; System.out.println("选个星巴克的杯子好拍(zhuang)照(bi)"); &#125; @Override protected void putBeverage() &#123; System.out.println("放现磨咖啡"); &#125;&#125; 茶类,继承自饮料模板类,重写父类的方法: 123456789101112131415161718public class TeaBeverage extends BeverageTemplate&#123; @Override protected void addCondiment() &#123; System.out.println("放点奶,变奶茶"); &#125; @Override protected void packageCup() &#123; System.out.println("选个保温杯,多喝几次"); &#125; @Override protected void putBeverage() &#123; System.out.println("放茶叶"); &#125;&#125; 可以看到每一个子类都有自己的具体操作实现,但是他们的操作步骤确已经订好了.这有点类似是,父类只是定义接口,具体实现交个子类. 但是你可能会想,加不加调料,那是喝的人说了算啊.我们现在的定义方式不就是所有的人都要加,那如果我想喝的就是绿茶,不需要奶,怎么办? 这种时候,我们其实可以稍微做些改动,修改一下我们的模板类: 1234567891011121314151617181920212223242526272829303132333435/** * 饮料模板 * Beverage:饮料 */public abstract class BeverageTemplate &#123; /** *准备饮料 * 将一系列方法按照一个固定的模板排布 */ public void prepareBeverage()&#123; boilWater();//烧水 putBeverage();//放饮料 packageCup();//选杯子装杯 if(needCondiment())&#123; addCondiment();//放调料 &#125; &#125; // 是否需要调料 private boolean needCondiment() &#123; return true; &#125; private void boilWater() &#123; System.out.println("烧开水..."); &#125; protected abstract void addCondiment(); protected abstract void packageCup(); protected abstract void putBeverage();&#125; 修改后我们可以看到,我们成功的加入了一个钩子,这个钩子有啥用?那就是当我们的子类如果不需要调料的时候,子类重新实现一下即可.我们也可以实现一个空方法如下面: 123456789101112131415161718192021222324252627282930313233343536373839/** * 饮料模板 * Beverage:饮料 */public abstract class BeverageTemplate &#123; /** *准备饮料 * 将一系列方法按照一个固定的模板排布 */ public void prepareBeverage()&#123; boilWater();//烧水 putBeverage();//放饮料 packageCup();//选杯子装杯 if(needCondiment())&#123; addCondiment();//放调料 &#125; otherMethod(); &#125; protected void otherMethod()&#123;&#125;; // 是否需要调料 private boolean needCondiment() &#123; return true; &#125; private void boilWater() &#123; System.out.println("烧开水..."); &#125; protected abstract void addCondiment(); protected abstract void packageCup(); protected abstract void putBeverage();&#125; 像上面这种,我们加入了otherMethod()然后里面实现是一个空方法.这种空方法如果看jdk会发现有很多都是这样弄的.比如io包里面就有. 总结现在是时候来总结一下了.茶和咖啡,我们将通用步骤抽出来,然后放到父类的通用模板里面,而具体的实现操作我们交给子类,子类的各实现中,我们应该不要互相调用.尤其是不应该调用父类的方法.这样的目的是减少依赖.即父类可以调用子类的实现,而子类中的实现应该避免去调用父类的组件.这个有点类似考大学,学子只需要好好读好书,考高分就行了,而学校是否录取你,你填好自愿之后,你也别去问了,它需要的时候自然回来通知你.书中称这种为好莱坞原则:别去调用高层次组建,高层次组件回来调用你. 现在该给模板方法模式下定义了:在一个方法中定义了一个算法的骨架,而将一些步骤延迟到子类中.模板方法使得子类可以在不改变算法结构的情况下,重新定义算放中的某些步骤. 仔细回想,我们的prepare()它可不可以算个算法步骤?第一步干嘛,第二步干嘛..这就是算法骨架么.然后所有的子类都将实现抽象方法.这不就是将实现放到子类中了么. 好了,就到此了.如果你有什么问题,欢迎一起探讨. 参考资料: &lt;&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring周报翻译 - 2017年10月3日周报]]></title>
    <url>%2F2017%2F10%2F09%2Fspring-io-weekly-newspaper%2F</url>
    <content type="text"><![CDATA[10月3号周报 今日突然兴起，想翻译文章，其实之前的我在LinuxCN上也翻译过，只是那边大多是Linux的文章，其实我想翻译Java周边的。那边的文章我也有翻译过，只是总觉得缺少什么，后来接触了Spring4ALL,也就随口问了下，没想到还竟然有人回复了我,所以也就有了下面这篇文章，算作投名状吧。 其实，能翻译我还是很高兴的，因为我觉得那26个字母非常可爱。^_^ 下面是是10月3号在Spring官网看到的周报内容，取出主要内容作为本文主体： 怎么使用Spring Framwork5来构建基于websocket的响应式应用 Spring AMQP和Spring Integration开发领导者Gary Russell发布了Spring AMQP 2.0.RC2。 经过一段长的时间，Spring Framework 5.0.GA现在终于发布了！新版本集成了Reactor项目，包括响应式web运行时环境，Kotlin扩展，以及全面基于Java EE 7和Java 8。新的发行版具有太多的新东西,如果想要知道更多的消息，查看版本说明，然后查看更新内容 Spring Reactor团队成员Simon Baslé宣布发布Spring和Reactor集成的新版本Reactor Bismuth。这个版本为Spring Framework 5.0奠定了基础，它自己本身也包含了非常多的功能！ Spring Framework 5.0版本更新了一些其他的项目，其中包括发布了Spring Data Kay。自从Spring Data于2009年成立以来，这是Spring Data更新的最大版本！该新版本以Spring Framework 5.0，Java 8和Java EE 7作为基准。它包括一个改进的仓储(repository) API（完整支持Optional&lt;T&gt;），支持响应式数据访问（Cassandra，Couchbase，MongoDB和Redis），新发布版本新增Spring Data Geode，使用非空注解并且优化了运行时检查空注解，通过Kotlin构造方法，支持Kotlin的null安全和不可变数据类，支持兼容Java 9和然后还有更多的请看文档：Spring Data Kay！ Spring消息中间件集成团队成员Artem Bilan宣布发布Spring for Apache Kafka 2.0.GA.新版本包括支持Apache Kafka，支持事务，消息头匹配，Apache Kafka 的Streams支持，新的KafkaAdmin，增加@KafkaListener和Consumer错误处理和群组支持的方案。它还支持使用@EmbeddedKafka进行测试。 Spring Cloud团队成员Ryan Baxter宣布发布Spring Cloud Dalston SR4。新版本更新了Spring Cloud Contract，Spring Cloud Config，Spring Cloud Commons，Spring Cloud Netflix和Spring Cloud Sleuth。 Spring REST Docs 开发领导者Andy Wilkinson宣布发布Spring REST Docs 1.2.2.RELEASE。这个维护版本包括一些错误修复和文档的改进，推荐大家升级 查看Spring Framework 5的全新的通过构造方法实现注解的实现 德语访谈中Andreas Falk提到的Spring Framework 5.0和Spring Security 5.0中的新功能。 由Zoltan Altfatter发表的这篇文章介绍了如何在Spring Integration流程中引入一个新的JMS消息来启动Spring Batch作业。 Rohit Kelapure: 为什么Pivotal公司的Cloud Foundry是运行Spring Boot应用程序的最佳选择 在这篇文章中，Cristina Negrean介绍了如何使用Spring Cloud Data Flow进行实时分析。 Gabriela Motroc在JAXEnter网上发布了一篇关于Spring Framework 5的新文章。非常多的功能文章都包含了 Aboullaite Mohammed：使用Elasticsearch和Kibana和Spring Boot集合来监控一些指标 Ordina JWorks：使用Spring Cloud来对微服务进行安全防护]]></content>
      <categories>
        <category>Spring周报翻译</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>周报</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-命令模式]]></title>
    <url>%2F2017%2F10%2F04%2Fjava-design-pattern-command%2F</url>
    <content type="text"><![CDATA[设计模式-命令模式命令模式是啥啊?感觉从字面上来理解,就是命令-执行.那么谁下命令?谁执行?执行的人从哪里知道命令?又怎么知道要执行哪些命令?命令模式的定义: 将’请求’封装成对象,以便使用不同的请求,队列或者日志来参数化其它对象.命令模式也支持可撤销的操作 命令模式让我们假设一件事情,如果有个遥控器,上面有个按钮,按钮可以控制家里的一件东西,比如说开关门,开关灯,开关电视.那么我们该怎么做了?其实这个就相当于一个命令模式了.人是下命令的.也就是按钮执行就是一个命令,按钮对应了什么操作,按下的操作,这个操作怎么执行了?是门执行,还是灯去执行,还是电视了?如果可以,我们可以想想另外一个例子:下馆子.通常我们去到餐馆,服务员给个菜单,然后我们写上我们要吃的菜;完事之后服务员拿走菜单,然后告诉厨师,厨师拿到菜单后开始炒菜.这个过程中,”我们”就相当于一个客户端,我们将菜品(请求)封装成了一个订单对象.服务员拿到请求,告诉厨师请求来了,厨师拿到订单看到里面的菜品(命令)就开始执行.命令模式过程图: 类关系图 这个过程如果想清楚了,回到我们为遥控器编程,现在我们拿到的就是一个空遥控器,要控制那个东西也不明白,所以让我们尝试编程看. 命令编程我们从简单的开始,必不可少的是命令的执行者,我们假设这次我们控制的是灯泡,Light.java:123456//命令执行者,命令执行对象public class Light &#123; public void on() &#123; System.out.println("light is on..."); &#125;&#125; 然后我们想要让它执行命令,想想看在订餐的时候,菜单是不是都是一样的? 所以我们先定义一个命令标记接口Command.java:123456789/** * 命令接口 */public interface Command &#123; /** * 命令接口具体执行 */ public void execute();&#125; 之后灯的开关,是不是执行一个命令,我们先只管”灯开”这一个命令(暂时不管灯关),这个命令肯定是继承命令标记接口的,因为我们肯定是要先统一了菜谱,然后再让客户去点菜的.所以我们的灯开命令LightOnCommand.java:123456789101112131415161718192021/** * 灯开的命令 */public class LightOnCommand implements Command &#123; // 命令执行者 Light light; //命令执行方法 @Override public void execute() &#123; light.on(); &#125; /** * 设置命令执行者 * @param light */ public LightOnCommand(Light light) &#123; this.light = light; &#125;&#125; 命令对象也有了,我们现在来让遥控器适配上,我们建立一个简单遥控器类SimpleRemoteControl.java:12345678910111213141516171819/** * 遥控器 */public class SimpleRemoteControl &#123; // 命令执行 Command slot; public SimpleRemoteControl() &#123; &#125; // 设置按钮命令,以后也可以绑定灯的开关,或者门的开关. public void setSlot(Command slot) &#123; this.slot = slot; &#125; //遥控器按钮按下去 public void buttonWasPressed()&#123; slot.execute(); &#125;&#125; 这些做完之后,我们看看,遥控器里面我们组合的是一个命令,而命令又统一继承自一个接口,然后各个命令的实现里面又各个命令执行者的组合实例.这样来看,我们按下遥控器,遥控器就会执行命令,命令会知道是那个执行者去执行.现在编写测试类RemoteControlTest.java:1234567891011121314151617//测试类public class RemoteControlTest &#123; public static void main(String[] args) &#123; //遥控器 SimpleRemoteControl remote = new SimpleRemoteControl(); //电灯 Light light = new Light(); LightOnCommand lightOnCommand = new LightOnCommand(light); //命令设置 remote.setSlot(lightOnCommand); //命令执行 remote.buttonWasPressed(); &#125;&#125; 这样,我们对遥控器编程算是完成了.如果我们想控制门,或者控制电视,那我们该怎么弄?其实就模仿灯的方式就行了.在这个实例里面,我们可以看到,遥控器其实是不知道具体是让灯开,还是让门开的,就是遥控器的按钮命令是可以任意的,这个应该算是解耦了把.遥控器只知道命令只要实现了Command的接口就可以了. 命令模式感觉这样确实非常清晰的,代码的demo版本理解和实现也不困难,但是不知道在实际开发中,怎么去用它,将它用在些设呢么地方.我在电商系统中的购物车下单是否可以这样使用了? 但是貌似也就一个地方用到了下单,那么我直接实现是否会比使用设计模式会更好了? 现在我感觉我的问题是,不知道怎么去扩展,没地方扩展,自然就会觉得简单直接的方式最好,也就无需去考虑适设计模式.但是设计模式应该在什么地方,什么时候去使用了?真是路,从来就没有一条好走的路.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-工厂模式]]></title>
    <url>%2F2017%2F10%2F04%2Fjava-design-pattern-factory%2F</url>
    <content type="text"><![CDATA[工厂模式最近看了很久的工产模式，一直没敢下笔。因为我发现我就算弄了很久也就明白了简单工厂和工厂模式，抽象工厂一直没有弄的太明白。今天想着不如试试看，至少先把自己会的记录下来吧，不然越久越容易忘记了。 工厂模式定义:定义了一个创建对象的接口,但由子类决定实例化的具体对象是哪个.工厂方法让类把实例化推迟到了子类中. 工厂简述想想一个例子，假如我们想吃批萨，那么如果是简单的话，我们可能就买个面粉自己揉几下然后放到烤箱里面。只要出来的时候是个饼的形状，也就差不多了（对我这种不调食的人来说…）。但是我们可能还有一些有追求的人，他可能想吃海鲜批萨，可能想吃芝士批萨，可能想吃原味批萨。这么多要求，你觉得在家里自己做麻烦么？麻烦怎么办？丢个别人呗。那么我们就建个工厂，专门给我们做批萨，而我想吃什么批萨的时候，打个电话过去，告诉它给我做个什么口味的批萨，然后它把吃的给我送过来就好了。这种方式就基本上就是一个工厂模式了。很显然，我们将”做批萨解耦出来了。工厂负责做批萨，而我只要批萨就行，至于你批萨是怎么做的，你工厂随便弄，最后给个批萨给我，就行。 简单工厂其实简单工厂是工厂模式的一种特例，严格来说它就是工厂模式。我们先来看看有哪些东西我们要在实例中用的：批萨，吃批萨的人或者说需要批萨的客户端，生成批萨的工厂。 产品批萨，如果各个地方不同，可以实现该类。Pizza.java: 1234567891011121314151617181920212223242526272829303132333435363738public class Pizza &#123; private String pizzaName; private String cheese; public String getCheese()&#123; return cheese; &#125; public String setCheese(String cheese)&#123; this.cheese = cheese; &#125; public String getPizzaName() &#123; return pizzaName; &#125; public void setPizzaName(String pizzaName) &#123; this.pizzaName = pizzaName; &#125; public Pizza() &#123; &#125; public Pizza(String pizzaName) &#123; this.pizzaName = pizzaName; &#125; public void prepare() &#123; System.out.println("prepare..."); &#125; public void bake() &#123; System.out.println("bake..."); &#125; public void box() &#123; System.out.println("box..."); &#125;&#125; 简单批萨工厂，SimplePizzaFactory.java: 1234567891011121314public class SimplePizzaFactory &#123; public Pizza createPizza(String pizzaType) &#123; Pizza pizza = null; if (pizzaType.equals("cheese")) &#123; // 芝士批萨 pizza = new Pizza("cheese"); &#125;else if(pizzaType.equals("chicken")) &#123;//鸡肉批萨 pizza = new Pizza("chicken"); &#125; return pizza; &#125;&#125; 需要批萨的人，我们将它看成批萨店好了:PizzaStore.java: 123456789101112131415161718public class PizzaStore &#123; private SimplePizzaFactory factory; public PizzaStore(SimplePizzaFactory factory) &#123; this.factory = factory; &#125; public Pizza orderPizza(String pizzaType) &#123; Pizza pizza = factory.createPizza(pizzaType); pizza.prepare(); pizza.bake(); pizza.box(); return pizza; &#125;&#125; 现在如果有人加盟店，继承自PizzaStore，然后传入他们的工厂就行了。比如说我想在深圳开一个深圳批萨，然后深圳本地有个当地的批萨工厂。 1234567public class ShenZhenPizzaStore extends PizzaStore&#123; public static void main(String[] args)&#123; SimplePizzaFactory simplePizzaFactory = new ShenzhenPizzaFactory(); PizzaStore pizzaStore = new PizzaStore(simplePizzaFactory); pizzaSore.orderPizza("chicken"); &#125;&#125; 这样任何在深圳地方的人想吃批萨只要在批萨店调用orderPizza方法就行了。 不过想想看，如果有一天我们有个长沙批萨店，他们要特殊化，主打高端人士批萨，这种时候他可能就希望是自己来决定是自己来生产批萨还是找长沙的批萨工厂。这种时候怎么办了？我们将代码做些改进，既然不管那个加盟店都必须要继承自PizzaSore,那我们将PizzaStore做一些改动： 1234567891011public abstract class PizzaStore &#123; public Pizza orderPizza(String pizzaType)&#123; Pizza pizza = createPizza(pizzaType); pizza.prepare(); pizza.bake(); pizza.box(); return pizza; &#125; protected abstract Pizza createPizza(String pizzaType);&#125; Pizza1234567891011121314151617181920212223242526272829public abstract class Pizza &#123; private String pizzaName; public String getPizzaName() &#123; return pizzaName; &#125; public void setPizzaName(String pizzaName) &#123; this.pizzaName = pizzaName; &#125; public void prepare() &#123; System.out.println("prepare..."); &#125; public void bake() &#123; System.out.println("bake..."); &#125; public void box() &#123; System.out.println("box..."); &#125;&#125;public class ChangShaPizza extends Pizza&#123; public ChangShaPizza()&#123; name = "ChangSha"; &#125;&#125; 然后长沙的pizza店：123456789public class ChangShaPizzaStore extends PizzaStore &#123; @Override protected Pizza createPizza(String pizzaType) &#123; if (pizzaType.equals("ChangSha")) &#123; return new ChangShaPizza(); &#125; return null; &#125;&#125; 我们将它定义为虚拟抽象类，并且增加抽象方法。这样每一个继承它的子类都必须自己实现一次createPizza方法，这样子类的实现，父类就完全不管了，只要定个统一接口就好了。 该总结一下啦，修改后我们的类主要是：PizzaStore,Pizza。我们将他们分下类：创建者类和产品类。 PizzaStore ：可以相当于创建者(Creator)类，它是一个抽象方法，定义了一个抽象工厂方法，让子类实现该方法来制造产品。创建者里面也会有抽象的产品(这里相当于pizza，因为你也可以继承pizza然后实现自己的独特pizza)的依赖，不过创建具体哪种产品交由子类实现。我们在ChangShaPizzaStore里面就是实现了具体的产品。 Pizza ： 相当于抽象产品类，其实我们也可以实现我们自己的Pizza，然后给它加点形状，特色啥的。这是个相当于工厂生产的产品抽象.如下图所示: 其中Creator就相当于我们的PizzaStore所有的加盟店必须继承它,并且实现创建pizza的方法,而ConcreteCreator是唯一知道应该创建那个一个具体产品的(ChangShaPizza),所有的产品都实现一个产品接口Product. 可以看到如果加盟店越多,可能pizza的具体种类也就越多,而我们的PizzaStore只依赖于Pizza这个顶级产品类,而具体是ChangShaPizza还是BeijingPizza它就不需要管那么多了.这里有个原则可以引出来:依赖倒置原则:要依赖抽象,而不依赖具体实现类. 抽象工产模式抽象工厂模式:提供一个接口,用于创建相关或依赖对象的家族,而不需要明确指定具体类. 可以看到,我们将工厂创建批萨的工厂也用一个工产接口来实现,而我们的新的store只依赖于工厂接口而不再是任何一个具体工厂接口. 如何理解了? 比如工厂模式中,我们生产pizza是基于子类的具体实现的,子类里面去进行具体的pizza封装,然后进行实例化,而抽象工产,它是不会去创建具体实例对象的,它的职责是创建相关或依赖对象的家族,它的每一个接口都是创建的一个单独的对象.比如Pizza,它会去创建做一个pizza需要的各种原料,各个原料其实都是一个单独的对象个体,它称为一个原料工厂.原料工厂是一个接口,然后各个实现了该接口的具体类去具体实现每种原来如何创建. 1234567891011public class BeijingPizza extends Pizza&#123; PizzaIngredientFactory pizzaIngredientFactory; public BeijingPizza(PizzaIngredientFactory pizzaIngredientFactory)&#123; this.pizzaIngredientFactory = pizzaIngredientFactory; &#125; void prepare()&#123; cheese = pizzaIngredientFactory.createCheese(); &#125;&#125; 工厂模式和抽象工厂模式,他们之前一个是用的继承方式(工厂模式),一个是用的组合(抽象工厂). 抽象工厂: 如果需要创建产品家族和想让制造的想关产品集合起来的时候可以用抽象工厂. 工厂模式: 把客户端代码从需要实例化的具体类中解耦. 总之工厂就是将对象的创建封装了起来,更加的松耦合,做到弹性的设计.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三方授权登录(oauth2)--Github授权登录]]></title>
    <url>%2F2017%2F09%2F29%2Foauth2-github-login%2F</url>
    <content type="text"><![CDATA[第三方授权登录-github授权登录 现在时下的框架都是非常流行第三方的授权登录，比如QQ，微信，微博，或者github等等，都是基于OAuth2授权令牌。 前期的准备在现在很多互联网项目当中很少没有第三方登录的，本来想用QQ或者微信来弄，但是发现认证有点麻烦，后来看到github上面的授权很简单，几乎只要有一个github账号就能开始开发了。其实不管是哪个平台授权登录，只要了解了一个原理，其它的都类似了。整个流程图如下： github 账号默认已经有了，如果没有的话作为开发者还是去注册一个吧，不多说了。 注册一个应用注册一个我们的应用，该应用就是我们需要用第三方授权登录的应用,github有个好处，可以用127.0.0.1直接挂到本地进行测试，不知道其他的第三方可不可以，还待验证。 获取到client_id和client_key注册新应用之后我们主要是要拿到client和client_key:拿到之后我们就可以开始开发了。 SpringBoot实现授权登录首先页面要有一个请求授权的操作，我们简化如下: 1&lt;a href="http://github.com/login/oauth/authorize?client_id=0430a6c311c3dd1f4869"&gt; github登录&lt;/a&gt; 之后，需要在配置的回调地址就是我们127.0.0.1设置的url地址： 123456789101112131415@GetMapping("login/github")public String loginGithub(String code, Model model) &#123; logger.info(code); System.out.println("code: " + code); String url = "https://github.com/login/oauth/access_token"; MultiValueMap&lt;String, String&gt; map = new LinkedMultiValueMap&lt;&gt;(); map.add("client_id", ConfigConstant.CLIENT_ID); map.add("client_secret", ConfigConstant.CLIENT_SECRET); map.add("code", code); JSONObject jsonObject = RemoteRequestUtil.post(url, map, JSONObject.class); System.out.println("jsonObject:" + jsonObject); model.addAttribute("result", jsonObject); return "login/success";&#125; 上面用了一个RemoteRequestUtils,其实你只要有一个可以调用post方法的就可以了,我用的是RestTemplate来实现远程调用的，请注意:RestTemplate 只支持传参数MultiValueMap。12345678910111213141516171819202122232425262728293031public class RemoteRequestUtil &#123; private static RestTemplate restTemplate = new RestTemplate(); public static &lt;T&gt; T post(String url, Object request, Class&lt;T&gt; clazz, Object... uriValues) &#123; return restTemplate.postForObject(url, request, clazz, uriValues); &#125; public static String get(String url) &#123; System.out.println(url); return restTemplate.getForObject(url, String.class); &#125; public static &lt;T&gt; T getT(String url, Class&lt;T&gt; clazz) &#123; System.out.println(url); return restTemplate.getForObject(url, clazz); &#125; public static &lt;T&gt; T postT(String url, String param, Class&lt;T&gt; clazz) &#123; System.out.println(url + ":" + param); return restTemplate.postForObject(url, param, clazz); &#125; public static &lt;T&gt; T postT(String url, Object param, Class&lt;T&gt; clazz) &#123; System.out.println(url + ":" + param); return restTemplate.postForObject(url, param, clazz); &#125; private RemoteRequestUtil() &#123; &#125;&#125; 当然你也可以是用POJO的方式，但是地城的HttpEntity也是用的MultiValueMap,不信可以自己看看源码，如果用POJO的方式如下: 12345678GithubRequestLogin param = new GithubRequestLogin();param.setClient_id(ConfigConstant.CLIENT_ID);param.setClient_secret(ConfigConstant.CLIENT_SECRET);param.setCode(code);JSONObject jsonObj = RemoteRequestUtil.postT(url, param, JSONObject.class);System.out.println(jsonObj); 到这里，我们就可以获取到access_token了,现在就可以用token获取用户信息了。 获取用户信息的代码： 1234567@GetMapping("user/info")public String getGitHubUserInfo(String token, Model model) &#123; String url = "https://api.github.com/user?access_token=" + token; String result = RemoteRequestUtil.get(url); model.addAttribute("result", result); return "login/success";&#125; 之后就是这样的：这样就可以说是将信息获取完了。其他第三方平台也是大同小异常，以后有机会再补上。当然还有一个问题要留下来，如果token过期了该怎么破了？怎么知道是否过期了？可以思考下。 代码比较简单，需要的话可以看我写的，代码写的有点不太贵方，只做参考，记得替换成你的client_id和client_secret：github授权登录]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>OAuth2</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal 类解析]]></title>
    <url>%2F2017%2F09%2F28%2Fjava-thread-local%2F</url>
    <content type="text"><![CDATA[ThreadLocal 类解析什么是ThreadLocalJDK里面关于ThreadLocal的解释就是：它提供一个线程局部变量，很拗口对不对，没错就是这么拗口，所以你用的少，我们都用的少。它其实就是说，在一个类里面，这个类可以被很多线程访问，那么ThreadeLocal给这些线程，每一个分配一个他们自己私有的局部变量。还是拗口对不对？不着急，现在只要知道，它就是给多线程环境下，每个线程分配一个单独的变量。 ThreadLocal 的组成我们用idea看看ThreadLocal里面含有什么东西： 初一看貌似挺多的。其实在这个里面它主要实现了一个Map，如果你还记得它是为每个线程分配一个独立的变量，那么也就不难理解，它其实就是往map里面给每个线程设置了一个变量给他们使用。然后看看ThreadLocal，主要就是几个方法：get(),set(T),initialValue(),remove()，其中initialValue是protected:123protected T initialValue() &#123; return null;&#125; 它主要是提醒所有的继承者，它需要初始化值。 然后看看get():12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 从这里就可以看到一个事情：get的时候它用到了Thread.currentThread()，然后又用getMap(threade)。为啥会这样？它貌似使用的thread做的键，对么？也就是当前线程。没错，一个map里面它用当前线程做key，然后取到当前线程的value，而不会影响到其它的线程。顺便看下ThreadLocalMap类,看到它是一个static class ThreadLocalMap {},这是不是就是静态变量唯一了？ 其它的set(),remove()就不多说了，肯定也是操作这个map。 ThreadLocal 使用讲了一些简单的源码，貌似也没弄明白到底怎么用它，使用它又有什么效果？我们接下来看下实例，我们使用一个序列号生成器来看下实际效果。首先我们定义一个序列号接口，主要用来获取序列号Sequence.java： 12345678/** * @author chen * @version V1.0 * @date 2017/9/28 */public interface Sequence &#123; int getNumber();&#125; 然后我们写一个生成序列号任务类ClientThread.java： 12345678910111213141516171819/** * @author chen * @version V1.0 * @date 2017/9/28 */public class ClientThread extends Thread &#123; private Sequence sequence; public ClientThread(Sequence sequence) &#123; this.sequence = sequence; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 3; i++) &#123; System.out.println(Thread.currentThread().getName() + " =&gt; " + sequence.getNumber()); &#125; &#125;&#125; 现在在客户端A我们开启三个任务，首先不用ThreadLocal: 1234567891011121314151617181920212223242526/** * @author chen * @version V1.0 * @date 2017/9/28 */public class SequenceA implements Sequence &#123; private static int number = 0; public int getNumber() &#123; number = number + 1; return number; &#125; public static void main(String[] args) &#123; Sequence sequence = new SequenceA(); ClientThread t1 = new ClientThread(sequence); ClientThread t2 = new ClientThread(sequence); ClientThread t3 = new ClientThread(sequence); t1.start(); t2.start(); t3.start(); &#125;&#125; 控制台结果为：可以看到每个线程对于number都是直接加的，这和static变量有关，但是我们如果想要每个线程都有单独的属性number了？只对我当前线程可以用，别人都不可以用了？我们加上ThreadLocal看看： 123456789101112131415161718192021222324252627282930313233/** * @author chen * @version V1.0 * @date 2017/9/28 */public class SequenceB implements Sequence &#123; private static ThreadLocal&lt;Integer&gt; number = new ThreadLocal&lt;Integer&gt;() &#123; @Override protected Integer initialValue() &#123; return 0; &#125; &#125;; public int getNumber() &#123; number.set(number.get() + 1); return number.get(); &#125; public static void main(String[] args) &#123; Sequence sequence = new SequenceB(); ClientThread t1 = new ClientThread(sequence); ClientThread t2 = new ClientThread(sequence); ClientThread t3 = new ClientThread(sequence); t1.start(); t2.start(); t3.start(); &#125;&#125; 在看下结果: 可以看到number的值在每个线程里面都是单独私有的，线程0不会影响线程1的number值。没错，这就是ThreadLocal了。 参考资料：ThreadLocal 那点事ThreadLocal 那点事-黄勇ThreadLocal 那点事(续集)]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（六）RPC]]></title>
    <url>%2F2017%2F09%2F26%2Fmessage-rabbitmq-6%2F</url>
    <content type="text"><![CDATA[RabbitMQ-消息中间件（六）RPCRemote Procedure Call (RPC)第二节讲过可以将耗时的任务通过工作队列给多个工作线程，如果我们想要调用一个在远程其它服务器上的一个功能并且等待它执行完后的结果，我们应该怎么做？是的，这是一个不同的场景，这种模式被叫做Remote Procedure Call或者说是RPC。 在本节中，我们准备使用RabbitMQ来构建一个RPC系统。一个客户端和一个可扩展的RPC服务端。如果我们没有耗时的任务，那么就值得转发，我们准备建造一个仿RPC服务，并且它返回一个斐波那契数。 客户端接口为了演示一个RPC服务是怎样被调用，我们准备创建一个简单的客户端。它暴露出一个叫call的方法接口，通过这个接口发送RPC请求，并且等待返回结果。123FibonacciRpcClien fibonacciRpc = new FibonacciRpcClient();String result = fibonacciRpc.call("4);System.out.println("result:"+result); RPC tips:尽管RPC是一个在计算中非常通用的模式，它还是经常被喷。最明显的问题就是程序员不知道一个被调用的方法到底是本地方法或者是一个很慢的RPC服务。在一个复杂的系统中造成的结果非常让人迷惑，并且非增加调试时候不必要的复杂性。滥用RPC可能会导致代码异常复杂混乱。如果一定要使用的话，参考下面的建议：1 明确知道被调用的方法是本地的还是远程的2 一定要为系统编写文档，确保确保组件之间的依赖非常清晰3 处理异常情况，当RPC 服务长时间宕机了，客户端应该怎样操作？如果可以的话，尽量避免使用RPC。你应该使用异步的方式来替换像RPC这种同步的。在下一个阶段异步推回结果 回调队列总的来说在RabbitMQ上做RPC是非常简单的。一个客户端发送一个请求的消息，然后服务端返回响应的消息。为了接收到响应，我们需要在发送请求消息的时候附带一个回调队列地址。我们可以使用默认的队列： 12345callbackQueueName=channel.queueDeclared().getQueue();BasicProperties props = new BasicProperties.Builder().replyTo(callbackQueueName);channel.basicPublish("","rpc_queue",props,message.getBytes()); Message properties . AMQP 0-9-1 协议预先定义了14个关于消息的属性。大多数属性使用的很少，一些特别的比如：deliveryMode: 标记一个消息作为persistent(持久化默认值为2),或者transient(非2的值)。contentType： 用来描叙mime-type 编码格式。例如经常使用的JSON编码：application/json。replyTo: 通常用来命名一个回调队列。correlationId: 用来关联RPC响应和请求。 我们使用的时候需要导入：import com.rabbitmq.client.AMQP.BasiProperties;. Correlation Id （关联 ID）在上面提出的方法中，我们建议未每一个RPC请求创建一个回调队列。它是非常低效的。但是幸运的是我们有一个更好的方式，我们可以为每一个客户端创建一个回调队列。 尽管我们用为客户端的形式替换了每一个rpc请求，但是它还是带来了新的问题，队列收到响应后无法知道这个响应应该属于哪个请求。现在我们就可以使用correlationId属性了。我们为每一个请求设置一个不同的correlationId。然后，当我们在回调队列里面收到请求的时候我们会关注这个属性，在这个属性值得基础上 我们可以匹配到哪个响应属于哪个请求。如果我们收到一个correlationId不存在值，我们可以安全的丢掉这个消息，因为这个响应不属于我们的请求。你可能会问，为什么我们应该忽略在回调队列里面的不明correlationId的消息，而不是用一个error来报错？这是因为这种情况在服务端是存在的，比如RPC服务可能在发送给我们结果的时候就刚好挂掉了，但是还没有给请求发送一个确认的消息，尽管这种情况很少，但是还是存在；如果这种情况发生了，重启的RPC服务会再次处理这个请求，这就是为什么在客户端我们应该对重复响应进行友好的处理，理论上，RPC应该是幂等的。 工作流程 我们的RPC工作的方式就像上图这样： 当一个客户端启动，它创建了一个匿名的独立的回调队列。 对于一个RPC请求，客户端发送一个消息，消息有两个属性：replyTo,设置回调队列；correlationId,为每一个请求设置一个唯一值。 请求发送到rpc_queue队列。 RPC工作线程(服务端)等待队列中的请求的消息。当请求出现，它就开始工作，工作完后，使用replyTo中的属性来返回一个结果消息给客户端。 客户端等待回调队列中的消息数据，当一个消息出现，它会先检查correlationId属性，检查响应中的值匹配上了请求中的值。 实践斐波那契算法： 12345private static int fib(int n)&#123; if(n == 0) return 0; if(n == 1) return 1; return fib(n-1)+fib(n-2);&#125; 非常简单并且没有太多边界判断，只支持正整数。我们只是用来演示而已，如果要其他的牛逼的算法，自己可以试着写。 RPC 服务端代码：RPCServer.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package me.chenzhijun.rpc;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/26 */public class RpcServer &#123; private static final String RPC_QUEUE_NAME = "rpc_queue"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); final Channel channel = connection.createChannel(); channel.basicQos(1); System.out.println("[x] awaiting rpc request"); Consumer consumer = new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; AMQP.BasicProperties replyProperties = new AMQP.BasicProperties.Builder().correlationId(properties.getCorrelationId()).build(); String response = ""; String message = new String(body, "UTF-8"); int n = Integer.parseInt(message); System.out.println("[x] fib(" + message + ")"); response += fib(n); channel.basicPublish("", properties.getReplyTo(), replyProperties, response.getBytes("UTF-8")); channel.basicAck(envelope.getDeliveryTag(), false); synchronized (this) &#123; this.notify(); &#125; &#125; &#125;; channel.basicConsume(RPC_QUEUE_NAME,false,consumer);// while (true)&#123; synchronized (consumer)&#123; try &#123; consumer.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;// &#125; &#125; public static int fib(int n) &#123; if (n == 1) &#123; return 1; &#125; if (n == 0) &#123; return 0; &#125; return fib(n - 1) + fib(n - 2); &#125;&#125; 和前面的章节一样，我们建立了connection,channel,queue。我们可能想要运行一个或者多个线程，为了负载我们需要设置channel.basicQos()中的prefetchCount设置。我们可以使用basicConsumer来访问我们设置的回调queue，我们提供了DefaultConsumer来做一些工作并且发送回来response。 RPC客户端代码：RpcClient.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package me.chenzhijun.rpc;import com.rabbitmq.client.*;import java.io.IOException;import java.util.UUID;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/26 */public class RpcClient &#123; private Connection connection; private Channel channel; private String requestQueueName = "rpc_queue"; private String replyQueueName; public RpcClient() throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setHost("localhost"); connection = factory.newConnection(); Channel channel = connection.createChannel(); replyQueueName = channel.queueDeclare().getQueue(); &#125; public String call(String message) throws Exception &#123; final String corraltionId = UUID.randomUUID().toString(); AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder().correlationId(corraltionId).replyTo(replyQueueName).build(); channel.basicPublish("", requestQueueName, properties, message.getBytes("UTF-8")); final BlockingQueue&lt;String&gt; response = new ArrayBlockingQueue&lt;String&gt;(1); channel.basicConsume(replyQueueName, true, new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; if (properties.getCorrelationId().equals(corraltionId)) &#123; response.offer(new String(body, "UTF-8")); &#125; &#125; &#125;); return response.take(); &#125; public void close() throws Exception &#123; connection.close(); &#125;&#125; 客户端代码理解也不难： 先创建connection，channel，然后声明一个回调的队列来为了(replies)。 我们订阅了回调队列，所以我们可以接收到RPC的response。 我们的call方法是实际上的RPC请求。 我们先生成一个唯一的correlationId，然偶后保存它，它的作用是匹配正确的响应response。 接下来，我们发布了请求request的消息，消息带有replyTo和correlationId. 接下来我们我们就等待何时的响应返回。因为我们的消费者转发处理在一个分开的线程，在响应到达前，我们需要准备一些东西来挂起我们的主线程main。使用BlockingQueue就是一种解决办法，我们在此列中创建了一个ArrayBlockingQueue设置了capacity为1,因为我们仅仅需要它等待一个响应。 handleDelivery方法只做了一个非常简单的工作，对于每一个消费者的响应信息，它会检查是否correlationId是否是我们需要的，如果是的话，它会将它放进BlockingQueue. 同时main线程是一直在等待从BlockingQueue中拿到响应。 最后我们返回响应结果给用户。 参考资料： rabbitMQ RPC]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（五）Topics]]></title>
    <url>%2F2017%2F09%2F25%2Fmessage-rabbitmq-5%2F</url>
    <content type="text"><![CDATA[RabbitMQ-消息中间件（五）TopicsTopics 前言上一章我们写了个路由日志系统，用direct的放交换机，比较了fanout和它之间的区别，一个是广播，一个是绑定。尽管我们使用direct交换机做了改进，但是它还是有一些限制，它没法根据不同的几个规则了区分。我们之前的日志是分了四种:info、debug、warn、error。但是我们可能实际中了，对error又分为系统错误或者业务错误。有时候我们针对业务开发，就不想去管理系统错误，那怎么办？为了再一次提高我们的日志系统，我们需要学习另一种交换机：topic交换机。 Topic exchange发送给top交换机的消息不能随意定义routing_key,它必须是一串单词，并且用点.分开。这些单词可以任意定义，通常他们是描叙这些消息的共同特点。routing key示范例如：stock.usd.nyse,nyse.vmw,quick.orange.rabbit。你可以定义你喜欢的任何单词，不过记得总共不能超过255 bytes。 routing_key定义好了，那么binding_key也必须和它保持同样的格式。topic得原理其实和direct是类似的：一个拥有特殊routingKey的消息，经过交换机转发给一个匹配它的bindingKey的队列。然而对于topic来说，有两个关于bingdingKey的重要场景： * (星号) 用来表示一个精确的单词； 用来表示0个或者多个单词； 可以看下图加深印象： 在这个例子中，我们准备发送所有描叙动物的消息，这些消息的routingKey由三个单词组成(其中两个.)。第一个词在routeKey里面描叙速度，第二个表示颜色，第三个表示物种：&lt;speed&gt;.&lt;colour&gt;.&lt;species&gt;。 我们先创建三个bingdingKey,队列1用bindingKey：*.orange.*;队列2用bindingKey：*.*.rabbit和lazy.#。 这些队列可以被总结为： Q1 只对橙色动物感兴趣； Q2 对所有兔子和所有懒的动物感兴趣； 一个消息如果带有quick.orange.rabbit,会被转发给两个队列，如果是lazy.orange.elephant也会转发给两个队列，如果是quick.orange.fox会只转发给第一个队列Q1，如果是lazy.brown.fox会只转发给第二个队列Q2.lazy.pink.rabbit匹配了Q2的两个bingdingKey，但是它只会接收一次消息。quick.brown.fox不会匹配任何绑定，所以它会被丢弃。 如果我们打破下规则，用一个单词或者四个词的routKey，比如orange或者quick.orange.male.rabbit?消息会匹配不上任何的bingdingKey,然后会被丢失掉。 在另一方面，如果用的是lazy.orange.male.rabbit,就算它是四个单词，他也会匹配到bingdingKey为lazy.#,然后消息转发给Q2队列。 Topic exchange是非常强大的交换机，并且可以像其他交换机一样工作。当一个队列只用#来作为bingdingKey，它会接受所有的消息，就相当于一个fanout交换机。当*和#没有在bingdingKey中使用，那么就相当于一个direct交换机。 代码实战生产者代码： 12345678910111213141516171819202122232425262728293031323334package me.chenzhijun.topic;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/25 */public class EmitLogTopic &#123; private static final String EXCHANGE_NAME = "log_topic"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost");//可以自定义 Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, "topic"); String routeKey = "errorMessage.server";//system.error; errorMessage.server String message = "key is " + routeKey + ",this is message"; channel.basicPublish(EXCHANGE_NAME, routeKey, null, message.getBytes()); System.out.println("message is send:" + message); channel.close(); connection.close(); &#125;&#125; 消费者代码：123456789101112131415161718192021222324252627282930313233343536373839404142package me.chenzhijun.topic;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/25 */public class EmitLogReceiver &#123; private static final String EXCHANGE_NAME = "log_topic"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, "topic"); String queueName = channel.queueDeclare().getQueue(); String[] messageType = new String[]&#123;"#", "system.*", "*.server"&#125;; for (String serverType : messageType) &#123; channel.queueBind(queueName, EXCHANGE_NAME, serverType); &#125; Consumer consumer = new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; super.handleDelivery(consumerTag, envelope, properties, body); System.out.println("[X] received,routeKey:" + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); &#125; &#125;; channel.basicConsume(queueName, true, consumer); &#125;&#125; 控制台输出结果： 在管理后台查看channel，可以看到生成的三个队列图:]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（四）路由]]></title>
    <url>%2F2017%2F09%2F25%2Fmessage-rabbitmq-4%2F</url>
    <content type="text"><![CDATA[RabbitMQ-消息中间件（四）路由路由上面一节我们用交换机fanout的方式进行了广播，这种方式如果在不需要对消息进行区分是没关系的，可以使用。但是有些场景下，比如日志打印，对于一些error的日志，我可能需要的是要保存到磁盘中进行持久化，而对于一些warn，info的日志，我可能只是想着输出下就可以了。那么这种情况下我们还用fanout的方式可能就不行了，我可能想要的是某一个队列接受特定的某一类消息做特别处理。比如我想要一个error队列专门来监听error的消息做打印，其它的队列就不管error的消息了。 绑定(Bingdings)在前面，其实我们已经创建过“绑定”了。比如我们使用到了：channel.queueBind(queueName, EXCHANGE_NAME, &quot;&quot;);,“绑定”：中文意思就是你和我之间有些关联，我们之间有关系；那么MQ也是，它的关联是队列和交换机，它绑定的是队列和交换机的关系，也就是说，这个队列对这个交换机感兴趣，会接受这个交换机的信息。但是接受什么消息了？这个可以在绑定的时候用另外一个参数来设置，也就是路由：俗成绑定的key。channel.queueBind(queueName, EXCHANGE_NAME, &quot;binding key,set by yourself&quot;);。这里我们可以看到，绑定key是否生效其实是跟交换机类型相关的，如果交换机是fanout，那就跟key没关系了，因为它是广播，只要有队列绑定到我交换机，产生了关系，我就懒的管你要不要，直接给你。这个有点类似中国爸妈，你是我孩子，只要我想给，你不要也得要，哈哈。 Direct 交换机前面章节，我们采用的fanout交换机，它是给所有队列发送消息。很明显在我们的日志系统需求里面不太适合，我们希望对消息进行一次过滤，过滤之后再发送给不同的队列做处理，比如有的队列接受消息去打印，有的去写磁盘等等。所以请注意了如果你不想管事，那就用fanout，把消息给你，你自己去做处理，直观无脑给消息就行，其他的就不用关心了。。用不了fanout，那我们用什么了？rabbitMQ有四种交换机，我们可以选择direct。direct交换机背后的算法规则很简单，队列的bingding key和消息的routing key那个能精确匹配就给队列发消息。 图中非常简单，两个队列同时绑定了direct交换机，Q1的范围是根据bingding key为orange的值得消息。它表明，如果消息中的routing key 为orange，那么消息就发送给Q1。Q2绑定了两个：black、green，如果消息中有routing key为black和green的都发送个Q2。那么如果消息没有routing key,或者队列没有bingding key，那么这种消息，交换机会直接丢弃。 多绑定 多个队列绑定同一个bingding key,在rabbitMQ中是合法的。如上图中，如果是这种形式，那么direct交换机的作用就是类似fanout，不过只有当消息的routing key和bingding key一直，上图中是消息如果路由key是black，那么交换机就会将消息分发给Q1和Q2两个队列。 实践: 发送日志(生产者)现在回到我们之前的问题，如果我们想要不同的队列，比如Q1将日志写入磁盘，Q2打印warn日志，Q3接收info日志。我们可以使用drect交换机，然后给不同的队列绑定bingding key。所以对于我们的生产者端： 声明交换机还是跟之前的类似，我们必须先声明一个交换机： 1channel.exchangeDeclare(EXCHANGE_NAME,"direct"); 传递消息： 1channel.basicPublish(EXCHANGE_NAME, severity, null, message.getBytes()); 订阅(Subscribing)消费者对消息的接收和之前的有一个不同，我们用bingding key来绑定消费者只对感兴趣的消息进行接收： 1234String queueName = channel.queueDeclare().getQueue();for(String serverType : msgType)&#123; //serverType,每一个queue绑定一个消息类型 channel.queueBind(queueName,EXCHANGE_NAME,serverType);&#125; 代码生产者，在发送消息的时候特别注意routing key,在此例子中也就是我们for循环里面的key： 123456789101112131415161718192021222324252627282930313233343536373839404142package me.chenzhijun.route;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/25 */public class EmitLogDirect &#123; private static final String EXCHANGE_NAME = "log_exchange"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, "direct"); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("warn", "this is message of warn ....."); map.put("debug", "this is message of debug ....."); map.put("info", "this is message of info ....."); map.put("error", "this is message of error ....."); for (String key : map.keySet()) &#123; channel.basicPublish(EXCHANGE_NAME, key, null, map.get(key).getBytes()); System.out.println(EXCHANGE_NAME + ",serverType:" + key + ",message:" + map.get(key)); &#125; channel.close(); connection.close(); &#125;&#125; 消费者,这里我们使用的是两种方式，第一种是全部绑定到一个队列，另外注释的是开启4个队列，对不动的消息做不同的处理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package me.chenzhijun.route;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/25 */public class ReceiveLogDirect &#123; private static final String EXCHANGE_NAME = "log_exchange"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME,"direct");// String[] messageType = new String[]&#123;"warn", "info", "debug", "error"&#125;;// for (String serverType : messageType) &#123;// String queueName = channel.queueDeclare().getQueue();// channel.queueBind(queueName, EXCHANGE_NAME, serverType);// Consumer consumer = createConsumer(serverType, channel);// boolean isAck = true;// channel.basicConsume(queueName, isAck, consumer);// &#125; String queueName = channel.queueDeclare().getQueue(); String[] messageType = new String[]&#123;"warn", "info", "debug", "error"&#125;; for (String serverType : messageType) &#123; channel.queueBind(queueName, EXCHANGE_NAME, serverType); &#125; Consumer consumer = new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.out.println("[x] received,routeKey: " + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); &#125; &#125;; boolean isAck = true; channel.basicConsume(queueName, isAck, consumer); &#125; private static Consumer createConsumer(String serverType, Channel channel) &#123; if (serverType.equals("warn")) &#123; return new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.out.println("[x] received,routeKey: " + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); System.out.println("warn消息我们只是打印.....:" + new String(body, "UTF-8")); &#125; &#125;; &#125; else if (serverType.equals("info")) &#123; return new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.out.println("[x] received,routeKey: " + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); System.out.println("info消息我们可以忽略.....:" + new String(body, "UTF-8")); &#125; &#125;; &#125; else if (serverType.equals("debug")) &#123; return new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.out.println("[x] received,routeKey: " + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); System.out.println("debug消息我们只做调试.....:" + new String(body, "UTF-8")); &#125; &#125;; &#125; else if (serverType.equals("error")) &#123; return new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; System.out.println("[x] received,routeKey: " + envelope.getRoutingKey() + ",message:" + new String(body, "UTF-8")); System.out.println("error消息很重要，存磁盘.....:" + new String(body, "UTF-8")); &#125; &#125;; &#125; System.out.println("其它消息不管了。。。"); return null; &#125;&#125; 参考文档： RabbitMQ Routing]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-单例模式]]></title>
    <url>%2F2017%2F09%2F23%2Fjava-design-pattern-single%2F</url>
    <content type="text"><![CDATA[设计模式-单例模式简述单例应该是最简单也是最常用的模式了。单例模式通常用在当一个系统只需要唯一一个类，比如线程池，日志打印等。单例模式的类就只有一个，听起来很简单，但是要写好单例却不容易。不信你自己先想想怎么保证系统里面只有一个单例？ 单例模式：确保一个类只有一个实例，并且提供一个全局访问点 单例实践要保证单例的第一步就是要保证类不能随意实例话，new 一个对象的代价是很简单的比如：new Object()。但如果不能new了？没错，我们第一步就是让构造方法私有话，不会对外开放private ObjectConstractor();但是如果我们还是需要使用这个类怎么办了？一个最简单的单例模式就可以写出来了： 12345678910111213public class Singleton &#123; private static Singleton instance;//使用static来确保唯一。但是static真的管用么？ public static Singleton getInstance()&#123; if(instance==null)&#123;//先判断是不是存在，不存在就实例一个。 instance = new Singleton(); &#125; return instance; &#125; private Singleton()&#123; &#125;&#125; 其实你可以想到，如果在多线程的环境下，我们上面的代码还是会有问题的，就是线程A和线程B同时同刻读取instance，然后两者都是读到null，然后就AB都会去实例话一次instance，这跟我们预想的不一致。 嗯我们可以做点改进，那就是同步： 123456public static synchronized Singleton getInstance2()&#123; if(instance==null)&#123; instance = new Singleton(); &#125; return instance;&#125; 没错在上列中我们是可以解决掉多线程的情况下，instance实例话两次的情况的，这种方式我们也叫做懒汉式单例模式。但是我们引入了synchronized,这又会造成一个新的问题，那就是我们，我们知道这种同步是会降低性能的，当然如果你的机器配置够好，那么对此就可以完全忽略，但是作为程序员，总有一种想要压榨性能的天生渴望～ 让我们再来做些改进：123456789101112public class Singleton &#123; private static Singleton singleton = new Singleton(); private Singleton()&#123; &#125; public static Singleton getInstance()&#123; return singleton; &#125;&#125; 这种改进的方式是在jvm加载该类的时候就已经初始话了，JVM保证任何线程访问这个类之前一定先创建了该类，这种方式也被形象的称为饿汉式单例模式。 单例模式实现方法懒汉式单例模式：123456789101112131415package single;public class Singleton &#123; private static Singleton instance; public static synchronized Singleton getInstance()&#123; if(instance==null)&#123; instance = new Singleton(); &#125; return instance; &#125; private Singleton()&#123; &#125;&#125; 饿汉式单例模式：1234567891011public class Singleton &#123; private static Singleton singleton = new Singleton(); private Singleton()&#123; &#125; public static Singleton getInstance()&#123; return singleton; &#125;&#125; 双重加锁校验：123456789101112131415161718package single.doubblechecking;public class Singleton &#123; private volatile static Singleton singleton; private Singleton()&#123;&#125; public static Singleton getSingleton()&#123; if(singleton == null)&#123; synchronized (Singleton.class)&#123; if(singleton == null)&#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 静态内部类：123456789101112public class Singleton &#123; private static class SingletonInstance &#123; private static final Singleton sinleton = new Singleton(); &#125; private Singleton() &#123; &#125; public static Singleton getSingleton() &#123; return SingletonInstance.sinleton; &#125;&#125; 枚举单例模式:12345678910111213141516171819package single.enums;public enum Singleton &#123; INSTANCE; private Object object; private Singleton()&#123; // init something.. object = new Object(); &#125; public void doSomething()&#123; //.... &#125; public Object getObject() &#123; return object; &#125;&#125; 枚举的方式特别有意思，所以我额外还写了一个测试类： 123456789101112131415package single;import single.enums.Singleton;public class Demo &#123; public static void main(String[] args) &#123; Singleton instance = Singleton.INSTANCE; instance.getObject(); System.out.println(instance.getObject()); System.out.println(instance.getObject()); System.out.println(instance.getObject()); System.out.println(instance.getObject()); &#125;&#125; 这四次输出都是一样的： 1234java.lang.Object@7f31245ajava.lang.Object@7f31245ajava.lang.Object@7f31245ajava.lang.Object@7f31245a 用枚举实现单例是&lt;&gt;里面提倡的，但是实际中我看很少有人用，我们公司也有人直接用的是懒汉式方法上加synchronized，历史的原因就不追述了，但是自己写的代码一定要好好的检查。 什么是好的代码？可能就是编写以后不需要再重构的代码。因为它的性能已经是最完美了，顶多也就是加一些判断逻辑而已。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（三）订阅与发布]]></title>
    <url>%2F2017%2F09%2F22%2Fmessage-rabbitmq-3%2F</url>
    <content type="text"><![CDATA[订阅与发布之前的实例我们都是将任务精确的转发给某一个工作线程（消费者）。这节我们讲讲怎么将一个任务转发给多个消费者。这种模式就是闻名的“订阅/发布”模式。 我们的初步想法便是一个发布者发布了一个消息，多个消费者接收到消息，做不同的事情。 交换机(Exchanges)前面已经说了了我们是通过队列来接受和发送消息的(p-q-c)[p:发布者，q:队列，r:消费者]。消息是由p 发布到q,q 再发给c。其实Rabbit又一个消息模型，在我们完成实例前，我们先来看看Rabbit的所有消息模型。 在Rabbit消息模型中最核心的点是生产者从不直接发送消息给队列。实际上，生产者都不知道生产的消息给了那个队列。 相反的，生产者只是给exchange(交换机)发送消息。交换机实际上是一个非常简单的东西，一边是从生产者接受消息，另一边就是将消息推送给队列。交换机必须明确的知道他们接到消息后该怎么处理消息：应该是将消息发送给一个明确的队列？还是应该给所有的队列？又或者是直接忽略不管？这些规则都是由交换机的类型去定义的。 有一些可用的交换机类型是：direc,topic,headers,fanout。今天我们讲的类型主要是fanout。我们可以先声明一个类型：1channel.exchangeDeclare("exchange_name","fanout") fanout交换机非常简单，就像它的名字一样（扇形交换机,广播），它收到消息后就广播给所有它知道的队列。 listing exchanges 想知道服务器可以支持哪些交换机类型可以使用：rabbitmqctl list_exchanges 之前的例子中我们使用的是：channel.basicPublic(&quot;&quot;,&quot;hello&quot;,null,message.getBytes());,这种方式是没有声明exchange_name的。默认的话是使用一个无名交换机，消息经过特定的routingKey转发到队列。现在我们可以用我们自定的exchangeName来代替： 1channel.basicPublish("exchangeName","",null,message.getBytes()); 临时队列之前我们都是声明了特定的队列名称的,因为之前的消息模型中，我们需要将生产者和消费者指定到同一个队列，所以声明队列的名称对我们很重要。 但是在fanout本节中，我们需要的是的监听所有的消息，我们关注的是最新的消息，而不是旧数据。要解决这个，我们需要先做两件事： 不管是什么时候我们连接到Rabbit 我们都需要一个空的，新队列。要做到这点，我们可以用一个随机名字来创建一个新队列，当然更好的是，让rabbitMQ帮我们选择一个随机名字。 一旦我们的消费者消费完断开了和队列的连接，队列应该自己删除掉。 在Java中，当我们提供一个无参的方法：queueDeclare(),我们创建了一个不可持久化，独立的，可自动删除的带有随机生成名字的队列。 1String queueName = channel.queueDeclare().getQuere(); queueName就是我们生成的随机队列名，它可能像：amq.gen-JzTY20BRgKO-HjmUJj0wLg. 绑定我们已经知道如何创建一个fanout类型的交换机和队列，现在我们需要告诉交换机发送消息给我们的队列。交换机和队列之间的这种关系就叫做绑定: 列出所有已存在的绑定 rabbitmqctl list_bindings 全模型图为： 实例编程生产者程序和之前的代码产不多，最大的不同点就是我们现在想要发布消息给我们自己定义的nameame的exchange,而不再是之前无名的exchanage。现在当我们发送消息的时候，我们就必须提供一个routingKey,不过由于是fanout类型的exchange,它具有广播给所有的队列的作用。(routingKey的主要作用是在exchange和queue中做选择)。下面是生产者代码: 1234567891011121314151617181920212223242526272829303132333435363738package me.chenzhijun;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.Random;import java.util.concurrent.TimeoutException;/** * sudo rabbitmqctl list_exchanges * * @author chen * @version V1.0 * @date 2017/9/22 */public class PublishProducer &#123; private static final String EXCHANGE_NAME = "logs"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, "fanout"); String message = new Random().nextInt(100) + ""; System.out.println("[sent] : " + message); channel.basicPublish(EXCHANGE_NAME, "", null, message.getBytes()); channel.close(); connection.close(); &#125;&#125; 消费者代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package me.chenzhijun;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/22 */public class SubcribeReceiver &#123; private static final String EXCHANGE_NAME = "logs"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, "fanout"); String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, EXCHANGE_NAME, ""); Consumer consumer = new DefaultConsumer(channel) &#123; /** * No-op implementation of &#123;@link Consumer#handleDelivery&#125;. * * @param consumerTag * @param envelope * @param properties * @param body */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; super.handleDelivery(consumerTag, envelope, properties, body); String messsage = new String(body, "UTF-8"); System.out.println("[x] Received : " + messsage+"...ooooooo"); &#125; &#125;; channel.basicConsume(queueName, true, consumer); &#125;&#125; 特别注意，此例中的消费中要先启动。如果exchange没有绑定queue，那么交换机接收到消息会直接抛弃它。该例中，生产者只是声明了交换机，而不会创建队列。这种模型也跟我们之前说的也是一样的，生产者并不知道消息给了那个队列。 如果要多个消费者接收到不同的消息做不同的事情，那么就让消费者绑定到同一个exchange_name,然后监听就可以了。 参考文档RabbitMQ part3]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java IO流]]></title>
    <url>%2F2017%2F09%2F20%2Fjava-io%2F</url>
    <content type="text"><![CDATA[Java IO流Java IO流简述JavaIO 中流都分别在java.io包中，主要分为几类： 字节流：Byte Stream-&gt;InputStream/OutputStream 字符流: Character Stream-&gt;Reader/Writer 缓冲流：Buffered Stream 数据流：Data Stream 只针对基本数据类型：(boolean, char, byte, short, int, long, float, double) 还有String 对象流: Object Stream,继承自Serializable,必须序列化 控制台的流.. InputStream字节流其实又叫做二进制流，因为它是最基本的二进制的数据的读取，也就是010101的读取与传输。如果读取的数据全是文字类型，那么推荐使用字符流的，如果是图片或者其它的图片可以使用字节流。看下InputStream 类里面的相关方法： InputStream实现了Closeble,Closeble继承了AutoCloseable，其实就是相当与一个标记，AutoCloseable是1.7才出来的，主要是为了t-w-r语法主动关闭流。 InputStream是abstract的，一定要注意它是一个抽象类。我们说的装饰者模式里面，InputStream的实际实现(实现了InputStream的相关方法)基本都是类似于组件，而InputStream相当与一个超类。里面的主要构成是8个方法一个属性。 available()123public int available() throws IOException &#123; return 0;&#125; 先看我们的第一个方法available(),它的作用是返回流中估计的长度，而这个长度怎么设置？看源码中的介绍最后归结两句话： 121. 永远也不要调用此方法来确认分配多大的缓存空间，因为它永远返回0。2. 任何继承`InputStream`的子类必须重写该方法。 意思就是子类自己去实现这个方法。 mark()1public synchronized void mark(int readlimit) &#123;&#125; mark()方法在原文中有个：Marking a closed stream should not have any effect on the stream. 这里的方法是个空方法，子类需要自己实现，当然这里没有强制要求。只是mark的作用是在reset的时候开始的。当调用mark的时候会从readlimit的位置开始将之后读到的内容放到缓存块中。这样reset之后再读取就能取到值了。 markSupported()markSupported()方法是指是否支持mark和reset方法，只有两者都支持得时候才能返回true,默认为false;可以查看FileInputStream和DataInputStream,这两个子类有分别不同的方式。 read()1public abstract int read() throws IOException; 读取流中的下一个字节数，返回一个0-255的int值。如果读到流的最后一个字节，返回-1。该方法子类必须自己重写。 read(byte[])123public int read(byte b[]) throws IOException &#123; return read(b, 0, b.length); &#125; 将流的内容读取到b字节数组（缓冲数组）中。如果b的长度为0，那么不会有任何的字节会读取到数组中，并且返回0。如果不是的话，它会将缓存小于等于b的数组长度，一次一次来读取内容。读取到最后返回-1。 read()1234567891011121314151617181920212223242526272829public int read(byte b[], int off, int len) throws IOException &#123; if (b == null) &#123; throw new NullPointerException(); &#125; else if (off &lt; 0 || len &lt; 0 || len &gt; b.length - off) &#123; throw new IndexOutOfBoundsException(); &#125; else if (len == 0) &#123; return 0; &#125; int c = read(); if (c == -1) &#123; return -1; &#125; b[off] = (byte)c; int i = 1; try &#123; for (; i &lt; len ; i++) &#123; c = read(); if (c == -1) &#123; break; &#125; b[off + i] = (byte)c; &#125; &#125; catch (IOException ee) &#123; &#125; return i;&#125; 其实这个方法可以看到是调用的read()方法来实现的。目的就是将读到的byte字节一个一个缓存到字节数组中。 reset()123public synchronized void reset() throws IOException &#123; throw new IOException("mark/reset not supported");&#125; 通俗的讲如果你在书的某一个位置放了一个书签，那么在你想返回到书签的位置的时候。这个动作就叫做reset().默认是抛出异常的，除非子类自己实现它。调用reset的时候，markSupport也必须返回true。并且mark的位置不能是无效的。比如你放书签不能超过书的范围吧。 skip(long)123456789101112131415161718192021public long skip(long n) throws IOException &#123; long remaining = n; int nr; if (n &lt;= 0) &#123; return 0; &#125; int size = (int)Math.min(MAX_SKIP_BUFFER_SIZE, remaining); byte[] skipBuffer = new byte[size]; while (remaining &gt; 0) &#123; nr = read(skipBuffer, 0, (int)Math.min(size, remaining)); if (nr &lt; 0) &#123; break; &#125; remaining -= nr; &#125; return n - remaining;&#125; 直接看代码实现。里面使用的read(byte,0,length);这不就是读取字符么。跳过多少个字符，就是先读多少个字符，读完之后之后读的字符再返回给你，返回实际跳过的字符。 MAX_SKIP_BUFFER_SIZEprivate static final int MAX_SKIP_BUFFER_SIZE = 2048; 这个属性的默认值为2048，好像也就只在skip里面用到了，就是定义一个最大的缓存大小。 其它流所有的字节流都是继承InputStream。 非缓存流获取数据是直接跟OS打交道。缓存流将数据读到内存空间然后内存空间空了才去调用底层OS方法。自动刷缓存可以设置autoflush,通过构造方法设置。 建立一个缓存流new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dataFile))) Data stream : DataInputStream/DataOutputStream 不要使用浮点数来表示精确的数字，如果需要精确的数字用BigDecimal Object Streams : ObjectInputStream/ObjectOutputStream 文件流（NIO）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile 编写]]></title>
    <url>%2F2017%2F09%2F19%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[Dockerfile 文件导读Dockerfile 是一个包含了很多指令的文本文件，基于这些指令我们可以使用build创建一个image。 docker build创建image是从dockerfile和context。构建中的上下文(context)是PATH和URL的文件集合，PATH是指本地的文件系统，URL是一个Git仓库地址。一个上下文通常是递归的，对于PATH包含了子目录，URL包含了它的子模块。docker build . build是有Docker 守护进程执行的，执行的第一件事就是将完整的上下文路径（包括递归的子目录）都会发送给守护进程，在大多场景下，最好的方式是用一个空目录作为上下文路径，然后里面放一个Dockerfile.然后在构建(build)将需要的文件加入到目录里面来。 千万注意不要使用根路径”/“作为PATH，它会导致将整个硬盘的文件都会发送给Docker的守护进程 如果要在构建上下文中使用一个文件，Dockerfile 引用了一些特殊的指令，比如COPY指令，为了优化构建，不需要的文件和目录可以加入到上下文目录的.dockerignore文件中。dockerignore file 一般地，Dockerfile被称作Dockerfile，并且放在上下文目录的根目录。你也可以在构建的时候使用 -f 标记来指定一个你文件系统任何地方中的Dockerfile。docker build -f /path/to/a/Dockerfile . 你也可以在构建成功后保存image时指定远端源仓库(repositories)和标签。 docker build -t shykes/myapp . 如果要在构建之后给多个仓库(repositories) 打tag，使用多个-t参数就可以了：docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest . 在Docker守护进程执行Dockerfile中的指令之前，它都会进行一下预编译来校验语法是否准确，如果有错误的话会返回error。123$ docker build -t test/myapp .Sending build context to Docker daemon 2.048 kBError response from daemon: Unknown instruction: RUNCMD Docker守护进程在最终生成image输出imageID前都会一条一条执行Dockerfile中的指令并且确认每条指令的结果。最后守护进程会自动清理你发送的上下文。 注意到每一条指定都是独立执行然后创建一个image。因此 RUN cd /tmp 不会对下一个指令有任何影响。只要允许的话，Docker会重复使用生成的image中的缓存，来加速docker build 执行成功效率。下面的输出中可以明显的看到使用了缓存：1234567891011121314$ docker build -t svendowideit/ambassador .Sending build context to Docker daemon 15.36 kBStep 1/4 : FROM alpine:3.2 ---&gt; 31f630c65071Step 2/4 : MAINTAINER SvenDowideit@home.org.au ---&gt; Using cache ---&gt; 2a1c91448f5fStep 3/4 : RUN apk update &amp;&amp; apk add socat &amp;&amp; rm -r /var/cache/ ---&gt; Using cache ---&gt; 21ed6e7fbb73Step 4/4 : CMD env | grep _TCP= | (sed &apos;s/.*_PORT_\([0-9]*\)_TCP=tcp:\/\/\(.*\):\(.*\)/socat -t 100000000 TCP4-LISTEN:\1,fork,reuseaddr TCP4:\2:\3 \&amp;/&apos; &amp;&amp; echo wait) | sh ---&gt; Using cache ---&gt; 7ea8aef582ccSuccessfully built 7ea8aef582cc 镜像构建完就可以尝试将它推送到远程repo; 格式1234#commentINSTRUCTION argumets指令 命令 参数 指令(INSTRUCTION)是大小写无关的，不过大家约定习俗般将它总是大写来区分指令和命令 Docker 执行Instruction是有顺序的。一个Dockerfile文件必须用FROM指令开始。FROM指令制定了构建的基础镜像。FROM只能在在最前面。 # 是docker的注释符。 指令解析环境变量替换环境变量由ENV语句声明也可以用在Dockerfile中；在dockerfile中使用ENV定义的变量可以使用$variable_name 和 ${variable_name},当然也有转义字符：’\’,如果\$variablename ,指代的就是$variable_name; 12345FROM busyboxENV foo /barWORKDIR $&#123;foo&#125; # WORKDIR /barADD . $&#123;foo&#125; # ADD . /barCOPY \$foo /quux # COPY $foo /quux 环境变量在Dockerfile中可以被以下的执行指令支持：ADD, COPY, ENV, EXPOSE, FROM, LABEL, STOPSIGNAL, USER, VOLUME, WORKDIR, ONBUILD,1.4版本之前的ONBUILD不支持。 在一整条的指令中，环境变量是同一个值： 123ENV abc=helloENV abc=bye def=$abcENV ghi=$abc 上文中def会返回hello，而不是bye，ghi会返回bye，因为在第二条指令中，set abc=bye 和def=$abc 并不是同一条指令。 .dockerignore 文件指令FROM FROM [AS ] FROM [:] [AS ] FROM [@] [AS ] FROM指令初始化了一个构建的舞台，并且为接下的操作设置了基础image。一个有效的Dockerfile必须由FROM开始。 默认FROM会有一个latest tag。 RUN RUN # shell 形式执行，默认shell执行器，如/bin/sh -c RUN [“executable”,”param1”,”param2”] # exec 形式执行 如果使用shell方式可以使用反斜杠来连接语句：12RUN /bin/bash -c &apos;source $HOME/.bashrc; \echo $HOME&apos; 上面相当于一句话：RUN /bin/bash -c &#39;source $HOME/.bashrc; echo $HOME&#39; CMD一个Dockerfile里面只能有执行一个CMD指令,如果有多个CMD，只有最后一个会执行。 CMD的三种写法 CMD [“executable”,”param1”,”param2”] CMD [“param1”,”param2”] CMD command param1 param2 shell版本： 12FROM ubuntuCMD echo &quot;This is a test.&quot; | wc - LABELLABEL是给镜像加源数据，通常是键值对的形式。 LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... 实例： 1: 12345LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot;LABEL com.example.label-with-value=&quot;foo&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;This text illustrates \that label-values can span multiple lines.&quot; 2: 1LABEL multi.label1=&quot;value1&quot; multi.label2=&quot;value2&quot; other=&quot;value3&quot; 3: 123LABEL multi.label1=&quot;value1&quot; \ multi.label2=&quot;value2&quot; \ other=&quot;value3&quot; 使用docker inspect查看： 123456789&quot;Labels&quot;: &#123; &quot;com.example.vendor&quot;: &quot;ACME Incorporated&quot; &quot;com.example.label-with-value&quot;: &quot;foo&quot;, &quot;version&quot;: &quot;1.0&quot;, &quot;description&quot;: &quot;This text illustrates that label-values can span multiple lines.&quot;, &quot;multi.label1&quot;: &quot;value1&quot;, &quot;multi.label2&quot;: &quot;value2&quot;, &quot;other&quot;: &quot;value3&quot;&#125;, MAINTAINER [deprecated]MAINTAINER &lt;name&gt; 作者信息 使用LABEL更加灵活：LABEL maintainer=”SvenDowideit@home.org.au“ EXPOSEEXPOSE &lt;port&gt; [&lt;port&gt;...] 在容器运行的时候暴露端口，不会自己和宿主机绑定。如果要和宿主机绑定需要-p，制定端口。或者-P随机系统端口。 ENV12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... 注意第一种没有=号，第二个有等号。实例1：12ENV myName=&quot;John Doe&quot; myDog=Rex\ The\ Dog \ myCat=fluffy 实例2：123ENV myName John DoeENV myDog Rex The DogENV myCat fluffy 推荐使用实例1的写法，因为这样只会生成一个单缓存层(single cache layer); ADD12ADD &lt;src&gt;... &lt;dest&gt;ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] # 如果有空格必须采取这种方式。 ADD的作用就是将src目录，文件，或者url的内容加入到镜像文件系统的dest目录下。 实例：12ADD hom* /mydir/ # adds all files starting with &quot;hom&quot;ADD hom?.txt /mydir/ # ? is replaced with any single character, e.g., &quot;home.txt&quot; ADD的匹配规则和golang语言的filepath.Match匹配规则一样。 dest 是一个绝对路径，或者WORKDIR的相对路径。 ADD 遵守下面的规则： src 目录必须在构建上下文之内。不可以使用ADD ../something /something,因为在第一步使用docker build的时候就已经发送了上下文路径包括子目录给了docker守护进程 如果src是一个url，而这个url没有斜杠结尾，那么它就以为是一个文件，会下载这个文件然后拷贝到dest目录中。 如果src是url，dest没有用斜杠结尾，那么就会生成dest/filename.比如ADD http://abc.com/foobar / 会创建成/foobar 如果src是一个目录，目录下的所有文件都会被拷贝，包括文件系统的metadata。本身的文件夹不拷贝。 COPY12COPY &lt;src&gt;... &lt;dest&gt;COPY [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] #如果路径有空格，采用这种方式 用法和ADD基本类似： 12COPY hom* /mydir/ # adds all files starting with &quot;hom&quot;COPY hom?.txt /mydir/ # ? is replaced with any single character, e.g., &quot;home.txt&quot; ENTRYPOINT12ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec form, preferred)ENTRYPOINT command param1 param2 (shell form) VOLUMEVOLUME [&quot;/data&quot;] 也可以用VOLUME /var/log /var/db VOLUME /var/log 来作为分享的数据卷。 1234FROM ubuntuRUN mkdir /myvolRUN echo &quot;hello world&quot; &gt; /myvol/greetingVOLUME /myvol 上面的意思就是挂载一个/myvol，然后在执行的时候用docker run -v test:/myvol 使用docker inspect可以看到“Mounts”,”Volumes”.如果dockerfile指定Volume而没有docker run -v 那么就可以看到Mounts中会出现一个宿主机随机挂载了一个目录到VOLUME里面的指定位置。 USER12USER &lt;user&gt;[:&lt;group&gt;] orUSER &lt;UID&gt;[:&lt;GID&gt;] WORKDIR1WORKDIR /path/to/workdir WORKDIR 指令会建立一个RUN，CMD，ENTRYPOINT，COPY，ADD这些指令的工作目录。如果没有WORKDIR，它会被接下来的任何一个Dockerfile中的指令创建，就算创建之后不会使用，它还是会存在。 WORKDIR可以在一个Dockerf中出现多次，如果提供的是相对路径，那么接下来的所有路径都会跟前一个WORKDIR相对关联。比如： 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 最后执行的pwd路径为/a/b/c。WORKDIR指令也可以有环境变量使用ENV来设置： 123ENV DIRPATH /pathWORKDIR $DIRPATH/$DIRNAMERUN pwd 最后的路径会是/path/$DIRNAME ARGSTOPSIGNALHEALTHCHECKSHELLDockerfile实例： 123456➜ tree.├── Dockerfile├── jar│ └── jar_test_file└── jar_outer_file 12345678FROM busyboxENV foo /barLABEL author=chenzhijunRUN mkdir /myvolRUN echo &quot;hello world$foo&quot;$foo &gt; /myvol/greetingVOLUME /myvolADD jar* /mydir/EXPOSE 8080 12345678910111213141516171819202122232425262728293031➜ docker build -t test:1.1 .Sending build context to Docker daemon 3.584kBStep 1/8 : FROM busybox ---&gt; efe10ee6727fStep 2/8 : ENV foo /bar ---&gt; Using cache ---&gt; d3b8b9eeff78Step 3/8 : LABEL author chenzhijun ---&gt; Using cache ---&gt; 050fc3d4d93dStep 4/8 : RUN mkdir /myvol ---&gt; Running in 52850795da36 ---&gt; afd7fb49f8bfRemoving intermediate container 52850795da36Step 5/8 : RUN echo &quot;hello world$foo&quot;$foo &gt; /myvol/greeting ---&gt; Running in 70b44e104f41 ---&gt; cbd977a40b6dRemoving intermediate container 70b44e104f41Step 6/8 : VOLUME /myvol ---&gt; Running in da2e28e0effa ---&gt; 4edd7f3c690cRemoving intermediate container da2e28e0effaStep 7/8 : ADD jar* /mydir/ ---&gt; 13feb93f43f0Removing intermediate container 4bd1b0a08ef2Step 8/8 : EXPOSE 8080 ---&gt; Running in 2185ad28d925 ---&gt; 62bfe542f989Removing intermediate container 2185ad28d925Successfully built 62bfe542f989Successfully tagged test:1.1 docker inspect:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 docker inspect test:1.1[ &#123; &quot;Id&quot;: &quot;sha256:62bfe542f98957d45fada954dcbb8ced1a1d8a65e42657b98c40f88b559f690e&quot;, &quot;RepoTags&quot;: [ &quot;test:1.1&quot; ], &quot;RepoDigests&quot;: [], &quot;Parent&quot;: &quot;sha256:13feb93f43f085b44a37f6272b55f4f9bcf1b899d441f0dec83561904c8cea40&quot;, &quot;Comment&quot;: &quot;&quot;, &quot;Created&quot;: &quot;2017-09-19T15:23:52.690407548Z&quot;, &quot;Container&quot;: &quot;2185ad28d9256023fdc96fad564c8c0eb699d4f6f9b09d4acabe73beeee6790c&quot;, &quot;ContainerConfig&quot;: &#123; &quot;Hostname&quot;: &quot;44c72a15738e&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;ExposedPorts&quot;: &#123; &quot;8080/tcp&quot;: &#123;&#125; &#125;, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;, &quot;foo=/bar&quot; ], &quot;Cmd&quot;: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;#(nop) &quot;, &quot;EXPOSE 8080/tcp&quot; ], &quot;ArgsEscaped&quot;: true, &quot;Image&quot;: &quot;sha256:13feb93f43f085b44a37f6272b55f4f9bcf1b899d441f0dec83561904c8cea40&quot;, &quot;Volumes&quot;: &#123; &quot;/myvol&quot;: &#123;&#125; &#125;, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: [], &quot;Labels&quot;: &#123; &quot;author&quot;: &quot;chenzhijun&quot; &#125; &#125;, &quot;DockerVersion&quot;: &quot;17.06.2-ce&quot;, &quot;Author&quot;: &quot;&quot;, &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;44c72a15738e&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;ExposedPorts&quot;: &#123; &quot;8080/tcp&quot;: &#123;&#125; &#125;, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;, &quot;foo=/bar&quot; ], &quot;Cmd&quot;: [ &quot;sh&quot; ], &quot;ArgsEscaped&quot;: true, &quot;Image&quot;: &quot;sha256:13feb93f43f085b44a37f6272b55f4f9bcf1b899d441f0dec83561904c8cea40&quot;, &quot;Volumes&quot;: &#123; &quot;/myvol&quot;: &#123;&#125; &#125;, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: [], &quot;Labels&quot;: &#123; &quot;author&quot;: &quot;chenzhijun&quot; &#125; &#125;, &quot;Architecture&quot;: &quot;amd64&quot;, &quot;Os&quot;: &quot;linux&quot;, &quot;Size&quot;: 1129213, &quot;VirtualSize&quot;: 1129213, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: null, &quot;Name&quot;: &quot;aufs&quot; &#125;, &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:08c2295a7fa5c220b0f60c994362d290429ad92f6e0235509db91582809442f3&quot;, &quot;sha256:76229699c752a26051d4861c0e481d45e9353f66f181fd496a5ef39a8833fbff&quot;, &quot;sha256:02a0230b59819ae96f49a8603d5b1630cd357e140f48bf1e03811c5f002d8b5f&quot;, &quot;sha256:c916120dc28545b2869e046d28ababe83cfa76c2cb31c52e497450bfebb1aafa&quot; ] &#125; &#125;]]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-装饰者模式]]></title>
    <url>%2F2017%2F09%2F19%2Fjava-design-pattern-decorate%2F</url>
    <content type="text"><![CDATA[设计模式-装饰者模式 组合比继承更具有灵活性 简述装饰者模式其实一开始我没想明白，怎么给装饰法？就像当初看Java IO包，始终也没明白怎么这么多类啊。很多人都说它是什么什么装饰者设计模式，那个时候也不怎么懂，就走马观花般过了就过了。最近重新回到基础，又买了本设计模式的书，其实真的不想看，但是看了一两章发现，我靠，这不就是我之前的搞法么？猛回首，发现自己踩过的坑人家也踩过，而我还在踩。当然能力也有限，记下此博客，一，如果有人能看到这里有点了解，那是甚好；二，如果以后自己忘记了，也有个标记，让我返回至此，然后来个回忆。 回到正途，装饰者，啥事装饰者？看看平常我们说装饰房屋，那么是不是先有个房子，然后再给房子搞下装饰，刷墙啊，放书架啊，这就是一个装饰。那么装饰者模式怎么回事了？动态的将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的扩展方案。 装饰者模式的最大特点是在装饰类里面进行了组合，组合的威力发挥到最大。当然这种组合也必须要有一个约束，那么就是必须在同一类中，总不能指驴为马吧。所以装饰的是一类或者一组类似的对象。 自我的理解有时候光想真的会发现自己很多地方好像已经明白了，但是实际上，当我们写代码的时候会发现，要注意点细节真的很多。装饰者模式中，其实讲究的是子类对父类的继承扩展，以及子类对于父类子集同类的组合。理解起来其实就是，我可以用我爸爸的人脉帮我办事，也可以用我哥哥的人脉办事，最终我做事可能就不需要太多的人脉了。这种方式很好的解耦了，而且对于扩展的话，我可以随便改，而对于我老爸，老哥，我却没有改的可能。这就是设计模式的一个设计原则：开放封闭原则，类对扩展开放，对修改关闭。 我们来实际敲敲代码，假设我们要开咖啡店，咖啡店有多种原料，根据不同的原料组成不同的咖啡，拥有不同的售价。 首先我们建立基础类： 12345678910111213141516171819package me.chenzhijun.coffee.type;/** * 咖啡的基类 * @author chen * @version V1.0 * @date 2017/9/19 */public abstract class Beverage &#123; String description = "unknown beverage"; public String getDescription() &#123; return description; &#125; public abstract double cost();&#125; 之后在咖啡基础类上建立具体的咖啡： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package me.chenzhijun.coffee.type;/** * 焦炒咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class DarkRoast extends Beverage&#123; public DarkRoast()&#123; description = "DarkRoast..."; &#125; @Override public double cost() &#123; return 1.1; &#125;&#125;package me.chenzhijun.coffee.type;/** * 浓缩咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class Espresso extends Beverage &#123; public Espresso()&#123; description = "espresso..."; &#125; @Override public double cost() &#123; return 1.2; &#125;&#125;package me.chenzhijun.coffee.type;/** * 优选咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class HouseBlend extends Beverage &#123; public HouseBlend() &#123; description = "houseblend..."; &#125; @Override public double cost() &#123; return 0.1; &#125;&#125; 可以看到上面的三个咖啡都是继承自抽象类Beverage，形成同类； 接下来我们建立装饰者的基类： 1234567891011121314package me.chenzhijun.coffee;import me.chenzhijun.coffee.type.Beverage;/** * 装饰者 * @author chen * @version V1.0 * @date 2017/9/19 */public abstract class CondimentDecorator extends Beverage &#123; public abstract String getDescription();&#125; 看到装饰者类也是继承Beverage，并且也同样是抽象类，唯一的抽象方法就是要实现getDescription(); 之后我们在装饰者类的基础上增加几个具体的装饰者： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package me.chenzhijun.coffee;import me.chenzhijun.coffee.type.Beverage;/** * 摩卡咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class Mocha extends CondimentDecorator &#123; Beverage beverage; public Mocha(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public String getDescription() &#123; return beverage.getDescription()+" mocha..."; &#125; @Override public double cost() &#123; return beverage.cost()+3.1; &#125;&#125;package me.chenzhijun.coffee;import me.chenzhijun.coffee.type.Beverage;/** * 大豆奶咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class Soy extends CondimentDecorator &#123; Beverage beverage; public Soy(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public String getDescription() &#123; return beverage.getDescription() + " soy..."; &#125; @Override public double cost() &#123; return beverage.cost() + 1.1; &#125;&#125;package me.chenzhijun.coffee;import me.chenzhijun.coffee.type.Beverage;/** * 加奶泡的咖啡 * @author chen * @version V1.0 * @date 2017/9/19 */public class Whip extends CondimentDecorator &#123; Beverage beverage; public Whip(Beverage beverage) &#123; this.beverage = beverage; &#125; @Override public String getDescription() &#123; return beverage.getDescription() + " whip..."; &#125; @Override public double cost() &#123; return beverage.cost() + 2.1; &#125;&#125; 现在我们队咖啡加了一些装饰：摩卡，大豆，奶泡，这些都是装饰者类，而且装饰者类里面都有一个Beverage,这就方便我们组合其他的同族的类的功能。 最后来看先卖咖啡： 1234567891011121314151617181920212223242526272829303132333435package me.chenzhijun.coffee;import me.chenzhijun.coffee.type.Beverage;import me.chenzhijun.coffee.type.DarkRoast;import me.chenzhijun.coffee.type.Espresso;import me.chenzhijun.coffee.type.HouseBlend;/** * @author chen * @version V1.0 * @date 2017/9/19 */public class Starbucks &#123; public static void main(String[] args) &#123; Beverage beverage = new Espresso();//浓缩咖啡已经是成品了 System.out.println(beverage.getDescription() + ", " + beverage.cost()); //双份摩卡加奶泡 Beverage beverage2 = new DarkRoast(); beverage2 = new Mocha(beverage2); beverage2 = new Mocha(beverage2);//多一份摩卡 beverage2 = new Whip(beverage2); System.out.println(beverage2.getDescription() + ", " + beverage2.cost()); // 大豆+摩卡+奶泡 Beverage beverage3 = new HouseBlend(); beverage3 = new Soy(beverage3);// beverage3 = new Mocha(beverage3); beverage3 = new Whip(beverage3); System.out.println(beverage3.getDescription() + ", " + beverage3.cost()); &#125;&#125; 这样就可以看到如果我们单纯的采用继承，那么将会继承自多个类，而且很笨重，而组合的方式很灵活； 装饰者模式中，jdk中用的最多的可能就是IO包了。基本上所有的IO类都继承至四大主类:Reader/Writer;InputStream/OutputStream。 其实我们只要理解了一个其它的就是可以类比了。 Reader/Writer 是在Stream之后引进的，因为对字符的操作比较多，所以引进了新的字符输入输出。jdk1.7之后都实现了AutoCloseable接口,try-with-resources语法来实现自动关闭流。 我们仅仅来看下InputStream类，所有的输入流都可以算作是InputStream的子类，在InputStream中定义了几个方法，查看子类中有个FilterInputStream类，而且大多数的类都继承自FilterInputStream而不是InputStream，仔细的看下FilterInputStream类中的方法，可以发现FilterInputStream中有一个InputStream的属性，而所有的方法都是用的是这个属性的方法，FilterInputStream就是一个装饰类，查看它的一个子类DataInputStream，可以发现DataInputStream中只有一个构造方法为DataInputStream(InputStream);FilterInputStream类的说明中，也指到它的作用就是包含一个其它的InputStream族类，然后使用它们的增加的方法来为己用。 类比下装饰者模式，FilterInputStream可以说就是一个装饰者，我们要用哪个InputStream的时候也是将InputStream层层包括装饰进去。如果这样来看JavaIO流其实也就并没有那么复杂了。当然因为装饰者模式的特点，我们可以看到生产了很多的小类，是的IO包的类异常的多。 参考文档：Head First 设计模式 第三章装饰者模式]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（二）任务队列]]></title>
    <url>%2F2017%2F09%2F18%2Fmessage-rabbitmq-2%2F</url>
    <content type="text"><![CDATA[消息中间件 RabbitMQ (二) 任务队列 第一篇中简单介绍了mq的使用，那么第二节中来了解下mq的其它内容. 工作队列这节我们将建立一个工作队列来讲任务分发到不同的消费工作者中去。工作队列的主要思想是避免在做资源密集型任务的同时又不得不等待一个个完成。这种时候我们像消息一样封装所要完成的任务然后将它发送到队列中，在后台运行的工作线程会将任务拿出，然后执行这个任务。当运行多个工作线程的时候，任务对他们都是可见的。 网络应用中如果一个任务无法快速执行完，那么工作队列就非常有用。 准备像上节一样，我们准备两个类，一个生产(Tasker.java)一个消费(Worker.java). Task.java ： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package me.chenzhijun;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * 发送者 * * @author chen * @version V1.0 * @date 2017/9/18 */public class Tasker &#123; private final static String QUEUE_NAME = "task_queue"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); //String message = getMessage(new String[]&#123;"this is my message"&#125;); //channel.basicPublish("", QUEUE_NAME, null, message.getBytes()); for (int i = 0; i &lt; 10; i++) &#123; String message = getMessage(new String[]&#123;"this is my message"&#125;); message = message + i; channel.basicPublish("", QUEUE_NAME, null, message.getBytes()); &#125; channel.close(); connection.close(); &#125; private static String getMessage(String[] strings) &#123; if (strings.length &lt; 1) return "Hello World!"; return joinStrings(strings, " "); &#125; private static String joinStrings(String[] strings, String delimiter) &#123; int length = strings.length; if (length == 0) return ""; StringBuilder words = new StringBuilder(strings[0]); for (int i = 1; i &lt; length; i++) &#123; words.append(delimiter).append(strings[i]); &#125; return words.toString(); &#125;&#125; Worker.java : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package me.chenzhijun;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * 接受者 * * @author chen * @version V1.0 * @date 2017/9/18 */public class Worker &#123; private final static String QUEUE_NAME = "task_queue"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); Consumer consumer = new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; super.handleDelivery(consumerTag, envelope, properties, body); String messageReceiv = new String(body, "UTF-8"); System.out.println("[x] received: " + messageReceiv); try &#123; doWork(messageReceiv); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(" [x] Done"); &#125; &#125; &#125;; channel.basicConsume(QUEUE_NAME, false, consumer); &#125; private static void doWork(String task) throws InterruptedException &#123; for (char ch : task.toCharArray()) &#123; if (ch == '.') Thread.sleep(1000); &#125; &#125;&#125; 在这次的模拟中，我们需要同时启动多个Worker，这样我们才能在控制台中看到不同的Worker会打印不同的值。 循环调度使用任务队列的一个优点是易于并行工作，当一个worker在工作的时候，我们可以将任务给另一个worker。默认情况下，RabbitMQ会发送任务给下一个消费者，着这样每一个消费者能得到一个比较平均的任务数量。这种任务转发方式叫做循环调度。可以启动两个Worker进行尝试一下。 消息确认一个任务执行的时间可能需要花费数秒。我们可能想要知道一个消费者在执行了一个长时间任务的时候，如果只是完成了一部分，然后挂掉了怎么办？用我们现在的代码，一旦RabbitMQ传递了一个消息给消费者，它就会立即将它做删除标记。所以在这个问题上，如果你在消费者执行期间kill掉了它，我们就永久的失去了这个消息。我们也会丢失掉所有MQ中间件给它的但是实际上并没有被正确处理的所有消息，以及这些消息的所有详细内容。 但是实际开发中，我们并不想失去任何一个队列中的任务。如果一个worker挂掉了，我们可能想要将这个任务转发给其他的worker。 为了保证消息绝不丢失，RabbitMQ支持[消息完成确认](http://www.rabbitmq.com/confirms.html),就像网络http请求的ACK(确认)。消费者在接收到详细完整的消息，并且完成了消息的任务，然后会发送一个ack给RabbitMQ，RabbitMQ才会删除它。 如果消费者线程死了(channel关闭，connection关闭，或者TCP 连接丢失)并且挂掉之前没有发送ACK。RabbitMQ会知道消息没有圆满完成，然后会让它重新排到队列中去，如果在这个时候有其它的消费者在线，它就会马上将消息推送给其他消费者。这个方式就能确保尽管有worker不稳定挂掉了，消息也不会丢失。 这里是不会有任何消息超时的，RabbitMQ只会在Worker挂掉之后才会重新转发消息，就算一个处理消息的过程会花费很长很长的时间。 默认情况下消息确认是打开的，Manual message acknowledgments。在之前的例子中我们明确的关闭了它autoAck=true;如果你在声明中用到了autoAck=false; 123boolean autoAck=false;channel.basicConsume(QUEUE_NAME, autoAck, consumer); 这个时候把线程时间拉长一点，然后强制kill掉，这个worker挂掉之后也不会重新被MQ转发消息了。 ps: 忘记消息确认会引发一个非常严重的后果，如果它不能释放未确认的消息的内容,RabbitMQ会不停的吃掉内存空间，如果要调试的话可以在Linux下使用sudo rabbitmqctl list_queues name messages_ready messages_unacknowledged 或者windows下使用rabbitmqctl.bat list_queues name messages_ready messages_unacknowledged. 消息持久化我们知道了就算消费者挂了，也能保证任务不会丢失。但是如果我们的RabbitMQ服务器宕机了呢？队列中的任务还是会丢失。 当RabbitMQ退出或者宕机的时候，它会丢失掉所有的队列和消息，除非你对它进行设置。要保证消息不会丢失，必须先做两件事情，我们需要标记队列和消息可以持久化。 首先我们需要保证RabbitMQ绝不会丢失掉我们的队列。为了做到这个，我们需要声明它为durable持久化。 123boolean durable = true;channel.queueDeclare(QUEUE_NAME,durable,false,false,null); 上面的命令本身没有错误，但是如果队列中已经有了QUEUE_NAME的队列，并且前面定义的队列不是durable的，RabbitMQ不会允许你重新用不同的参数定义一个已经存在的队列，如果尝试这样做的话，它会返回一个错误。变通的方法就是取另一个唯一的名字。如果队列名字改了，消费者和生产者要对应到一个队列上。 声明了队列为持续化之后，我们也需要将消息标记为持久化，通过设置MessageProperties(继承至BasicProperties) 的值为PERSISTENT_TEXT_PLAIN. 1234import com.rabbitmq.client.MessageProperties;channel.basicPublish("",QUEUE_NAME,MessageProperties.PERSISTENT_TEXT_PLAIN,message.getBytes()); 消息持久化注意：标记消息持久化不能完全的保证消息不会被丢失，尽管它告诉了RabbitMQ保存消息到磁盘，但是在RabbitMQ接收到消息，这里仍然有一个很短的时间，可能让RabbitMQ没有保存它到磁盘。RabbitMQ不允许对每一个消息进行fsync，它仅仅是存到缓存当中而不是真的写入到磁盘当中。这种持久化的保证不是特别稳定的，但是它也足够我们在简单的任务队列中使用。如果你需要稳定的强保证你可以使用[publish confirms](https://www.rabbitmq.com/confirms.html) 公平转发你可能注意到了任务分发有时候并不是精确的像我们期待的那样，例如有一种场景：两个worker，当所有的消息不管是简单的还是难的，可能出现一个worker非常忙，而另一个却没有什么任务。RabbitMQ是不会知道这回事的，然后还是照样的循环分配任务。 这种事情的发生是因为RabbitMQ只是在一个消息进入到队列中转发一个消息，它不会去为消费者考虑未确认的消息，它只是盲目的一个接一个的转发消息给一个消费者。 为了防止这种事情，我们可以使用basicQos方法来设置prefetchCount=1属性值。它会告诉RabbitMQ不要在同一个时间段给一个worker太多的任务，或者换种说话，不要转发一个新的消息给一个worker直到它完成前一个队列并且回复ACK。另外，MQ会将这个任务转发给下一个不忙的worker。 123int prefetchCount=1;channel.basicQos(prefetchCount); 注意队列大小，如果所有的worker都很忙，队列就有可能会堵塞，你可能就需要持续关注它，或者增加worker，或者采用其它的策略来代替。 使用消息的消息确认(acknowledgments)和prefetchCount,你可以建立一个工作队列，持久化的设置可以让RabbitMQ就算重启，任务也不会消失。 如果想要了解更多的关于Channel的方法和MessageProperties的属性，你可以在线看文档 参考文档：RabbitMQ Java]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ-消息中间件（一）]]></title>
    <url>%2F2017%2F09%2F15%2Fmessage-rabbitmq%2F</url>
    <content type="text"><![CDATA[消息中间件 RabbitMQ (一)简述RabbitMQ 是一个消息中间件，类似于传统的邮局，不过RabbitMQ充当了邮箱，邮局，邮递员的角色。和邮局不同的是，它使用消息接受，存储，发送二进制数据。 RabbitMQ 的一些术语Producing: 生产者，发送消息方，只发送消息。 queue: 消息队列，允许消息从RabbitMQ传送到应用程序，消息只能被存储在队列中。队列仅仅被宿主机的内存和磁盘空间限制，本质上它是一个大的缓存块。生产者可以往队列发消息，消费者也可以从队列中接受消息。 Consuming:消费者，类似于收信人，消费者程序通常是在等待接收消息。 ps:生产者，消费者和中间件不应该在同一台宿主机上面，实际中大多数应用都不会这样做。 “Hello World” Send.java,发布者发送消息 12345678910111213141516171819202122232425262728293031323334353637383940package me.chenzhijun;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/15 */public class Sender &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); //连接本地的消息中间件，如果是其它机器换成ip就行了,也可以对连接进行授权，协议版本等控制// connectionFactory.setPassword(); Connection connection = connectionFactory.newConnection();//socket连接, 大多数任务的完成都是调用connection的api Channel channel = connection.createChannel(); //1 发送消息之前，必须先声明一个发送消息的队列，然后我们往队列里面发送消息 channel.queueDeclare(QUEUE_NAME, false, false, false, null);// 队列声明是幂等的，它只会在不存在的时候创建 String message = "Hello,World"; channel.basicPublish("", QUEUE_NAME, null, message.getBytes());// 消息内容是一个字节数组，可以用使用任何编码 //2 关闭 channel.close(); connection.close(); System.out.println("[x] sent:" + message); &#125;&#125; Receiver.java,先要声明确定建立连接，因为需要等待接收消息 12345678910111213141516171819202122232425262728293031323334353637package me.chenzhijun;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;/** * @author chen * @version V1.0 * @date 2017/9/16 */public class Receiver &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws IOException, TimeoutException &#123; //1 前面的操作都是类似，都是需要链接的一些配置 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); System.out.println("[X] receiving message"); Consumer consumer = new DefaultConsumer(channel) &#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; super.handleDelivery(consumerTag, envelope, properties, body); String message = new String(body, "UTF-8"); System.out.println("[x] Received : " + message); &#125; &#125;; channel.basicConsume(QUEUE_NAME,true,consumer); &#125;&#125; maven仓库包： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;3.5.7&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 使用Demo的时候需要先下载安装RabbitMQ，可能还需要安装erlang，安装完成后打开网站localhost:15672,就可以看到rabbitMQ的管理后台了。 参考文档： RabbitMQ Java]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>message</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-观察者模式]]></title>
    <url>%2F2017%2F09%2F14%2Fjava-design-pattern-observer%2F</url>
    <content type="text"><![CDATA[设计模式-观察者模式简述观察者模式的有点类似现在的手机消息推送，比如你注册了一个APP，然后app有了更新一定会通知你，不然你看看手机里面的推送栏是不是每天淘宝都会推送给你一条消息。哈哈。当然为什么它知道要给你推送，不给哪些非智能机推送了？简单的更类似与现在报纸-订阅。只要你订了报纸，那么每天报纸更新都会把报纸给你送过来。所以我认为观察者模式应该也叫”订阅模式”，因为必须客户端先去注册（订阅）然后才能收到注册中心的更新或者通知。 实例我们先来想想观察者模式中有哪些主体： 注册中心（主题（或者叫被观察对象）） 客户端 (观察者) 现在我们分析下里面的行为： 报社（注册中心-主题）怎么知道有哪些人订了报纸（客户注册）了？那么就是报社必须要有一个记录有哪些人（客户端-观察者）在报社注册的’花名册’；那么现在是法治社会，当然不能强迫去订报纸吧，所以人们订报纸都是自愿的，那么自愿就必须自己去报社订阅(主动注册)。订了报纸之后，不能收不到报纸吧？那报社现在都是比较强势的，一般把报纸送到门口，所以你必须有一个接受的地址，这种算是约定；既然可以订阅，那么当然也有取消订阅（取消注册）；那么报社也就在’花名册’把你的名字去掉就可以了。 整个实现过程就是这样，观察者模式并不复杂，主要是报社(注册中心-主题)，花名册(订阅者名单)，订阅者（接受通知者），订阅(行为)，门(注册中心通知接受者的地址)。开始写我们的代码： 1234567891011121314151617181920212223242526272829303132333435363738 public class NewsPaperSubject&#123; private List observers;//花名册 private String newsPaper; //新闻内容 public void register(Observer observer)&#123; observers.add(observer);//注册一个观察者、增加一个订阅客户 &#125; public void unRegister(Observer observer)&#123; observers.remove(observer);//取消订阅 &#125; public void sendNewsPage()&#123; for(Observer obj:observers)&#123; obj.updateNewsPageOnDoor(newsPaper); // 告知订阅者，新闻有更新 &#125; &#125; public void setNews(String news)&#123; this.newsPaper = news; // 设置新闻 sendNewsPage(); &#125;&#125;public class Observer&#123; private NewsPaperSubject newsPaperSubject; public void updateNewsPageOnDoor(String news)&#123; System.out.println("更新news...."+news); &#125; public Observer(NewsPaperSubject newsPaperSubject)&#123; this.newsPaperSubject = newsPaperSubject; this.newsPaperSubject.register(this); &#125;&#125; 根据上面两个类我想你能基本了解订阅者模式是怎样一回事了。不过我们写的代码还是有点问题，耦合太严重，我们不应该针对具体实现编程，所以我们进行一次改版： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 //主题超类 public interface Subject&#123; void registerObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers(); &#125; public class NewsPaperSubject extends Subject&#123; private List observers;//花名册 private String newsPaper; //新闻内容 @Override public void registerObserver(Observer observer)&#123; observers.add(observer);//注册一个观察者、增加一个订阅客户 &#125; @Override public void removeObserver(Observer observer)&#123; observers.remove(observer);//取消订阅 &#125; @Override public void notifyObservers()&#123; for(Observer obj:observers)&#123; obj.update(newsPaper); // 告知订阅者，新闻有更新 &#125; &#125; public void setNews(String news)&#123; this.newsPaper = news; // 设置新闻 notifyObservers(); &#125;&#125;// 观察者接口public interface Observer&#123; void update(String newsPaper); &#125; // 具体观察者 public interface StudentObserver&#123; private Subject newsPaperSubject; public StudentObserver(Subject newsPaperSubject)&#123; this.newsPaperSubject = newsPaperSubject; newsPaperSubject.registerObserver(this); &#125; @Override public void update(String news)&#123; System.out.println("updateNews.."+news); &#125; &#125; 新版本的情况如果我们有了个政府观察者，我们只需要实现一个Observer新的实现对象就可以了。这种就是针对接口编程，具体的实现细节都是隐藏的。这样其实我们就做到了松耦合。对于注册中心来说只要订阅者实现了Observer的update接口，并且注册到花名单就行了。这样当注册中心发现改变需要通知的时候就可以通知到所有花名单中的订阅者。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-策略模式]]></title>
    <url>%2F2017%2F09%2F12%2Fjava-design-pattern-strategy%2F</url>
    <content type="text"><![CDATA[Java设计模式-策略模式简述策略，什么是策略？就在不同情况下用不同的方式去处理。那么策略模式是什么？ 模式，模式简而言之就是套路，就是前人总结下来得到验证的套路。所以策略模式其实就是用策略去解决一类事情的套路。 策略模式的一个小思考说到套路，那么一个套路就要有组成部分，不然怎么套路？而策略模式中，它的主要点就是变化的情况，以及应对变化的处理方法。比如有一家超市打折，可能为了吸引顾客、庆祝节日等，但是可能中途来个产品经理，他说为了利润最大化，要根据付款总数来对顾客实行不同打折额度。这种时候程序员心中都是千万匹马儿在奔腾，就不能为为别人做点好事么？（其实心里就是想着，tmd，又改需求，我还想回家陪老婆孩子的）~~~咳咳，走远了。 就这个超市打折来说，打折这种事情是一定存在的，而对打折的方式可能还会有各种变化。那么怎么去应对这种变化？再说一个例子，如果有个制造鸭子的工厂，我们都知道鸭子有很多类（比如死的，活的），死的就像玩具类~，活的就像烤鸭类(貌似烤之前才是活的.),不管怎样，我想说大多数鸭子都有一些基本行为，比如：飞？叫？游泳？诸如此类..但是我们会发现，鸭厂产出的鸭子最终还是要卖出去的，那么这个鸭子能不能飞？能不能叫？能不能游泳？那都是买家说了算，谁给钱听谁的。那么好了A可能要一个能飞，但不能叫，也不能游泳；B呢想要个能飞，能叫，能游泳；这下如果鸭厂只能生产一种只会飞，或者只会叫的鸭子，那肯定是不行的，但是也不能为了A专门搞个生产线，B专门搞个生产线，这样搞，要是N多客户，N多需求，那可怎么办？说了这么多，基本是废话，那么还是来代码看看。 鸭厂造鸭鸭厂生产鸭,先确定一个Duck类，鸭子基本上有飞，叫，游泳的功能属性;那么鸭子就有了三个方法： 123456789public abstract class Duck&#123; public void fly()&#123;&#125;; public void swim()&#123;&#125;; public void gua()&#123;&#125;;//呱呱叫&#125; 先看下如果要满足A，不用策略模式的话，我们是这样玩： 1234567891011121314151617public class ADuck extends Duck&#123; @Override public void fly()&#123; System.out.println(&quot;A的鸭子会飞&quot;); &#125; @Override public void swim()&#123; System.out.println(&quot;A鸭子不能游泳&quot;); &#125; @Override public void gua()&#123; System.out.println(&quot;A鸭子不能叫，太吵&quot;); &#125;&#125; 对于每一个需求方都需要重新实现一遍，这是不现实的，要是上千个可怎么办？而且用继承的方式，我们就必须的检查每一个继承的方法，那些方法要重写， 那些不需要. 鸭厂改造想想看鸭子，鸭子还是鸭子，我们换个角度想，鸭子了，是不会变的，那它还是主体；客户变的是啥？是鸭子的功能，而这种功能我们可以集中起来，对于鸭子我们将它的功能抽出来。当他需要什么功能再给它什么功能。只有它需要我再给。这也是一个设计原则：多用组合，少用继承 12345678910111213141516171819202122public interface FlyBehavior&#123; void fly();&#125;public interface SwimBehavior&#123; void swim();&#125;public abstract class Duck&#123; FlyBehavior flyBehavior; SwimBehavior swimBehavior; public void fly()&#123; flyBehavior.fly(); &#125;; public void swim()&#123; swimBehavior.swim(); &#125;; //public void gua()&#123;&#125;;//呱呱叫&#125; 我们用接口将飞和游泳的行为组合进去Duck，你会问，为啥不直接让Duck实现flyBehavior和swimBehavior接口了？其实如果使用实现的话，那么就会发现一个鸭子可能实现了N个行为，而且对于N个行为你都的重写他的实现方法，不能好的复用。 然后我们只需要实现FlyBehavior和SwimBehaior接口就可以了：123456789101112131415161718192021陆地鸭：public class CanNotSwim implements swimBehavior&#123; @Override public void swim() &#123; System.out.println(&quot;不会游泳的鸭子....&quot;); &#125;&#125;用翅膀飞的鸭子：public class FlyWithWings implements FlyBehavior&#123; @Override public void fly() &#123; System.out.println(&quot;用翅膀飞......&quot;); &#125;&#125; 然后我们将鸭子的行为给我们要造的鸭子： 12345678910会飞不回游泳的鸭子：public class CanFlyButCanNotSwimDuck extends Duck&#123; public CanFlyButCanNotSwimDuck() &#123; swimBehavior = new CanNotSwim(); flyBehavior = new FlyWithWings(); &#125;&#125; 刚刚是通过构造方法来指定行为，将鸭子造出来的。那么这样其实貌似每一种鸭子我好像还是得new一个子类出来，还是不太优雅，怎么弄？ 给鸭子升级： 1234567891011121314151617181920212223public class Duck&#123; public void fly( FlyBehavior flyBehavior)&#123; flyBehavior.fly(); &#125;; public void swim( SwimBehavior swimBehavior)&#123; swimBehavior.swim(); &#125;;&#125;public class Test&#123; public static void main([]string args)&#123; Duck A = new Duck(); a.fly(new FlyWithWings()); a.swim(new CanNotSwim()); Duck B = new Duck(); B.fly(new FlyWithWings()) &#125;&#125; 当然也可以通过set方法来设置行为。 123456789101112131415161718public class Duck&#123; FlyBehavior flyBehavior; SwimBehavior swimBehavior; public void fly()&#123; flyBehavior.fly(); &#125;; public void swim()&#123; swimBehavior.swim(); &#125;; public void setFlyBehavior(FlyBehavior flyBehavior)&#123; this.flyBehavior=flyBehavior; &#125;&#125; 这种时候我们就能看到策略模式的用处了，当来了一个新的功能，比如造一个’带火箭飞的鸭子’，那么只需要实现一个飞的行为就可以了 123456789public class FlyWithRocket implements FlyBehavior&#123; @Override public void fly() &#123; System.out.println(&quot;火箭带飞....&quot;); &#125;&#125; 只需要需要的地方再设置成飞的行为为FlyWithRocket就可以了。 可以看到我们将会变化的部分抽离出来，然后进行组合；这也说明了软件开发的一个设计原则：针对接口编程,而不是实现编程。对于抽出来的一组行为，其实是独立于鸭子的，这也说明我们可以在其他地方使用这些行为。比如造鸭鸣器就可以复用鸭子的叫的行为。 策略模式总结总的来说，策略模式就是定义了一组行为，将各行为分别封装起来，然后可以互相替换。这组行为是独立于真正使用行为的实体的。 如果在再开发中，思考下哪些是变动的，哪些是可以抽出来的。业务变动是不可预计的，不能为了设计而设计，但是多留心，代码也能写的更加优雅。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastJson 转换Map问题]]></title>
    <url>%2F2017%2F08%2F19%2Fjava-fastjson-map%2F</url>
    <content type="text"><![CDATA[使用FastJson 转换成HashMap&lt;String,List&gt;问题背景需求有个json串需要转换成Map&lt;String,List&gt;这种格式,在使用fastjson的时候发现出现异常,调试后发现是类型转化出错,它将List转成了JSONArray,这个就很尴尬了.这个问题也找了很久,一直以为它可以直接转换成我想要的List,没想到最后来了个JSONArray. 解决方案 因为实际中我们需要的是List而不是JSONArray,如果每个都是自己手动去改变,不太符合程序员的特质.所以想到一个通用办法解决:将JSONArray的值取出来,转换成List再重新塞进Map对象里面,具体实现代码: 12345678910111213141516171819202122232425private static &lt;T&gt; HashMap&lt;String, List&lt;T&gt;&gt; fromJson2Map(String jsonString)&#123; HashMap&lt;String, Object&gt; jsonMap = JSON.parseObject(jsonString, HashMap.class); HashMap&lt;String, List&lt;T&gt;&gt; resultMap = new HashMap&lt;&gt;(); for(String key : jsonMap.keySet()) &#123; JSONArray jsonArray = (JSONArray)jsonMap.get(key); List list = handleJSONArray(jsonArray); resultMap.put(key, list); &#125; return resultMap;&#125;private static &lt;T&gt; List&lt;T&gt; handleJSONArray(JSONArray jsonArray)&#123; List&lt;T&gt; list = new ArrayList(); for(Object object : jsonArray) &#123; JSONObject jsonObject = (JSONObject)object; T obj = (T)JSONObject.parseObject(jsonObject.toJSONString(), Object.class); list.add(obj); &#125; return list;&#125; 在最后T obj = (T)JSONObject.parseObject(jsonObject.toJSONString(), Object.class); 其实我一开始是犹豫的,主要是Object.class,后来想想Java的Object是一切对象的父类,自然可以多态转成子类啊.所以这里就完美的解决了问题. 当然代码应该会有性能问题,应该是new ArrayList() 吧, 每次都会new一个新对象,用完了之后也没有其它的用了,一直等着垃圾回收?.突然觉得自己路还有很长的要走.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 特殊字段脱敏]]></title>
    <url>%2F2017%2F08%2F19%2Fjava-sensitive%2F</url>
    <content type="text"><![CDATA[利用Java给敏感字段脱敏需求客户信息中,有敏感字段比如身份证,银行卡,手机号码等字段要进行脱敏处理(加星号). 解决方案 想实现一个工具类调用XxxUtils.maskObject(obj);就可以实现脱敏,过程中想到的是用反射,然后给需要脱敏的字段加上注解.可能有需求要将星号变成美元符号,手机号码保留前3位与后四位,身份证保留前后,各一位,所以考虑在注解@Sensitive中加入3属性sensitiveChar替换字符,prefixLength前缀预留长度,suffixLength后缀预留长度;之后再maskObject()方法中进行反射之后将值进行脱敏处理. 定义的Sensitive注解: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Sensitive&#123; /** * 是否打码,默认为true * @return */ boolean mask() default true; /** * 前缀预留位数 * * 比如: * prefixLength=3 * * private String idCard=&quot;41234560987123543&quot; * * 打码后为: idCard= 412************** * * @return */ int prefixLength() default 0; /** * 后缀预留位数 * * 比如: * suffixLength=3 * * private String idCard=&quot;41234560987123543&quot; * * 打码后为: idCard= **************543 * @return */ int suffixLength() default 0; /** * 敏感字符替换字符 * * 比如: * sensitiveChar=&quot;$&quot;,(suffixLength=3); * * private String idCard=&quot;41234560987123543&quot; * * 打码后为: idCard= $$$$$$$$$$$$$$543 * @return */ String sensitiveChar() default &quot;*&quot;;&#125; Sensitive主要为类的属性进行描叙,方便我们反射的时候获取预留信息. 定义的反射工具类MaskUtils: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.util.StringUtils;import java.lang.reflect.Field;import java.util.ArrayList;import java.util.List;/** * * @author chen * @version V1.0 * @date 2017/8/17 */public final class MaskUtils&#123; private static final Logger logger = LoggerFactory.getLogger(MaskUtils.class); /** * List&lt;object&gt; 打码 * * @param obj * @param &lt;E&gt; * @return * @throws InstantiationException * @throws IllegalAccessException */ public static &lt;E&gt; List&lt;E&gt; maskListObject(List&lt;E&gt; obj) throws InstantiationException, IllegalAccessException &#123; if(null == obj || obj.isEmpty()) &#123; return new ArrayList&lt;&gt;(); &#125; List&lt;E&gt; maskList = new ArrayList&lt;&gt;(); for(E e : obj) &#123; maskObject(e); maskList.add(e); &#125; return maskList; &#125; /** * 打码 * * @param obj * @param &lt;E&gt; * @return * @throws IllegalAccessException * @throws InstantiationException */ public static &lt;E&gt; E maskObject(E obj) &#123; logger.info(&quot;打码前:&#123;&#125;&quot;, JsonUtil.toJSONString(obj)); if(null == obj) &#123; return null; &#125; Class aClass = obj.getClass(); Field[] declaredFields = aClass.getDeclaredFields(); for(Field field : declaredFields) &#123; field.setAccessible(true); Sensitive annotation = field.getAnnotation(Sensitive.class); if(null == annotation || !field.getType().equals(String.class)) &#123; continue; &#125; if(annotation.mask()) &#123; String value = null; try &#123; value = (String)field.get(obj); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; if(StringUtils.isEmpty(value)) &#123; continue; &#125; int suffix = annotation.suffixLength(); int prefix = annotation.prefixLength(); String character = annotation.sensitiveChar(); try &#123; field.set(obj, maskString(value, produceCharacter(value.length() - prefix - suffix, character), prefix, suffix)); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; logger.info(&quot;打码后:&#123;&#125;&quot;, JsonUtil.toJSONString(obj)); return obj; &#125; public static String maskString(String id, String replacement, int prefix, int suffix) &#123; if(null == id || &quot;&quot;.equals(id)) &#123; return &quot;&quot;; &#125; String value = id.replace(id.substring(prefix, id.length() - suffix), replacement); return value; &#125; public static String maskPhone(String mobilePhone) &#123; return maskString(mobilePhone, &quot;*&quot;, 3, 4); &#125; public static String produceCharacter(int len, String character) &#123; StringBuilder sb = new StringBuilder(); for(int i = 0; i &lt; len; i++) &#123; sb = sb.append(character); &#125; return sb.toString(); &#125; private MaskUtils() &#123; &#125;&#125; 工具类里面抛出两个异常,其实可以查看到是不会出现异常的情况的. 测试类: 12345678910111213@Datapublic class User&#123; @Sensitive(prefixLength = 3, suffixLength = 4) private String userPhone;&#125;User user = new User;user.setUserPhone(&quot;13823456789&quot;);MaskUtils.maskObject(user);//userPhone:138****6789 其实Java反射这样用着还是不错的,第一次尝试自己造轮子. 于是当我第一次把轮子给我的小伙伴的时候,他直接怼了我一句:花了一下午搞定这个,你直接重写toString不久可以了?……. WTF, 还有这种操作????????? 但是后来仔细想想,他的这种可能侵入就很大了,比如我的类toString是输出类的原始信息,按照他这样,那我怎么获取类原始信息.另外每一个类里面都要去这样写,我这种懒人不太适合.所以~~~~,我就喜欢造轮子.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔-8-19]]></title>
    <url>%2F2017%2F08%2F19%2Fessay-8-19%2F</url>
    <content type="text"><![CDATA[8月19日 阶段总结日期格式?SimpleDateFormat ===&gt; SimpleDateFormat Double 后面有个E?A: double 为啥后面有个E ,E后面的数字代表后面乘以10的位数 java 格式化金额显示A: numberFormat,留4位小数: 123456789public static final int DEFAULT_FORMAT = 4;public static String formatCurrency(Double money)&#123; NumberFormat numberFormat = NumberFormat.getNumberInstance(); numberFormat.setMaximumFractionDigits(DEFAULT_FORMAT); return numberFormat.format(money);&#125; 查看服务器上java命令位置:A: whereis javac/java ls -al 一直找到没有软连接位置 jenkins 构建的时候,nohup: failed to run command 没有找到目录,没有权限A: jenkins启动springboot脚本 12345678910111213141516#!/bin/bashsource /etc/profile#pname=$1pname=com.apec.warehouse-server-1.0-RELEASE.jarpuser=rootpid=`ps aux | grep $pname | grep $puser | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`if [[ -z $pid ]]; then echo &quot;I can NOT find $pname running by $puser&quot;fi#/home/cncsen/warehouse/com.chmod 777 /home/cncsen/warehouse/com.apec-warehouse-server-1.0-RELEASE.jarkill -9 $pid &gt;/dev/null 2&gt;&amp;1cd /home/cncsen/warehouse/exec nohup java -Xmx128m -Xss256k -jar $pname --cacheType=single --spring.profiles.active=test 5 &gt;&gt;server.log &amp;tail -f server.log]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>Question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用jenkins对springboot进行CI]]></title>
    <url>%2F2017%2F08%2F10%2Fjenkins-ci%2F</url>
    <content type="text"><![CDATA[写给小白看的jenkins CI教程目标使用jenkins持续部署一个springboot项目,项目使用jar包发布.从安装jenkins的机器将jar包发布到指定的服务器目录,并且使用脚本运行springboot. (前面几张图可能看不见,以后补上) 个人背景Java工程师毕业一年,Linux一般熟悉,Java初阶,所以不用担心难度,如果有疑问可以联系我:`vbookchen@gmail.com` 系统环境与准备jenkins就是相当于一个web服务,与环境没有太大关系 1) Linux/Windows 操作 2) jenkins.jar/jenkins.msi //安装就是了,其他的是系统可以下载war包在tomcat里面运行 3) gitlab.com 账号 4) gitlab 公有仓库放置 spring-boot 源码 5) maven // 可以用jenkins安装也可以用本地的 6) JDK 安装jenkins的主机必须要有jdk, docker版本除外 7) 部署服务的Linux服务器 基础搭建安装好jenkins之后,windows是会在服务里面可以看到一个jenkins服务的.占用端口8080.Linux的没操作过,docker版本的jenkins镜像记得映射本地端口. mac版本应该也是在8080端口,可以看到服务. 登录localhost:8080 看到一个初始化页面,页面会提示初始化密码的问题,一串很长的字符: 输入密码之后会让我们选择安装模式,安装插件,我们选择默认模式: 稍等一会,根据网速快慢不一样,等待时间不一样,之后选择创建用户密码: ps:这里记得选择右边Save And Finish那个,左边的continue as admin 这个时候是没有创建的,默认账号密码为admin/初始密码 安装成功后可以说第一步完成. 插件安装默认安装的插件是没有maven和ssh插件的,所以需要重新安装: 1) 安装maven插件,maven integration plugin: 2) 安装ssh插件,publish over ssh: 3) 安装git插件,默认的插件安装列表已经安装了 4) 如果插件安装不成功(gfw,国内有时候是这样),可以到系统管理--插件管理--高级--上传插件;插件下载地址:jenkins插件下载地址,找到对应的插件hpi文件下载后上传就可以了. 安装之后第二步完成 环境配置配置好基本环境可以省很多时间,看到jenkin主页的左侧有很多选项: 1) 配置全局属性,系统管理--&gt;系统设置看到Environment variables,加入LANG=zh_CN.UTF-8,其实可以先去系统管理--&gt;系统信息找到file.encoding,看看属性是不是utf-8. 2) 配置ssh信息,系统管理--&gt;Publish over ssh 如果没有publish over ssh那么是第二步中插件没有安装成功. 3) 配置JDK信息,系统管理--&gt;Global Tool Configuration,选中jdk--jdk安装 4) 配置maven项目,系统管理--&gt;Global Tool Configuration,选中maven--maven安装 新建任务基础打好后,开始建楼 1) jenkins主页左边栏:新建建立第一个CI任务. 2) 对任务项做相关配置: 123456a) 任务名称/介绍;b) 源码地址(git/svn);c) 构建触发器;d) Build;e) PostSteps; 配置需要将生成的jar包等上传到哪个服务器,执行哪个脚本 ;f) 保存 更正下配置,maven 构建那里跳过测试,应该是 -Dmaven.test.skip=true -X 3) 成功后的页面: 4) 点击立即构建---build history---console output可以看到构建的输出 ps 附录一段脚本 stop.sh: 12345678#stop.sh#!/bin/bashecho &quot;Stopping SpringBoot Application&quot;pid=`ps -ef | grep girl-0.0.1-SNAPSHOT.jar | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`if [ -n &quot;$pid&quot; ]then kill -9 $pidfi start.sh: 1234567891011121314151617181920212223242526export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/jreecho $&#123;JAVA_HOME&#125;echo &quot;start startup girl&quot;chmod 777 /home/girl/girl-0.0.1-SNAPSHOT.jarecho &quot;grant right....&quot;cd /home/girl/nohup $&#123;JAVA_HOME&#125;/bin/java -jar girl-0.0.1-SNAPSHOT.jar &gt; /home/girl/server.log &amp;whatToFind=secondsfunction watch()&#123; tail -1f server.log | while IFS= read line do echo &quot;buffering:&quot; &quot;$line&quot; if [[ &quot;$line&quot; == *&quot;$whatToFind&quot;* ]]; then echo $msgAppStarted pkill tail fi done&#125;watchecho &quot;start success&quot; 问题：jenkin tail -f server.log的时候一直build unstable。 但是又想让控制台输出日志，所以在脚本里面加上一个watch方法，这样就可以查看到日志输出了。然后在jenkin job配置中添加 在执行脚本的时候添加BUILD_ID=dontKillMe;就在kill的时候保证jenkins主进程不被杀掉。 如果是centos服务器,用的是yum install java安装的Java环境,查找java_home的方法:用ls -al一直找到软连接的真实指定位置: 记得要给路径或者脚本赋权限,不然jenkins可能出现nohup: failed to run command java: No such file or directory,这是没有权限造成的.]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-基础之string]]></title>
    <url>%2F2017%2F08%2F08%2Fjava-string%2F</url>
    <content type="text"><![CDATA[Java 基础-String12345678910111213141516171819The class String includes methods for examining individual characters of the sequence, for comparing strings, for searching strings, for extracting substrings, and for creating a copy of a string with all characters translated to uppercase or to lowercase. Case mapping is based on the Unicode Standard version specified by the Character class.`String`类包括了一系列字符的方法，比如：比较字符，查找字符，子字符串，创建一个全是大写或者全是小写的字符。匹配是在unicode标准版本规范在`Character` 类的基础上。The Java language provides special support for the string concatenation operator ( + ), and for conversion of other objects to strings. String concatenation is implemented through the StringBuilder(or StringBuffer) class and its append method. String conversions are implemented through the method toString, defined by Object and inherited by all classes in Java. For additional information on string concatenation and conversion, see Gosling, Joy, and Steele, The Java Language Specification.Java语言提供了对字符的操作符`+`做了特殊的支持,并且对其他的objects转换成string也做了相应支持。String的连接是StringBuilder(或者说StringBuffer)来实现的它的append方法的。String转换是通过实现定义在Object(所有的java方法都继承了它)的toString方法。更多的额外的字符操作和转换的特殊操作，看看gosling，joy，steele，写的《java语言规范》。Unless otherwise noted, passing a null argument to a constructor or method in this class will cause a NullPointerException to be thrown.除非有额外的标记，传递一个null参数给一个构造方法或者非构造方法在这个类中，会导致`NullPointerException`被抛出。A String represents a string in the UTF-16 format in which supplementary characters are represented by surrogate pairs (see the section Unicode Character Representations in the Character class for more information). Index values refer to char code units, so a supplementary character uses two positions in a String.一个String呈现的字符串是用UTF-16格式，不够的补位。The String class provides methods for dealing with Unicode code points (i.e., characters), in addition to those for dealing with Unicode code units (i.e., char values).这个String类提供方法处理Unicode代码点和unicode代码单元。 上面是摘抄自String类的介绍，翻译的地方有些不是特别理解。 String 父类String继承自Object,实现Serializable, CharSequence, Comparable重写了Object的toString，equals,hashCode方法;实现了compareTo hashCode方法重写:12345678910111213141516171819202122232425/** * Returns a hash code for this string. The hash code for a * &#123;@code String&#125; object is computed as * &lt;blockquote&gt;&lt;pre&gt; * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] * &lt;/pre&gt;&lt;/blockquote&gt; * using &#123;@code int&#125; arithmetic, where &#123;@code s[i]&#125; is the * &lt;i&gt;i&lt;/i&gt;th character of the string, &#123;@code n&#125; is the length of * the string, and &#123;@code ^&#125; indicates exponentiation. * (The hash value of the empty string is zero.) * * @return a hash code value for this object. */public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 可以看到String重写了hashCode方法，计算规则中n为长度。i为坐标就是index.表明如果是空字符串那么hash值为0。hashCode中是31的幂次方。为什么是31不是其他的数，貌似有人说是2的5次方-1，也有说是可以稍微大点的质数都可以。 重写了equals方法: 123456789101112131415161718192021222324252627282930313233343536/*** Compares this string to the specified object. The result is &#123;@code* true&#125; if and only if the argument is not &#123;@code null&#125; and is a &#123;@code* String&#125; object that represents the same sequence of characters as this* object.** @param anObject* The object to compare this &#123;@code String&#125; against** @return &#123;@code true&#125; if the given object represents a &#123;@code String&#125;* equivalent to this string, &#123;@code false&#125; otherwise** @see #compareTo(String)* @see #equalsIgnoreCase(String)*/public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; equals方法中先看到12String也实现了compareTo: /** Compares two strings lexicographically. The comparison is based on the Unicode value of each character in the strings. The character sequence represented by this {@code String} object is compared lexicographically to the character sequence represented by the argument string. The result is a negative integer if this {@code String} object lexicographically precedes the argument string. The result is a positive integer if this {@code String} object lexicographically follows the argument string. The result is zero if the strings are equal; {@code compareTo} returns {@code 0} exactly when the {@link #equals(Object)} method would return {@code true}. This is the definition of lexicographic ordering. If two strings are different, then either they have different characters at some index that is a valid index for both strings, or their lengths are different, or both. If they have different characters at one or more index positions, let k be the smallest such index; then the string whose character at position k has the smaller value, as determined by using the &lt; operator, lexicographically precedes the other string. In this case, {@code compareTo} returns the difference of the two character values at position {@code k} in the two string – that is, the value: this.charAt(k)-anotherString.charAt(k) If there is no index position at which they differ, then the shorter string lexicographically precedes the longer string. In this case, {@code compareTo} returns the difference of the lengths of the strings – that is, the value: this.length()-anotherString.length() * @param anotherString the {@code String} to be compared. @return the value {@code 0} if the argument string is equal to this string; a value less than {@code 0} if this string is lexicographically less than the string argument; and a value greater than {@code 0} if this string is lexicographically greater than the string argument.*/public int compareTo(String anotherString) {int len1 = value.length;int len2 = anotherString.value.length;int lim = Math.min(len1, len2);char v1[] = value;char v2[] = anotherString.value; int k = 0;while (k &lt; lim) {char c1 = v1[k];char c2 = v2[k];if (c1 != c2) {return c1 - c2;}k++;}return len1 - len2;} 12345678可以看到hashCode，equals，compareTo 中都有提到value？那么value是什么？稍后看。现在回到compareTo,它是Comparable接口中唯一一个方法，约定俗成的方式是如果大返回1，如果等于返回0，如果小于返回-1。实现Comparable接口还有一个好处可以在集合中排序。其中我们讲到`value`，看到String类中value的定义是:`private final char value[];`这说明我们的String类其实底层的实现也是char,这也是为啥String不属于基础类型的一个原因吧,但是语言开发者考虑到String用的比较多所以也做了很多特殊的处理.### final ,可变不可变可以在源码中看到String为定义为final,并且里面的值也是大多定义为final的.定义为final的类是不能被*继承*的.另外定义为final也就意味着值是不能改变的.既然不能改变值,那么String类其实可以说是安全的. 想想看为啥value不是public而是private,其实这是必须的,因为如果是public那么就可以改变里面的值,记住final是不能改变引用指向的对象,但是对象里面的值还是可以改变的: public class StringMe{ public final static char[] aChar = new char[]{‘1’,’2’,’3’};} public class StringTest{ public static void main(String[] args) { System.out.println(StringMe.aChar); StringMe.aChar[0] = 34; System.out.println(StringMe.aChar); //改变了achar[0] //StringMe.aChar = new char[]{&apos;3&apos;};//编译报错,想改变aChar指向的对象 } }1234### String 特殊点常常会见到下面这种情况,面试中常有: // String a = “a”;// String a_new = new String(“a”);//// System.out.println(a == a_new);//false// System.out.println(a.equals(a_new));//true//// String abc = “ab” + “c”;//// String abc_new = new String(“abc”);// System.out.println(abc == abc_new);//fasle// System.out.println(abc.equals(abc_new));//true//// abc_new = “abc”;// System.out.println(abc == abc_new);//true// System.out.println(abc.equals(abc_new));//true// final String ab = “ab”; String abcd = ab + “cd”; String abcd_origin = “abcd”; System.out.println((abcd == abcd_origin)); // true final String abcde = &quot;abcde&quot;; String abcdef = abcde + &quot;f&quot;; String abcdef_origin = new String(&quot;abcdef&quot;); System.out.println(abcdef == abcdef_origin); //false 123![String类反编译图片](/images/encode-java-stringtest.png)反编译的图片 String ab = “ab”;String cd = “cd”;String abcd1 = ab + cd;String abcd2 = “ab” + cd;String abcd3 = ab + “cd”;String abcd5 = “abcd”;String abcd6 = “ab”+”cd”;System.out.println((abcd1 == abcd2)); // falseSystem.out.println((abcd2 == abcd3)); // falseSystem.out.println((abcd3 == abcd5)); // falseSystem.out.println((abcd5 == abcd6)); // trueSystem.out.println((abcd1 == abcd5)); // false 123456789101112131415161718192021222324252627282930313233343536![String类反编译图片2](/images/encode-java-stringtest2.png)图中可以看出其实String底层的相加也是用的StringBuild....未完待续....// 相似点，同类StringBuilder: 如果不考虑多线程的问题,那么我们就是用Stringbuilder其实在String的底层实现中,也是用的StringBuilder来对字符做操作;StringBuffer: StringBuffer的所有方法都是同步的,可以看到源码中都有Synchronize关键字.操作字符串的方法基本类似.// 常用方法indexOf(); 返回第一个匹配的位置,没有匹配返回-1;append();split();split中的&quot;abc&quot;.split(&quot;abc&quot;),会返回&quot;abc&quot;,长度为1; toString();equals();//复写了ObjecthashCode();//复写了Objecttrim();去掉两端空格.....// 是否同步，线程安全String中的方法是不同步的,但是String是一个final类.final类不能被继承,基本类型的包装类基本都是final的,也就是都不能被继承. 另外记住一个. final修饰的String类, 不是说private String a; 那么a也是final类型的.这是错误的. 如果一个String a = &quot;1234&quot;; a=&quot;1233423&quot;; 这个时候a是重新指向了1233424而不是将原来的值改变了.### String.intern()今天看到一个方法，intern() 方法返回的是strings pool ，由String类独自管理。字符池一开始是空的。如果要两个string1.intern()==string2.intern(),仅仅只有当两个的equals（Object）相等才行，调用intern()如果在字符池里面有就返回字符池里面的，如果没有就会加入到字符池中，能够保证的是调用此方法，一定会返回在字符串池中的唯一一个值。#### String.indexOf()总是觉得String应该是Java里面用的最多的，既然String基于char[]数组，那么多String的一些操作是怎样的？看了下indexOf的源码，然后品味了一下,其实暴露给用户使用的只有indexOf(str),indexOf(str,offSet),两个方法，但是看看源码里面indexOf至少有三个，这个不得不让我们想想：为什么有这么多？而这几个indexOf 在里面的实现都是调用了下面的这个函数`indexOf(source,sourceOffset，sourceCount，target，tagetOffset，targetCount)`;其实可以看到就是源字符串，目标字符串，以及一些字符串的属性。这里一开始我觉得用四个参数就可以了，sourceCount貌似完全可以自己算出来。: static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) { if(fromIndex &gt;= sourceCount) { // 起始位置大于等于目标字符的长度 return (targetCount == 0 ? sourceCount : -1); // 目标字符长度是否为0，true，原来的长度，false，-1 } if(fromIndex &lt; 0)// 设置从源字符串哪个位置开始查找 { fromIndex = 0;// 设置边界 } if(targetCount == 0)//如果目标字符长度小于0,长度为空 { return fromIndex; } char first = target[targetOffset];// 第一个查找字符为目标字符偏移量的第一个, 我们使用的时候targetOffset 默认为0 int max = sourceOffset + (sourceCount - targetCount);//源字符偏移量+（源字符串长度-目标字符的长度） 最大位置。保证源字符串不越界。 for(int i = sourceOffset + fromIndex; i &lt;= max; i++) // sourceOffset 我们看到的也是0，默认设值。fromIndex 为从源字符串哪个位置开始 { /* Look for first character. */ if(source[i] != first) { while (++i &lt;= max &amp;&amp; source[i] != first)// 找到一个first字符为值，确定位置 ; } /* Found first character, now look at the rest of v2 */ if(i &lt;= max)// { int j = i + 1; int end = j + targetCount - 1;// 设值找值不超过目标字符串的长度，已经找到第一个位置，所以减1 for(int k = targetOffset + 1; j &lt; end &amp;&amp; source[j] == target[k]; j++, k++) //k为目标字符串偏移targetOffset之后的第二个字符位置（上一步已经找到第一个），确定字符。 // 开始找第二个字符， // 假设这里是找到的，所以j++, k++ , 如果不是source[j]==target[k],那么for跳出，执行外层for ; if(j == end) { /* Found whole string. */ return i - sourceOffset;// 返回的是偏移量之后的位置 } } } return -1; } public static void main(String[] args) { String no = &quot;1234567890&quot;; System.out.println(no.length()); int i = no.indexOf(&quot;&quot;,110); System.out.println(&quot;i=&quot; + i); } ` 其实在indexOf这里打个断点，随便启动一个Java带main方法的程序，会发现在执行main之前都会,indexOf会被多次调用,应该是先加载了jdk里面的lib下的jar文件，用idea调试出来是可以看到路径的。 另外看到indexOf的方法级别上是static int indexOf() 非public，private，而是默认的限定符，附一张限定符的图,accesscontrol.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apec-804-problem]]></title>
    <url>%2F2017%2F08%2F04%2Fapec-804-problem%2F</url>
    <content type="text"><![CDATA[7月25日问题Persistable QModel生成。?A: 其实这是一个QSL框架生成的，所有Entity都会生成一个QEntity类。 8月1日问题maven complier install package 的区别？A:complier只是编译，不会生成jar包，package生成jar包，但是不会将包发布到仓库，只在项目下，install会将jar包发布到仓库让别人也可以用 maven install 跳过测试?A: 执行的时候加上 -Dmaven.test.skip=true jpa 插入枚举值得时候默认为1？A:默认使用 ordinal 值，其实应该为string maven 设置java版本？A: pom文件中加入： 123456789101112&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; springboot 使用jrebel热加载？A: 现在pom文件里面加入devtools: 之后使用ctrl+alt+shift+/ 选中Registry 勾上app.running 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 8月3日问题SimpleDateFormate 日期格式类使用注意？A:日期工具类里面使用了SimpleDataFormate=&gt; format方法里面有个calendar.setTime(date);.如果是多线程中，这个方法没有做同步Thread-A，有一个date，然后Thread-B有一个date，那么造成出现预料之外的结果，当然这种情况出现在共享一个SimpleDateFormate当中，既设置一个全局的simpleDateformat。如果在方法中设置局部变量的simpledateformate，那么局部变量是线程安全的。当然追求效率可以使用ThreadLocal,设置局部缓存 Java 反射获取属性？A:注解是分为执行状态（运行，编译），执行位置（类，方法，属性），如果用declared， 包括所有非父类属性。如果不用declared，获取所有为public的属性getFields()===&gt; 获取所有属性为public的getDeclaredFields()===&gt; 获取所有非继承的数据(public,protect,default….)]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>Question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题总结]]></title>
    <url>%2F2017%2F07%2F24%2Fapec-724-problem%2F</url>
    <content type="text"><![CDATA[Java问题总结7-14 1) idea编码不一致 A: FileEncoding –&gt;globalEncode&amp;&amp;projectEncoding–&gt;Default encoding for properties UTF-8 所有项目尽量一开始就设置UTF-8再导入项目 2) idea vim插件冲突的问题， A:直接设置vim emulation。 将handler全部改为ide 3) redis.clients.jedis.exceptions.JedisConnectionException: no reachable node in cluster A: 没有找到可以用的redis集群 4) idea 查看当前类的信息 A: alt+Q 可以快速查看当前是哪个类 5) idea ctrl+o 重写某个方法 7-15 1) spring定时任务， CRON表达式 */5 * * * * ? A:CRON的格式是固定的，如果需要在spring中开启定时任务，给类加上@Service注入，然后在方法上加入@Scheduled(cron = &quot;*/5 * * * * ?&quot;)就可以了开启一个定时任务了。 2) java list排序自定义 A:如果有需求需要自己给list里面的对象设置排序。可以重写方法。 12345678Collections.sort(pushData, (AppTips o1, AppTips o2) -&gt; &#123; if(o1.getCreateDate().before(o2.getCreateDate())) &#123; return 1; &#125; return 0; &#125;); 3) java传入引用，没有修改值,传入一个list到方法里面，在方法里面修改了list，之后list的值没有改变，引起误解为啥传入对象，修改了对象的值而没有改变对象内容。 123456789101112131415161718192021222324252627282930313233import java.util.ArrayList;import java.util.List;public class ListTest&#123; public static void main(String[] args) &#123; List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add(&quot;adf&quot;); a.add(&quot;werwerwe&quot;); List&lt;String&gt; b = new ArrayList&lt;&gt;(); setB(b); System.out.println(b.isEmpty()); // true setB2(a,b); System.out.println(b.isEmpty()); // false &#125; private static void setB(List&lt;String&gt; b) &#123; b=new ArrayList&lt;&gt;(); b.add(&quot;test1&quot;); b.add(&quot;tet2&quot;); &#125; private static void setB2(List&lt;String&gt; a,List&lt;String&gt; b)&#123; b.addAll(a); &#125;&#125; A: 其实Java除基本类型之外，其他的都为引用传递。这是对的，在上面的例子中，因为在setB中，我们将引用b重新指定到了一个对象new ArrayLsit&lt;&gt;()所以b此时已经不再指向原来的空间了。 4) redis有一个database A: 一开始直接用redis-cli,用keys *发现没有值可以获取，但是代码里面可以获取到值，后来发现其实springboot做了设置redis.database=4。其实在redis客户端里面使用select 4就可以了。 7-21 1)requestParam 参数可以放在body里面 2)nested exception is java.lang.IllegalArgumentException: Not enough variable values available to expand A: 这个异常耽误了我半天的时间，后来发现其实是restTemplate使用的时候url不要拼接json串，因为它遇到{}会认为是一个变量，然后去解析值。详情]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>Question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[klook-607-problem]]></title>
    <url>%2F2017%2F06%2F07%2Fklook-607-problem%2F</url>
    <content type="text"><![CDATA[年度问题日结,将今年每天遇到的问题做一个记录: &lt;月.日 问题&gt; 6.7 问题 Hashmap寻找的时间复杂度 hashmap的实现是”数组+链表”的形式， 12341. 先根据key值计算出hash值以及h值（h值是java实现中处理得到的更优的index索引值）2. 查找table数组中的h位置，得到相应的键值对链表3. 根据key值，遍历键值对链表，找到相应的键值对，4. 从键值对中取出value值。 Golang 反射将数字转成float64 value.(float64) Golang map判断是否有键值 if val,ok:=map[key];ok{ // 有值 } 5.25 问题 数据库无记录时插入，有记录时（设置的主键重复）更新 insert into city_th_tbl (id,name_col) values (8,&#39;名字&#39;) on duplicate key update name_col=&#39;名字1&#39;; ，当id=8插入的时候有重复的值的时候，将name_col更新为名字1，只针对于MySQL。 将docker info之后显示的data space 修改大小truncate -s 200G /var/lib/docker/devicemapper/devicemapper/data 一键停止所有正在运行的docker容器,docker stop $(docker ps -a -q) ,开启：docker start $(docker ps -a -q) grep “查的内容” “文件位置” -C 10 ; 使用grep查找的时候把匹配内容的上下10行也显示出来 5.18 问题 查看linux centos 某个软件是否安装 答案:rpm -qa | grep software 启动某个软件 答案: 设置开机启动sudo systemctl enable docker，设置立即启动sudo systemctl start docker 查看系统内核信息 uname -a ubuntu安装软件出错，“Ubuntu unable to locate package” 答案 `先更新 apt-get update ; 之后 apt-get upgrade` vim 替换 答案：:s/old/new/g,替换一行的所有old为new.:命令模式，s是指本行，g是指所有 查看磁盘大小 答案：df -lh,fdisk -l virtual box 安装Ubuntu，之后使用本地命令行登录。 答案： Ubuntu 装一个openssh-server . vb里面加一个hostonly-adapter. 之后将nat 端口转发 查看当前电脑时区 答案：date 使用nginx ,实现多个tomcat 负载均衡。 原文地址;参考了这个博客，我也实现了。 不过我做了一些小改动。在tomcat上我直接docker pull tomcat镜像，之后根据tomcat镜像生成的容器，来做的实现。没有那么麻烦。nginx 主要在http模块增加了配置: 123456789101112131415server&#123; listen 80; server_name localhost; location / &#123; proxy_pass http://blance; &#125;&#125;upstream blance&#123; server localhost:3280 weight=5; server localhost:3380 weight=5;&#125;include /etc/nginx/conf.d/*.conf;include /etc/nginx/sites-enabled/*; 5.17 问题 怎么给mysql 唯一key 取名 答: index,key在MySQL指代的是同一个东西，不过在key有一个unique key 它会限制多行的该列不会有相同值。 12345678910 -- 建表语句 create table language_tbl( id int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;语言的数字id&apos;, language_code varchar(20) NOT NULL COMMENT &apos;语言显示名:en_US;zh_CN;zh_TW;ko_KR;th_TH&apos;, create_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;创建时间&apos;, last_modify_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP , flag int(1) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;&apos;, PRIMARY KEY (id),unique key `idx_language_code_flag` (language_code,flag)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 可以看到我们自定义的unique key 名为idx_language_code_flag,可以用show index from language_tbl;查看一下表的index。另外如果表已经建立好:create index idx_language_code on language_tbl(language_code);，其中idx_langauge_code是自定义的名字。 删除表 答:drop table if EXISTS language_tbl; 删除表,包括表数据。 `delete from language_tbl;`删除表的所有已有数据。 当访问一个URL时报404，nginx/1.10.2，这种情况查看 nginx 日志判断看是否Nginx访问到了。答: 查看日志，第一步要确定我们要查看的那个日志在那个位置，进入到/etc/nginx目录使用grep，找到log目录。 1234 grep &apos;log&apos; nginx.conferror_log /var/log/nginx/error.log warn;log_format main &apos;$remote_addr - $remote_user [$time_local] $request_time &quot;$request&quot; &apos;access_log /var/log/nginx/access.log main; 5.15 问题 Mysql 使用group by是否可以将多行中同列的不同值合并成一条 答: 主要使用group_concat()函数，可以将同组id相同的值合并到一个值当中 1234567891011121314151617181920SELECT genenic_id, group_concat(LANGUAGE)FROM genenic_language_tblGROUP BY genenic_id;SELECT genenic_id, group_concat(LANGUAGE separator &apos;;&apos;)FROM genenic_language_tblGROUP BY genenic_id; SELECT genenic_id, group_concat(LANGUAGE ORDER BY id DESC)FROM genenic_language_tblGROUP BY genenic_id; SELECT genenic_id, group_concat(DISTINCT LANGUAGE)FROM genenic_language_tblGROUP BY genenic_id; 一条SQL更新两个表 答: update tbl_name1 a, tbl_name2 b set a.xx=&#39;&#39;,b.xx=&#39;&#39;;这样有个不好的地方。这两个表的所有字段都会更新。所以这种情况的使用场景是两个表有唯一主键关联，一定要仔细检查。 123456789 begin;update city_ch_tbl a,city_en_tbl b set a.desc_col = &apos;adsf&apos;,b.desc_col=&apos;testsss2&apos;where a.id = 2 or b.id=21231;rollback;commit;update city_tbl a, city_detail_tbl bset a.displayType_col=1 ,b.publish_status=?, a.publish_time=current_timestampwhere a.id =b.cityId_col and a.id=? redis Key 加密登录 答:如果redis配置了密码，修改了redis.cnf 的requirepass chen； 12src/redis-server redis.confauth password chen 5.13 问题 mysql 替换数据库的某些值 答案: 使用MySQL的replace()函数 update city_en_tbl set desc_col = REPLACE(desc_col, &#39;ne&#39;, &#39;ne1&#39;) where id = 3; mysql 字符相加 答: 使用MySQL的CONCAT(str1, str2, str3, ...) 5.12 问题 mysql 增加唯一键 alter table tbl_name add unique key(col_name) alter table city_en_tbl add unique key(cityId_col) nginx 反向代理 robots.txt是啥？ 谷歌爬虫会根据robots的文件内容确定哪些爬，哪些不爬，可以减少网站的带宽。 Linux 查看软连接 Linux增加软连接为ln -s afile bfile,查看为ls -il nginx site-ennable ## 5.10 问题 Curl 命令 curl [option] [url],平常使用最多的是前后端分离后，我们使用post，get请求某个接口。 12345678910111213-- 头信息curl -H &quot;Token:aadfdwd29b568918db&quot; http://www.baidu.com/test/schedule_rule/autoextend-- 一个比较全面的例子 post请求curl -X POST \http://mywebsite.com/project/url/query \-H &apos;cache-control: no-cache&apos;\-H &apos;content-type: application/json&apos;\-H &apos;token: ce15-fba-ead1-e9d2-2a03041c9&apos;\-d &apos;&#123; “id”: “22,23,76,12&quot;, “language”: “” &#125;&apos;]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>Question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 网络]]></title>
    <url>%2F2017%2F05%2F14%2Fdocker-network%2F</url>
    <content type="text"><![CDATA[有了底层，有了数据，还差一个网络基础配置，那样就完美了 1,宿主机和Docker容器端口对应docker run -ti --name test-network -d -p 50001:8080 chenzhijun/javaweb:1.0 或者用 docker run -ti --name test-network -d -p 50001:8080 chenzhijun/javaweb:1.0 /root/run.sh 创建一个在chenzhijun/javaweb:1.0镜像上的test-network容器，并且将本地的50001端口映射到docker的8080端口。chenzhijun/javaweb:1.0 镜像可以在hub.docker.com上下载，进入到容器后，进入root文件夹下，启动run.sh。然后再本地就可以用localhost:50001端口访问了。 2,映射所有接口地址docker run -ti --name network -d -p 127.0.0.1::8080 chenzhijun/javaweb:1.0这样就是在本地随机分配了一个端口映射到docker的8080。123&gt; docker port network&gt; 8080/tcp -&gt; 127.0.0.1:32768&gt; docker inspect network 3,容器互联实现容器间通信容器的连接系统是除了端口映射外另一种可以与容器中应用进行交互的方式。它会在源和接受容器之间创建一个隧道，接收容器可以看到源容器制定的信息。docker run -d -P --name web --link name:alias images-name,其中name是要链接的容器的名称，alias是这个链接的别名。docker run -it -d --name web2 --link db:mydb centos bash创建一个和db容器有关联,在web2容器/etc/hosts中文件内容为:123456789[root@a1307bd12135 /]# cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.6 mydb d121bb075116 db172.17.0.5 a1307bd12135 使用Docker快速掌握新技术要点并完成适当的技术储备]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 数据管理]]></title>
    <url>%2F2017%2F05%2F13%2Fdocker-data%2F</url>
    <content type="text"><![CDATA[Docker 的数据管理应该核心中的核心。因为不管哪个公司，数据应该永远是第一位。 1,Docker 数据卷简介Docker 的数据卷是一个可供容器使用的特殊目录，它绕过文件系统，提供很多的特性: 数据卷可以在容器之间共享和重用 对数据卷的修改会立马生效 对数据卷的更新，不会影响镜像 卷会一直存在，直到没有容器使用数据卷的使用，类似于Linux下对目录或文件进行mount操作。 2,容器内建数据卷使用docker run命令的时候，使用-v标记可以在容器内创建一个数据卷，多次使用-v标记可以创建多个数据卷。docker run -d -P --name web -v /webdata centos /bin/bash，这条命令成功的创建了一个名为web的容器(-P 是指允许外部访问容器需要暴露的端口)，但是我们用docker exec -ti web bash 或者docker attach web想进入到容器内部时，都会提示我们，容器未启动。之后运行docker start web,这在我们的理解中应该是启动了一个容器，但是但我们再想进入到容器时，发现还是提示我们容器未启动。用docker ps查看后台运行的docker容器，返现确实web没有运行。后来想了一下容器创建的步骤，容器确实创建了。但是我们想进入到容器里面却缺少交互界面。所以用了另一个命令docker run -ti -d -P --name web -v /webdata centos /bin/bash,加上-ti之后再用命令进入到容器就可以了。进入之后会发现在容器中根目录多了一个webdata目录。 2.1,挂载宿主机的一个目录到容器-v 标记也可以挂在一个本地的目录到容器里面作为数据卷。docker run -ti -d -P --name webdata -v /Users/alvin/test/docker/webdata:/share/webdata centos /bin/bash，这条命令是指将本地/Users/alvin/test/docker/webdata目录挂载到webdata容器里面的/share/webdata目录。如果要挂载多个目录，多次用-v标记就可以了。docker run -ti -d -P --name webdata1 -v /Users/alvin/test/docker/webdata:/share/webdata -v /Users/alvin/test/docker/webdata:/share/webdata1 -v /Users/alvin/test/docker/webdata:/share/webdata2 centos /bin/bash 将webdata,webdata1,webdata2都挂载到docker容器里面相应的位置。如果想要对容器的数据卷目录进行权限控制的话，docker也是允许的，默认是rw权限。如果只想要可读权限。docker run -d -P -ti --name webdata -v /Users/alvin/test/docker/webdata:/share/webdata:ro centos bash,目录到容器里面就只有读的权限了。这种方式特别适合挂载单个文件到容器里面。--volumes-from参数所挂载的数据卷的容器自身并不需要保持在运行状态。如果要删除挂载了数据卷的容器，数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时，显示的是使用docker rm -v命令来指定同时删除关联关联的容器。 2.2,利用数据卷容器迁移数据12docker run -ti --volumes-from dbdata --name db4 -v /Users/alvin/test/docker:/backup centos /bin/bashtar cvf /backup/backup.tar /dbdata 首先将dbdata容器内的数据卷和db4容器想关联，之后在宿主机和db4容器做数据卷共享，之后将数据卷容器的数据打包到和宿主机共享的位置。这样就做到了数据的备份了。 2.3,恢复数据 可以使用本地宿主机和docker文件夹共享。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 容器]]></title>
    <url>%2F2017%2F05%2F13%2Fdocker-container%2F</url>
    <content type="text"><![CDATA[Docker 容器容器是 Docker 的另一个核心概念 容器是镜像的一个运行实例，所不同的是，它带有额外的可写文件层 1,创建容器docker create -it centos:latest 新建的容器处于停止状态，可以用 docker start 命令来启动它。 2,新建并启动容器docker run centos /bin/echo &#39;Hello world&#39; ，输出了Hello world,之后就没了,使用 docker ps 看不到容器运行。Docker 后台的操作分解：1234567* 检查本地是否存在指定的镜像，不存在就从共有仓库下载* 利用镜像创建并启动一个容器* 分配一个文件系统，并在只读的及各项层外面挂载一层可读写层* 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去* 从地址池配置一个IP地址给容器* 执行用户指定的应用程序* 执行完毕后容器被终止 docker run -ti ubuntu:latest /bin/bash ，在容器内部运行ps命令，可以看到只有bash应用12-t 让Docker分配一个伪终端并绑定到容器的标准输入上-i 则让让其的标准输入保持打开 用exit退出后，容器也停止运行，处于终止状态。 2.1,守护态运行很多时候需要让Docker容器在后台以守护态形式运行，用户可以通过添加-d参数来实现。docker run -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;,会生成一个id。之后用docker logs id就能看到记录日志。 2.2,重启容器docker restart container，会将一个运行态的容器终止，然后重新启动它。 3,终止容器docker stop [-t|--time[=10]] container，它会先向容器发送SIGTERM信号，等待-t秒后（默认10秒），再发送SIGKILL信号 4,进入容器使用-d参数后，容器进入后台运行。有时候需要计入到容器里面进行操作，可以使用docker attach ,docker exec等命令。 a) attach 进入到容器docker attach 65bde56eaf56-容器id,attach 命令进入到容器里面，不过要首先启动容器，另外当多个窗口同时attach到同一个容器的时候，所有窗口都会同步显示。当某个窗口因命令阻塞时，其它窗口也无法执行操作。 b) exec 进入到容器docker 1.3之后，提供了一个更方便的工具exec，可以直接在容器内运行命令。docker exec -ti 65bde56eaf56[|container-name] /bin/bash[|bash|other-command]可以很方便的进入到容器里面 5,删除容器docker rm [OPTIONS] container-id1[|container-name1] container-id2[|container-name2] ,删除处于停止运行的容器。123* -f, --force=false 强行终止并删除一个运行中的容器* -l, --link=false 删除容器的连接，但保留容器* -v, --volumes=fasle 删除容器挂载的数据卷 6，导入和导出容器a) 导出容器导出容器是指导出一个已经创建的容器到一个文件，不管此时这个容器是否处于运行状态，可以使用docker export 命令dcoker export container.docker export container-id[|container-name] &gt; test_export.tar;可以将导出的文件传输到其它机器上，在其它机器上通过导入命令实现容器的迁移。 b) 导入容器可以将导出的文件又导入，成为镜像cat test_export.tar | docker import - test/ubuntu:v1.0 之前的导入镜像和这个有点类似。实际上，既可以使用docker load 命令来导入镜像存储文件到本地的镜像库，也可以使用docker import 命令来导入一个容器快照到本地镜像库。两者的却别在于容器快照文件将丢弃所有的历史记录和元数据（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新制定标签等元数据信息。docker load是在之前save的时候镜像信息是什么，load之后也是同样的。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 镜像]]></title>
    <url>%2F2017%2F05%2F13%2Fdocker-image%2F</url>
    <content type="text"><![CDATA[镜像是Docker的三大核心概念之一。 Docker运行容器前需要本地存在对应的镜像，Docker会尝试先从默认镜像仓库下载，可以自定义默认仓库位置。 1，获取镜像docker pull name[:TAG],如果TAG没有的话，默认是latest。获取到镜像后，就可以使用镜像创建容器，在其中运行bash应用。 2，查看镜像信息docker images,列出本地主机上已有的镜像。docker tag dl.dockerpull.com:5000/ubuntu:latest ubuntu:latest,给本地镜像添加新标签,新标签的镜像id是一样的。docker inspect image-id,获取image-id的详细信息，json串。如果只想要某个信息的话，可以用docker inspect -f .RootFS 48b5,RootFS是json的某一项内容的详情。48b5,是镜像id。 3，搜寻镜像docker search image-name,搜索远端仓库中共享的镜像。 –automated=false 仅显示自动创建的镜像 –no-trunc=false 输出信息不截断显示 -s, –start=0 指定仅显示评价为指定星级以上的镜像 4，删除镜像使用镜像的标签删除镜像docker rmi image [image...],其中的image可以为标签或ID;docker rmi image:tag,删除某个标签为tag的image;不带tag时默认为latest;如果有容器是以要删除的镜像为基础创建的，那么镜像文件默认是无法被删除的。如果要强行删除一个容器，可以用docker rmi -f image:tag,最好是先将基于镜像的容器删除掉docker rm container-id，之后再去删除镜像。 5，创建镜像创建镜像的方法有三种: 基于已有镜像的容器创建，基于本地模板导入，基于Dockerfile创建。 a)基于已有容器创建docker commit [OPTIONS] container [REPOSITORY[:TAG]]: -a, –author=”” 作者信息。 -m, –message=”” 提交信息。 -p, –pause=true 提交时暂停容器运行。 eg:docker commit -m &quot;added test images&quot; -a &quot;chenzhijun&quot; 7e036 newimage:1.0 以容器id为7e036*的容器为基础创建一个image。image-name为newimage,image-tag为1.0，如果1.0为空，默认为latest。 b)基于本地模板导入本地先下载了一个apache-tomcat-7.0.75.tar.gz,然后使用cat apache-tomcat-7.0.75.tar.gz |docker import - my-tomcat:1.0这样就可以创建一个本地my-tomcat镜像。 6，存出和载入镜像使用docker save 和 docker load命令来存出和载入镜像a)存出镜像docker save -o my-name.tar image[:tag]这样就能将image存出到一个my-name.tar的文件下了 b)载入镜像docker load laod --input my-name.tardcoker load &lt; my-name.tar 7，上传镜像docker push NAME[:TAG]1234docker tag test:latest user/test:latestdocker push user/test:latest-- 可能需要登录]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-encode 编码注意事项]]></title>
    <url>%2F2017%2F05%2F10%2Fjava-encode%2F</url>
    <content type="text"><![CDATA[Tomcat URL 编码配置http://localhost:8080/examples/servlets/servlet/饺子?author=饺子 其中http: 对应scheme，协议 localhost: 对应Domain，主域 8080: port，端口，配置在tomcat examples: ContextPath，配置在tomcat servlet/servlet: ServletPath，在web.xml中中配置 饺子: pathInfo，指到具体的servlet author=饺子: QueryString，传递的参数，如果是post就是表单方式提交 浏览器编码将非ASCII字符编码成16进制数然后再之前加上”%”对URL的URI不分进行解码的字符实在中定义的.默认为ISO-8859-1，QueryString的解码字符要么是Header中的ContentType定义的charset，默认是ISO-8859-1，要使用ContentType中定义的编码，需要将connector的，其中的useBodyEncodingForURI 仅仅是对QueryString进行BodyEncoding解码，不针对整个URI。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Encode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 初步使用]]></title>
    <url>%2F2017%2F05%2F09%2Fvim-start%2F</url>
    <content type="text"><![CDATA[vim使用命令:help :set nui: 插入a: 当前光标后面插入x: 删除当前光标u: 撤销control + r 前进:w 保存:q 退出:wq 保存并退出:q! 强制退出dd 删除一行并且存入剪切板2dd 剪切2行,从当前光标行开始计算p 粘贴o 之后新增一行O 之前新增一行cw 从光标开始到第一个空格全部删除,并在当前位置插入0 数字0,到行头$ 到行末 ^ 到本行第一个不是blank字符的位置(blank指代空格,tab,换行,回车)g_ 到本行最后一个不是blank字符的位置(blank指代空格,tab,换行,回车)fa 查找下一个为a的字符处,fs 下一个为s的字符处..3fa 当前行第3个出现a的位置ta 查找下一个为a的字符的前一个字符处,ts 下一个为s的字符的前一个字符处..F T 和 f t 类似,只是方向想反 dt” 删除所有内容,直到遇到” /patern 文本查找,用n下一个 yy 拷贝3yy 拷贝三行 :e &lt;path/to/file&gt; 打开一个文件:saveas &lt;path/to/file&gt; 另存为:bn :bp 切换打开的文件 . 重复上次的命令N 重复某个命令多少次 eg:2dd 删除2行,剪切也是 3p 粘贴文本三次 10idesu [ESC] 写下10次desu desu desu desu desu desu desu desu desu desu . 重复上一个命令–10”desu” 3. 重复3次&quot;desu&quot;.只会输入3个 N G 到几行 比如34G 跳转到34行gg 到第一行G 到最后一行w 到下一个单词的开头 We 到下一个单词的结尾 E 当前光标在那个字母上,按或者#后,再按一次可以让光标移动了,\是下一个,#是上一个 % 可以匹配(,{,[ 括号的另一半,光标直接移动到另一个位置 &lt;start position&gt;&lt;command&gt;&lt;end position&gt;命令联动 0y&lt;dollar符号&gt; 复制一行 gU 转大写gu 转小写v 视图模式 ,然后按下hjkl 就可以选择了视图模式下如果有,(map (+) (&quot;foo&quot;)).而光标键在第一个 o 的位置vi” → 会选择 foo.va” → 会选择 “foo”.vi) → 会选择 “foo”.va) → 会选择(“foo”).v2i) → 会选择 map (+) (“foo”)v2a) → 会选择 (map (+) (“foo”)) i和a可以代表in all 块操作control + vcontrol + dItest [ESC]&lt; &gt; 缩进J 行缩进 :split 分屏control+w 切换]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[klook-508-problem]]></title>
    <url>%2F2017%2F05%2F08%2Fklook-508-problem%2F</url>
    <content type="text"><![CDATA[5月8日问题MySQL二进制运算:mysql可以直接使用位运算。我们现在是否支持多平台platform使用的就是位运算的方式。 web mobile web ios Android 1 0 1 1 这种设置的好处是以后扩展起来可以加位就可以了。其实我觉得很蛋疼，可读性真的差，但是想想linux的权限控制系统rwx其实感觉也还可以接受。不过要是位数异常多的话~GG吧。这种其实也有好处，平常设计表中可能需要多条记录，或者用另一个表关联，这种位运算就可以直接在一个表里面做选择了。那么如何操作位运算了？ 只要记着，你想要那个平台，比如web，它就是8，因为它指代的是二进制的1000,如果你想找出支持web平台的记录，那么只要用platform&amp;8那么就可以找出所有web位是1的记录。select * from tbl_aa where platform&amp;8 千万别写成select * from tbl_aa where platform=platform&amp;8 这样的结果，是将platform进行了赋值，将位运算的结果赋值给了platform再去做查找筛选。补充一些，位运算还有两个运算符号: 或:| ;异或；^，下面的代码在数据库中会显示成:1select 1^4,4^4,4^12,1|4,1&amp;4 1^4 4^4 4^12 0^0 1l4 1&amp;4 5 0 8 0 5 0 所有的数字对于计算机来说都是0101，那么&amp;它认为的就是相同的位子都是1才是1（可以看成是一条河流有两个闸门，只有两个闸门都打开==1，水才能一直流如江河，不会断流），|它只要当有一个1的时候就认为是对的（相当于一个分流闸门，左边可以走，右边可以走，都可以走，走的通就行），^它是只有当两个不相同的时候才为1，相同的时候为0(这就是叫人道理了，凡事不要想着一刀切走极端，凡事有好有坏才是1。哈哈) MySQL插入的时候max(num)+1遇到了一种情况，就是排序的时候默认是max+1，之前想过一种就是先将max取出来，然后再插入，这种情况就是要先查一次数据库，最近用golang语言做后台开发，没少因为调用多个接口多次访问数据库被前辈们教育。我也知道他们是为我好，所以尝试改动。所以想着能少执行一次SQL就开始写了。 123456insert into tbl_myname (id,name,priority) values (null,?,( select case when max(temp.priority) IS NULL then &apos;1&apos; else max(priority)+1 end from ( SELECT priority,id FROM tbl_myname ) temp )) 123select case when max(temp.priority) IS NULL then &apos;1&apos; else max(priority)+1 end from ( SELECT priority,id FROM tbl_myname ) temp 可以看到这里做了两次查询，难道直接select max(priority)+1不行么？还真的不行。 如果不做临时表直接选择max(priority)+1数据库执行的时候将会报错You can&#39;t specify target table &#39;tblmyname&#39; for update in FROM clause。当然这种插入数据库的方式并不推荐，为啥？因为如果高并发的情况下，我相信，肯定会出现priority相同的情况，如果对priority控制的很严的情况那么是不能出现这种情况的。我能想到的是在进入这个方法的时候加锁，尽量避免并发的情况。 golang枚举操作golang是没有枚举这种说法的。但是它有一个关键字iota 1234567891011121314151617181920func Test_enum(t *testing.T) &#123; fmt.Println(ALL) fmt.Println(th_TH) fmt.Println(ko_KR) fmt.Println(zh_TW) fmt.Println(zh_CN) fmt.Println(en_US) //fmt.Println(ALL|b|c|d)&#125;const ( //英语，中文简体，繁体，韩文，泰文,ALL ALL = iota //0 th_TH //1 ko_KR //2 zh_TW=&quot;test&quot; //3 zh_CN // 值为test， iota为 4 en_US=iota //5) 用了之后发现，跟我想的不一样。之前在Java里面我可以定义一个value，然后用enum.name()获取名字，就相当于一个key-value键值对，而golang里面貌似这样的做法没有。不知道是是什么原因，不过此路不同总有一条其它的路能通的。 golang 判断是否为数字golang里面一开始以为可以直接找一个包或者类来判断传入的字符是否是数字，一开始以为可以用unicode.IsDigit(r rune),后来发现rune其实是一个unit32类型，byte就是一个unit8类型，我的想法仅仅是一个接口然后直接传入string，看来是不行了。所以就想了个注意，自己写。怎么写？第一个想的就是正则： 123456789101112131415func isDigit(str string) bool &#123; if len(str) &gt; 0 &#123; var regexp = regexp.MustCompile(&quot;^\\d+$&quot;) matchResult := regexp.MatchString(str) if matchResult &#123; return true &#125; else &#123; return false &#125; &#125; return false&#125; 简单方便，直接传入string，就能得到想要的值了？以前Java写的顺手，因为很完善，做业务很方便，如今用go，很多东西也不是很熟悉，所以自己动手丰衣足食。 ps:今天还看到了Java Integer源码里面一个sizeTable12final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999, 99999999, 999999999, Integer.MAX_VALUE &#125;; 觉得好可爱的写法。哈哈判断位数倒是可以了嘿嘿。]]></content>
      <categories>
        <category>Question</category>
      </categories>
      <tags>
        <tag>Question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎样用SSH连接VirtualBox]]></title>
    <url>%2F2017%2F04%2F26%2Fhow-to-connect-virtual-box-with-ssh%2F</url>
    <content type="text"><![CDATA[用 iterm2 ssh连接virtual box中运行的centos minimal版本基础准备 安装好虚拟机vb centos官网下载minimal版本，也可以下载dvd版本之后只安装minimal iterm2 安装过程很简单，直接点击vb的new，之后按照提示一步一步点下去就可以了。给个安装教程网址 特殊注意点如果选择host-only adapter的时候出现无法确认，或者无法选择的情况。这种情况下是因为虚拟机本身没有开一个host-only adapter。可以打开vb的系统设置，然后找到网络(network),选择Host-Only Networks。新建一个就可以了 有问题欢迎留言，一起交流学习。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 基础]]></title>
    <url>%2F2017%2F04%2F22%2Fgit%2F</url>
    <content type="text"><![CDATA[Git 基础常用 git 基础命令克隆项目地址:git clone [http://project/address] 查看本地分支/详情:git branch / git branch -v 创建分支:git branch branchName 分支重命名:git branch -m branchNameOld branchNameNew 切换分支:git checkout branchName 创建并切换到分支:git checkout -b branchName 如果需要操作远程仓库的某个分支，可以用git checkout -b branchName origin/branchName ，意思是基于远程仓库的 origin/branchName 分支的基础上创建本地的branchName分支，分支名最好一样，当然也可以不一样。在push的时候使用git push origin HEAD:branchName就可以了。 查看本地repo文件状态:git status 查看文件详细不同:git diff查看历史:git log 添加文件:git add fileName / git add . commit文件:git commit -m &quot;注释信息&quot; 查看远程主机名:git remote -v 更新文件: git pull origin master / git pull,origin为远程名，master为分支名 提交文件: git push orgin master / git push 格式:git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; 查看远程分支:git branch -r 查看所有分支:git branch -a 常用的貌似就这些，但是远远不止这些，还有git init,git rebase 很大一部分命令，这么多我怎么记得住了？可以用给命令设置别名的方法: 使用alias设置系统别名,每次会话结束后会失效. 1234567891011如果要设置`gst`为`git status` 可以这样: 设置别名:`alias gst=&apos;git status&apos;`查看别名:`alias gst`覆盖别名:`alias gst=&apos;覆盖后的命令&apos;`删除别名:`unalias gst`如果需要每次都生效：mac写入到.bash_profile中就可以了 git本身设置 1git config --global alias.st status 下次使用 git st 就可以 github 远程仓库同步有一种情况，如果你fork了别人的仓库到自己的github仓库，然后本地clone了自己github仓库里面的项目，这时候别人的仓库有了更新，而我们肯定也是要同步更新的，这个时候该怎么办？ 解决方法: 1:增加fork的源仓库地址到本地:git remote add remoteName https://github.com/remote/address 2:查看分支信息: 12345git remote -vorigin https://github.com/chenzhijun/java-core-learning-example.git (fetch)origin https://github.com/chenzhijun/java-core-learning-example.git (push)upstream https://github.com/ORIGINAL_OWNER/java-core-learning-example.git (fetch)upstream https://github.com/ORIGINAL_OWNER/java-core-learning-example.git (push) 这里的upstream就是原来的库，orgin指代的就是自己的github仓库了。 3:同步远程库到本地:git fetch upstream 4:保证切换到master分支:git checkout master 5:合并更新到本地:git merge upstream/master 6:推送到自己的github仓库:git push origin master 这样github仓库和远程库是同步更新的了。 – 参考链接 设置远程源 同步fork]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 优化]]></title>
    <url>%2F2017%2F04%2F22%2Fmysql-optimize%2F</url>
    <content type="text"><![CDATA[MySQL 优化基础mysql 版本5.7 查询数据库版本: select @@version; 查询数据库的变量: 12345show variables; -- 当前会话show session variables; -- 当前会话show global variables; -- 全局 当我设置long_query_time的时候用到了set global long_query_time = 5;但是查询的时候用到了show global variables like &#39;%long_query_time%&#39;这种情况下查到的值始终为10,中间的原因就是局部和全局的问题:12345678910111213141516171819202122mysql&gt; SHOW SESSION VARIABLES LIKE &quot;long_query_time&quot;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+mysql&gt; SET @@GLOBAL.long_query_time = 1;mysql&gt; SHOW GLOBAL VARIABLES LIKE &quot;long_query_time&quot;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 1.000000 |+-----------------+----------+mysql&gt; SHOW VARIABLES LIKE &quot;long_query_time&quot;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+ long_query_time记录的是时间的秒。如果设置为0秒，那么所有的sql都会被记录。 show variables like &#39;log_queries_not_using_indexes&#39;; 可以查看是否索引查询启用日志。所有没有索引的查询都会记录下来。 show variables like &#39;slow_query_log_file&#39;; 是否打开慢查询日志set global slow_query_log = on; 打开慢查询日志 show global variables like &#39;slow_query_log_file&#39;; 查询慢查询日志文件路径set slow_query_log_file=&#39;/usr/local/var/mysql/logs/query_slow.log&#39;; 设置慢查询日志文件路径 query_slow.log 12345# Time: 2017-04-22T07:02:51.980026Z# User@Host: root[root] @ localhost [127.0.0.1] Id: 3# Query_time: 0.000179 Lock_time: 0.000074 Rows_sent: 2 Rows_examined: 2SET timestamp=1492844571;select * from store limit 10; 第一行是执行sql的时间；第二行是用户和执行的主机；第三行是sql总共执行时间，锁表，查出的row；第三行是执行时间的时间戳；第四行是执行的SQL语句。 mysqldumpslow -t 3 logfile:分析日志文件的前3条日志 explain sql语句:执行计划分析sql语句]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 使用 XStream 操作 XML]]></title>
    <url>%2F2017%2F04%2F21%2Fjava-xml-xstream%2F</url>
    <content type="text"><![CDATA[Java 使用 XStream 操作 XMLXStream 简介XML 是一种严格的文本格式，我想大家都是知道的。XStream 的作用主要是操作 XML，当然JDK也有自己的方法来实现，但是今天我们用XStream来操作XML。 XStream 基本使用XStream 可以将 Object 序列化成 XML。如果有包结构，那么序列化成xml的时候节点会带上包名。 Student.java 12345678910package com.czj.student;public class Student &#123; private String name; private String major; private int age;&#125; 我们希望生成的xml的文件为: 12345&lt;Student&gt; &lt;name&gt;小王&lt;/name&gt; &lt;major&gt;英语专业&lt;/major&gt; &lt;age&gt;2&lt;/age&gt;&lt;/Student&gt; 如果直接使用XStream来生成xml如下: Main.java 1234567891011121314151617181920package com.czj.student;import com.thoughtworks.xstream.XStream;/** * Created by alvin on 4/21/17. */public class Main &#123; public static void main(String[] args) &#123; Student student = new Student("小王","英语专业",2); Student student2 = new Student("小李","法学专业",3); XStream xStream = new XStream(); String xml = xStream.toXML(student); System.out.println(xml); &#125;&#125; 运行后生成:xml 12345&lt;com.czj.student.Student&gt; &lt;name&gt;小王&lt;/name&gt; &lt;major&gt;英语专业&lt;/major&gt; &lt;age&gt;2&lt;/age&gt;&lt;/com.czj.student.Student&gt; 可以看到如果是带有包名的Object(Studen.java)，那么生成的xml节点就会带上包名。这明显就不是我们所需要的XML。那么如何改变了？可以用到Annotation注解的形式: 123@XStreamAlias("student")public class Student &#123;&#125; 给所要改名的节点的类上加上@XStreamAlias(&quot;student&quot;)注解，貌似可以了？其实加上了注解之后，那么XStream怎么知道要取解析注解了？所以我们要指明一点： 1xStream.processAnnotations(Student.class); 所以原来的代码就变成:Student.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.czj.student;import com.thoughtworks.xstream.annotations.XStreamAlias;/** * Created by alvin on 4/21/17. */@XStreamAlias("student")public class Student &#123; private String name; private String major; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getMajor() &#123; return major; &#125; public void setMajor(String major) &#123; this.major = major; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public Student() &#123; &#125; public Student(String name, String major, int age) &#123; this.name = name; this.major = major; this.age = age; &#125;&#125; 而Main.java:1234567891011121314151617181920package com.czj.student;import com.thoughtworks.xstream.XStream;/** * Created by alvin on 4/21/17. */public class Main &#123; public static void main(String[] args) &#123; Student student = new Student("小王","英语专业",2); XStream xStream = new XStream(); xStream.processAnnotations(Student.class);//开启注解 String xml = xStream.toXML(student); System.out.println(xml); &#125;&#125; 就能生成我们想要的xml结果了: 12345&lt;student&gt; &lt;name&gt;小王&lt;/name&gt; &lt;major&gt;英语专业&lt;/major&gt; &lt;age&gt;2&lt;/age&gt;&lt;/student&gt; XStream 操作命名空间seo中通常要生成站点sitemap.xml.我们尝试去写一个对应的java类:Sitemap.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.czj.sitemap;import com.thoughtworks.xstream.annotations.XStreamAlias;import com.thoughtworks.xstream.annotations.XStreamAsAttribute;import com.thoughtworks.xstream.annotations.XStreamImplicit;import java.util.List;/** * Created by alvin on 4/21/17. */@XStreamAlias(value="urlset")public class Sitemap&#123; @XStreamImplicit private List&lt;Url&gt; url; @XStreamImplicit private List&lt;ImageUrl&gt; imageUrl; @XStreamAsAttribute @XStreamAlias("xmlns") private String xmlns = "http://www.sitemaps.org/schemas/sitemap/0.9"; @XStreamAsAttribute @XStreamAlias("xmlns:image") private String image = "http://www.google.com/schemas/sitemap-image/1.1"; public List&lt;Url&gt; getUrl() &#123; return url; &#125; public void setUrl(List&lt;Url&gt; url) &#123; this.url = url; &#125; public String getXmlns() &#123; return xmlns; &#125; public void setXmlns(String xmlns) &#123; this.xmlns = xmlns; &#125; public List&lt;ImageUrl&gt; getImageUrl() &#123; return imageUrl; &#125; public void setImageUrl(List&lt;ImageUrl&gt; imageUrl) &#123; this.imageUrl = imageUrl; &#125; public String getImage() &#123; return image; &#125; public void setImage(String image) &#123; this.image = image; &#125;&#125; 可以看到命名空间 123456789&lt;?xml version="1.0" encoding="utf-8" ?&gt;&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:image="http://www.google.com/schemas/sitemap-image/1.1"&gt; &lt;url&gt; &lt;loc&gt;loc0&lt;/loc&gt; &lt;changefreq&gt;changefreq0&lt;/changefreq&gt; &lt;lastmod&gt;lastmod0&lt;/lastmod&gt; &lt;priority&gt;priority0&lt;/priority&gt; &lt;/url&gt;&lt;/urlset&gt; XStream 常用注解介绍@XStreamAlias(value=&quot;urlset&quot;): 生成的节点别名 @XStreamImplicit : 忽略节点生成，常用于List a ，a也是一个节点，而实际上a并不需要。 @XStreamAsAttribute @XStreamAlias(&quot;xmlns&quot;) : 属性，属性别名 千万要记得如果采用注解形式，一定要将xStream.processAnnotations(Student.class);打开，否则无法运用注解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>XStream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库常用函数汇总]]></title>
    <url>%2F2017%2F04%2F21%2Fmysql_draft%2F</url>
    <content type="text"><![CDATA[MySQL 常用函数和语句DDL语句修改表的某个字段定义:alter table tableName modify clounmnName column_definition [first|After col_name] 增加表字段:alter table tablename add column column_definition [first|after col_name] eg: 123alter table emp add column age int(3);alter table emp add birth date after ename; 删除表字段:alter table tablename drop column col_name修改字段名:alter table tablename change [column] old_col_name column_definition [First|after col_name] eg: table emp change age age1 int(4)```1234修改表名称`alter table tablename rename [to new tablename]`按照sal排序后查询从第二条记录开始的3条记录`limit`:`select * from emp order by sal limit 1,3.` select concat(‘drop table test1’,table_name,’;’) and table_name like ‘tmp%’`bit类型的数据列用bin(),hex()来显示 mysql 只给表中的第一个timestamp类型设置默认值为系统日期，如果有第二个timestamp类型，则默认为0值。规定timestamp类型字段只能有一列的默认值为current_timestamp;timestamp 和时区有关，插入的时候先转换成本地时区后存放，从数据库取出来时也同样需要将日期转换成本地时区后显示。 查看时区: show variables like &#39;time_zone&#39; 表中的第一个timestamp自动设置为系统时间。如果插入时为null，该列自动设置为当前的日期和时间 函数基础函数LPAD(str,n,pad); RPAD(str,n,pad):用字符串pad对str最左边和最右边进行填充，知道长度为n个长度为止。 LTRIM(str); RTRIM(str):去掉字符串str左侧和右侧空格 REPEAT(str,n):返回str重复n次 REPLACE（str,a,b): 用字符串b替换字符串str中所有出现的字符串 STRCMP(s1,s2):比较s1，s2的ASCII码值得大小 TRIM(str): 去掉str左右两端的空格 SUBSTRING(str,x,y):返回从字符串str中的第x位置起y个字符长度的子串 COUNT(*),COUNT(id): 如果为*，就是返回总行数；如果只是某个列，特别注意如果该列有值为null，那么count不计数 数值函数ABS(x)函数:返回x的绝对值 CEIL(x)函数:返回大于x的最小整数 FLOOR(x)函数:返回小于x的最大整数，和CEIL的用法刚好相反 MOD(x,y):返回x/y的模 RAND()函数:返回0~1内的随机值 ROUND(x,y)函数:返回参数x的四舍五入的有y位小数的值 TRUNCATE(x,y)函数:返回数字x截断为y位小数的结果 日期与时间函数curdate():返回当前日期，只包含年月日 curtime():返回当前时间，只包含时分秒 now():返回当前的日期和时间，年月日时分秒全都包含 UNIX_timestamp(date):返回日期date的UNIX时间戳，Unix_timestamp(now()) FROM_unixtime(unixtime):返回unixtime时间戳的日期值，和unix_timestamp(date)互为逆操作 week(date),year(date):前者返回所给的日期是一年的第几周，后者返回所给的日期是哪一年 hour(time)和minute(time):前者返回所给时间的小时，后者返回所给时间的分钟比如当前时间为23:34,selec hour(curtime()),minute(curtime())==&gt;23 34 monthname(date):返回date的英文名称,四月份的话,select monthname(curdate()) ==&gt; April Date_format(date,fmt) : 将date格式化成fmt格式，select date_format(now(),’%M,%D,%Y’) date_add(date,INTERVAL expr type): 返回与所给日期date相差INTERVAL时间段的日期 datediff(date1,date2):用来计算两个日期之间相差的天数 流程函数IF(value,t,f): select if(salary&gt;2000,’high’,’low’) from salary; 如果value成立，则显示为t，否则为f ifnull（value1，value2) 如果value1为null则显示为value2 case when [value] then [result].. else[default] END : 与if-else差不多 select case when id&lt;5 then ‘err1’ else ‘err&gt;5’ end from city_tbl 其它函数database():返回当前数据库名 version():返回版本 user():返回当前登录用户 inet_aton(IP):返回IP地址的网络字节序表示：select init_aton(‘192.168.1.1’)==》3232235777 inet_ntoa(num):返回网络字节序的IP地址标识：select inet_ntoa(3232235777)==&gt;192.168.1.1 password(str):返回字符串str的加密版本，一个41位长的字符串 MD5(str):返回字符串的str的MD5值，常用来对应用中的数据进行加密 show engines:返回存储引擎列表 show variables like &#39;have%&#39;:返回have为前缀的所有系统变量 alter table *** auto_increment=n; 设置强制自动增长列的初始值，该强制默认值存储在内存中，如果该值在使用之前数据库重启，那么需要在数据库启动后重新设置 对于innoDB表，自动增长列必须是索引 对于text,blob字段的表，如果进场做删除和修改记录，要定时执行optimize table对表进行碎片整理 help ‘show engines’:如果忘记了某个方法，某个关键字，都可以用help来帮助]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用 Linux 命令]]></title>
    <url>%2F2017%2F04%2F18%2Flinux-java-command%2F</url>
    <content type="text"><![CDATA[常用 Linux 命令1.查找文件1234567891011121314151617find / -name filename.txt 根据名称查找/目录下的filename.txt文件。find . -name &quot;*.xml&quot; 递归查找所有的xml文件find . -name &quot;*.xml&quot; |xargs grep &quot;hello world&quot; 递归查找所有文件内容中包含hello world的xml文件grep -H &apos;spring&apos; *.xml 查找所以有的包含spring的xml文件find ./ -size 0 | xargs rm -f &amp; 删除文件大小为零的文件ls -l | grep &apos;.jar&apos; 查找当前目录中的所有jar文件grep &apos;test&apos; d* 显示所有以d开头的文件中包含test的行。grep &apos;test&apos; aa bb cc 显示在aa，bb，cc文件中匹配test的行。grep &apos;[a-z]\&#123;5\&#125;&apos; aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。 2.查看一个程序是否运行123ps –ef|grep tomcat 查看所有有关tomcat的进程ps -ef|grep --color java 高亮要查询的关键字 3.终止线程1kill -9 pid 4.查看端口被占用的程序pid1lsof -i:8080 8080 为端口号 5.复制文件1234cp src tagetcp -r src targetscp srcFile remoteUserName@remoteIp:remoteFileAddress 将本地目录拷贝到远程目录 ； 加上-r参数 为文件夹拷贝scp remoteUserName@remoteIp:/path/filename /local/local_destination 将远程目录拷贝到本地目录 ；加上-r 为文件夹拷贝 6.创建和删除目录1rmdir/mkdir 7.查看结尾1000行1tail -1000f filename 查询动态日志文件的时候 8.查看端口占用情况1netstat -tln | grep 8080 查看8080的使用情况 9.端口属于哪个程序1lsof -i:8080 10.查看进程12ps aux|grep java 查看java进程ps aux 查看所有进程 11.文件下载12wget http://file.tgzcurl http://file.tgz 12.远程登录1ssh userName@ip 13.打印信息1echo $JAVA_HOME]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的 Linux 基础命令]]></title>
    <url>%2F2017%2F04%2F18%2Flinux-command-ls%2F</url>
    <content type="text"><![CDATA[常用 Linux 命令man: 查询帮助 12man infoman ls ls: 列出目录 12ls 目录1 目录2 -&gt; 同时列出目录1，2的文件夹下的文件ls -R 目录名 -&gt; 递归列出目录下的所有目录和文件 find: 查找文件 1234find 目录 -name 文件名 find ./blog/chenzhijun.github.com -name &quot;*.md&quot; | xargs grep &quot;Life&quot;| xargs 传递命令查找出./blog/chenzhijun.github.com目录下所有md 为结尾的文件，并且在在文件中查找内容包含 Life 字符串的内容 grep: 根据条件查找文件内容，配合find找更牛逼 1grep PATTERN filename -&gt; 在文件filename中找到pattern pwd: 当前工作目录的全路径 rm: 移除文件，主要参数-rf ，强制递归删除 mv: 移动文件 cat: 查看全部文件内容 more: 查看文件内容，逐行显示。 1more +100 filename less: 查看文件内容，上下滚动查看 1less +100 filename vim/vi:编辑文件，如果不存在就创建 ping: 测试网络连通 1ping www.baidu.com tar: 文件解压缩 123456789101112131415-c 创建归档-x 解压归档-v 显示处理过程-f 目标文件，后面必须跟目标文件-j 调用bzip2 进行解压缩-z 调用gzip 进行解压缩-t 列出归档中的文件 tar -cvf filename.tar . ### 将当前目录所有文件归档，但不压缩，注意后面有个’.‘ ，不可省略，代表当前目录的意思 tar -xvf filename.tar ### 解压 filename.tar 到当前文件夹tar -cvjf filename.tar.bz2 . ### 使用 bzip2 压缩tar -xvjf filename.tar.bz2 ### 解压 filename.tar.bz2 到当前文件夹tar -cvzf filename.tar.gz ### 使用 gzip 压缩tar -xvzf filename.tar.gz ### 解压 filename.tar.gz 到当前文件夹tar -tf filename ### 只查看 filename 归档中的文件，不解压 ln: 两个文件中创建链接，硬链接，软链接 123ln source dest ### 为 source 创建一个名为 dest 的硬链接ln -s source dest ### 为 source 创建一个名为 dest 的软链接 chmod: 改变文件权限，读，写，执行； 123456789101112131415ls -al 可以查看文件的详情，其中 所有者 、 用户组 、 其他都占3个-rwxr--r-- 1 locez users 154 Aug 30 18:09 filenamer=read,w=write,x=execute chmod +x filename ### 为 user ，group ，others 添加执行权限 chmod -x filename ### 取消 user ， group ，others 的执行权限 chmod +w filename ### 为 user 添加写入权限 chmod ugo=rwx filename ### 设置 user ，group ，others 具有 读取、写入、执行权限 chmod ug=rw filename ### 设置 user ，group 添加 读取、写入权限 chmod ugo=--- filename ### 取消所有权限 rwx对应111，如果赋予rwx三个权限，就是7，如果给所有者，用户组，其它都赋予rwx的权限，那么就是777 wget: 下载工具 12wget -O newname.md https://github.com/LCTT/TranslateProject/blob/master/README.md ### 下载 README 文件并重命名为 newname.mdwget -c url ### 下载 url 并开启断点续传]]></content>
      <categories>
        <category>Liux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 语言简单基础介绍]]></title>
    <url>%2F2017%2F04%2F12%2Fgo-language-start%2F</url>
    <content type="text"><![CDATA[Go简单介绍GoGo的出名，估计真的是靠“爹”。反正人家有个好爹，这是毋庸置疑的。有时候我会想，为啥要用GO，其实Go能做的事情，Java也能做，到现在我也没有想明白。可能Go的存在就是为了那些C/C++想快速转到互联网行业的一个中转语言吧。说实话，对于指导Go里面有指针的第一时间。心中就是千万匹草泥马。但是草归草，生活的继续，还是得硬着头皮学。我有一个心理目标，既然学，就要分清楚学习到什么程度，以致于来花费多少时间。没错，对于Go，我的想法是能用，并且能稍微的用好一点。 Hello Go一切语言的开始都是 Hello，World 1234567package mainimport &quot;fmt&quot;func main()&#123; fmt.Println(&quot;Hello,World&quot;)&#125; 上面就是 main.go 文件。go 语言的文件后缀都为 .go。这个基本上每个语言都有这个特性啊。哈哈。 第一行：go 语言包管理机制，所有开始必须有个包名“自报身份”。第二行：import 导入需要用到包第三行：func main(){} 定义一个 main 函数。func 定义一个函数，main函数是可以运行的入口 Go 语法 定义一个 variable 的string变量 123var variable stringvar variable = &quot;string&quot;variable := &quot;string&quot; 大小写，变量可见性 12var Variable string //外部可见var variable string //外部不可见 函数定义 1234567891011func Amethod(a int, b string)(int,error)&#123;//外部可见，传入a，b两个参数，返回int，error两个类型的参数 //do something return v1,err1&#125; func amethod(a int, b string)(int,error)&#123;//外部不可见，传入a，b两个参数，返回int，error两个类型的参数 //do something return v1,err1&#125; intResult,err := Amethod(1,&quot;str&quot;) // 调用Amethod方法，并且定义intResult，err来作为接受返回值的两个变量 流程控制 12345678910111213141516171819if a&gt;0&#123; //do thing&#125;for i := 0;i &lt; 10; i++ &#123; // 常见for循环 //do thing&#125; for 1==1&#123; // 无限死循环&#125;var aSlice := []int&#123;1,2,3,4&#125;for idx,val := range aSlice&#123;// 循环slice数组，range关键字 fmt.Print(idx,val);&#125;switchcontinuebreakgo 开启协程 其实一开始写Go我是拒绝的，有几次都感觉，Java用起来多爽啊，Go这东西简直反人类。最近用着基本上也就没有抱怨了，很多时候都是怕自己做不好，还有自己的舒适区。其实刚毕业的一两年都是一个软件开发者最重要的时间吧。我16年毕业，半年Java开发后跳槽，说实话，如果不是女朋友来深圳的话，我可能真的会在易宝支付做下去吧。来深圳找工作并不是很顺利，很多时候自己心理压力大，也有一点事自己学的不足。一方面还没有做满一年就跳槽，怕留给公司不好的影响，另一方面，半年的时间也就了解了一些基本的支付业务，并没有实际的开发到少东西。而来到新公司，第一件事情就是做一个自己不熟悉的东西。那个时候真的是一边做，也有一遍流泪的。越想越心酸，能力不足，眼前又痛苦。想想别人可以挑公司，而我还被公司挑来挑去。而现在的公司又每天都是做着我不熟悉的Go开发。那个时候心里憋屈啊。 在同事们看来，我从进公司第一天到完成Go开发组长的任务总共用了三天的时间，并且组长也是相当满意任务的完成度。只是他们不知道，我心里是多苦。从熟悉的Java转到Go，我真的是心里一万遍打死了自己，责怪自己。我的同事对我很好，给了我很大的帮助。我真的非常谢谢他们。完成任务后的第一天，我就开始做实际的生产上的项目了。那段时间什么都不知道，什么都不懂。我有时候真的都会默默流泪。但是不管怎样，如果你没能力，那么你只能好好的做好你手中的事情，硬着头皮，最终也是做完了。我很讨厌这种不能自己决定选择的生活，而我不管怎样，也不能放弃。chenzhijun，加油，每天努力一点点。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简单入门-搭建 JavaWeb Docker 运行环境]]></title>
    <url>%2F2017%2F04%2F12%2Fdocker-start%2F</url>
    <content type="text"><![CDATA[Docker 入门 容器技术已经越来越火爆，作为攻城狮有必要了解一下Docker 什么是DockerDocker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。正如 Docker 的 Logo 所示。一条大鲸鱼载着各种集装箱。大鲸鱼就可以看成宿主机，集装箱就是在宿主机上的容器，容器之间是相互隔离的。 怎么用DockerDocker 最重要的的是 image ，也就是镜像。先有镜像，再建立容器。 Docker 的使用方式是很简单的，暂时我也只是刚刚学会一些简单的使用方式。暂时先介绍 Docker 几个命令的使用方式： docker –version 查看当前安装的 Docker 的版本 docker –info 查看 Docker 的信息 docker images 查看所有的镜像 docker ps -a 查看所有的容器 docker ps 查看正在运行的容器 docker rm 删除容器 docker rmi 删除镜像 多加一个i，rmi。 docker stop 停止运行的容器 docker start 启动一个容器 docker run -ti [–name container-name] -v [宿主机地址]:[容器地址] &lt;镜像名&gt; /bin/bash eg:docker run -ti –name web -v /Users/alvin/Devtools/docker/:/mnt/software/ centos /bin/bash 这条命令的意思在 centos 镜像上建立一个名为 web 容器。-v 的意思是在宿主机的/Users/alvin/Devtools/docker/ 挂载在 web 容器的/mnt/software/ 位置。 docker commit &lt;容器ID&gt; chenzhijun/javaweb:1.0 保存容器为一个镜像 怎么创建镜像一切从理论都是屁话，理论结合实践才是王道。 下面建立一个Javaweb运行环境的容器，生成image上传到hub.docker.com。 1:首先要下载Centos 镜像 pull centos```1234562:在Tomcat官网上下载Tomcat linux 版本；在 Oracle 官网下载 JDK 。解压后将tomcat和jdk存放在本地宿主机 [自定义目录:/User/alvin/Devtools/docker/] 3:在centos上建立一个新容器 ```docker run -ti --name chenzhijun -v /Users/alvin/Devtools/docker/:/mnt/software/ centos /bin/bash 4:进入容器里面，将software下的tomcat和jdk移动到/opt/下面 1234Vim ~/.bashrc # 在.bashrc 最后面加入下面两行export JAVA_HOME=/opt/jdkexport PATH=$PATH:$JAVA_HOME 5:进入容器后创建脚本 12345vi /root/run.sh#!/bin/bashsource ~/.bashrcsh /opt/tomcat/bin/catalina.sh run 6:修改脚本文件权限 1chmod u+x /root/run.sh 7:保存容器到镜像 1234567891:查看容器的iddocker ps -a2:保存容器为镜像docker commit (容器id) chenzhijun/javaweb:0.13:启动镜像docker run -d -p 58080:8080 --name javaweb chenzhijun/javaweb:0.1 /root/run.sh启动一个容器，并且容器运行的时候运行脚本，绑定宿主机的端口58080到容器的端口8080，运行的容器名字为javaweb 8:上传镜像到hub.docker.com,首先保证在hub上有一个相同名字的镜像地址，跟git类似。 12345docker loginusername:password:docker push chenzhijun/javaweb 好了，暂时照着这个操作就基本上可以了。 如果遇到问题可以给我留言，也可以联系我的邮箱：vbookchen@gmail.com]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻转链表]]></title>
    <url>%2F2017%2F04%2F06%2Freverse-node-md%2F</url>
    <content type="text"><![CDATA[翻转链表 比如链表1-2-3-4-5 翻转为 链表5-4-3-2-1 格式 构建基础链表，链表的数据结构比较简单，就是一个数据项，一个下一个节点。我在这里其实遇到过一个理解问题，总是把node当成了链表。其实一个node就是一个小块。不是那一条链，只不过它的中间有下一个node，这样一层一层组成了一个链表。所以记得一个链表中，node指的是一个节点，而不是一个链。能这样理解，那么翻转链表也不是什么问题了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package node;/** * Created by alvin on 4/2/17. */public class Node &#123; private int data; private Node nextNode; public int getData() &#123; return data; &#125; public void setData(int data) &#123; this.data = data; &#125; public Node getNextNode() &#123; return nextNode; &#125; public void setNextNode(Node nextNode) &#123; this.nextNode = nextNode; &#125; public Node(int data) &#123; this.data = data; &#125; public static Node initNode(int num) &#123; Node node = new Node(0); Node temp = node; for (int i = 1; i &lt; num; i++) &#123; Node nextNode = new Node(i); temp.setNextNode(nextNode); temp = nextNode; &#125; return node; &#125; public static void main(String[] args) &#123; Node node = initNode(5); out(node,"init:"); &#125; public static void out(Node node,String prompt)&#123; System.out.print(prompt); while (null != node) &#123; System.out.print(node.getData()+","); node = node.getNextNode(); &#125; &#125;&#125; 之后我们写测试代码：123456789101112131415161718192021222324252627package node;/** * Created by alvin on 4/2/17. */public class MainClass &#123; public static void main(String[] args) &#123; Node node = Node.initNode(4); Node.out(node,"遍历init:"); System.out.println("\n=========="); Node reversNode = reverseNode(node); Node.out(reversNode,"遍历翻转:"); System.out.println("\n=========="); Node node1 = Node.initNode(4); Node.out(node1,"递归init:"); Node reversNode2 = reverseNode1(node1); System.out.println("\n=========="); Node.out(reversNode2,"递归翻转:"); &#125;&#125; 遍历反转像图片中所示的一样，第一次在上面，我们定义pre，cur，next三个指示量，链表的好处在于，只要将链表中的节点指向另一个地方就完成了对这个节点链表的操作。每一次我们先将第一个节点保存，将第二个节点的nextNode 指向第一个节点。依次类推，得到的最终node链表就是我们翻转后的了。12345678910111213141516//遍历翻转private static Node reverseNode(Node node) &#123; Node pre = node; Node cur = node.getNextNode(); Node next = null; pre.setNextNode(null); while (null != cur) &#123; next = cur.getNextNode(); cur.setNextNode(pre); pre = cur; cur = next; &#125; node = pre; return node;&#125; 递归翻转递归的程序必须有一个终止递归的条件。 递归的思路是递归链表到最后那个节点，然后一层一层的将nextNode 至为上一个node，而上一个node设置为null，这样可以防止在递归回来时，第一个节点和第二个节点造成节点死循环。 1234567891011121314//递归翻转 private static Node reverseNode1(Node node) &#123; if (null == node || null == node.getNextNode()) &#123; return node; &#125; Node reHead = reverseNode1(node.getNextNode()); node.getNextNode().setNextNode(node); node.setNextNode(null); return reHead; &#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 虚拟机（一）]]></title>
    <url>%2F2017%2F04%2F05%2Fjava-virtual-machine%2F</url>
    <content type="text"><![CDATA[Java 虚拟机Java 运行时数据区Java 虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。不同的区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有的则随着线程的启动和结束而建立和销毁。数据区分为5大区域：程序计数器，Java虚拟机栈，本地方法区，Java堆，方法区。 程序计数器内存中较小的一块空间，可以当做是当前线程所执行的字节码的行号指示器。简单点其实应该算是空着着当前线程的下一步操作。Java虚拟机的多线程其实是线程轮流分配执行时间的方式来实现的，意思就是一个处理器在任何一个确定的时间，只会执行一条线程的指令。想想如果线程没执行完，cpu的时间没了，另一个线程占去了cpu，那么下次cpu如何知道当前线程执行到哪里了呢？要解决这个问题，是不是的有个区域必须的保持独立性，不能被非当前线程干扰？所以把程序计数器设置成线程私有，这样就完美解决这个问题了。如果线程执行的是Java方法，计数器就是记录的正在执行的虚拟机字节码指令地址。如果正在执行的是native方法，计数器的值就为空(undefined)。另外非常要注意的点就是程序计数器是唯一一个没有规定任何OutOfMemoryError。 Java 虚拟机栈虚拟机栈也是线程私有的，它的生命周期与线程相同。都跟线程同生共死了，当然虚拟机栈也是线程私有的。虚拟机栈描叙的是java方法执行的内存模型：每个方法在执行的时候都会创建一个栈帧，栈帧主要用来存储局部变量表、操作数栈、动态链接、方法出口的信息。每一个方法从调用到完成，对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。经常说方法中的局部变量是线程安全的，从这里可以得到解释，局部变量定义在虚拟机栈中，那么它是属于线程私有的，就不会存在多线程中共享变量的情况，所以说局部变量在多线程环境下是线程安全的。栈的局部变量表存放了编译期（注意是编译，不是编辑）可知的各种基本数据类型（boolean,byte,char,short,int,float,long,double)、对象引用和returnAddress类型。局部变量表的最小单位为局部变量空间（slot）。其中64位长度的long和double类型的数据会占用2个局部变量空间。局部变量表所需的内存空间在编译期间就完成分配，进入一个方法需要在栈中分配多大的局部变量空间是完全确定的，运行期间不会改变局部变量表的大小。Java虚拟机规范中，如果这个区域线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverFlowError异常；如果虚拟机栈可以动态扩展，在扩展期间无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈本地方法栈与虚拟机栈所发挥的作用比较类型，区别是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务。很多Java的源代码中就有这种native方法。此区域也会抛出StackOverFlowError 和 OutOfMemoryError。 Java 堆Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。Java堆得唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。Java堆是垃圾收集器管理的重要区域，因此也被称为GC堆（Garbage Collected Heap）堆中还细分为新生代和老年代。Java堆可以处于物理不连续的内存空间中，只要逻辑上是连续的就可以。如果堆中内存完成实例分配，并且堆也无法扩展的时候，将会抛出OutOfMemoryError异常。 方法区方法区与Java堆一样，是各个线程共享的内存区域，它用于内存已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。该区域除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。按照这一块的存在目的，主要是存储加载的类型信息，常量等信息。这块的回收主要是类型的写在和常量池的回收，回收的效益并不一定会很理想。当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构-队列]]></title>
    <url>%2F2017%2F03%2F23%2Falgorithm-queue%2F</url>
    <content type="text"><![CDATA[数据结构-队列 程序的开发越来越发现，重要的其实是一些基础的东西。程序=数据结构+算法；这个等式真的越来越让人有感悟。 今天遇到一个问题，有个妹子告诉了你一串加密后的qq号，解密的规则是第一个数删除第二个数排最后，第三个数删除，第四个数排最后，依次类推到没有一个数可以删除，按照删除的数的顺序就是解密后的qq号码。请问，如果你对妹子有想法的情况下，你该如何解决这个问题了？这个问题的解决方式可以用到队列，队列是一个先进先出的数据结构。实现队列的方式有很多，在Java里面可以用数组，或者用list，这种线性结构存储。下面看Java代码实现 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.ArrayList;import java.util.Arrays;/** * 解密qq号 * 首先将第1个数删除，紧接着将第2个数放到这串数的末尾， * 再将第3个数删除并将第4个数再放到这串数的末尾， * 再将第5个数删除……直到剩下最后一个数，将最后一个数也删除。 * 按照刚才删除的顺序，把这些删除的数连在一起就是QQ号了 * &lt;p&gt; * Created by alvin on 3/23/17. */public class Queue &#123; public static void main(String[] args) &#123; Integer[] qqNumber = new Integer[]&#123;6, 3, 1, 7, 5, 8, 9, 2, 4&#125;; //6 1 5 94 7 2 8 3 ArrayList&lt;Integer&gt; queue = new ArrayList&lt;Integer&gt;(); for (Integer i : qqNumber) &#123; queue.add(i); &#125; int size = queue.size(); System.out.println("加密后qq:" + Arrays.toString(queue.toArray())); String qq = ""; for (int i = 0; i &lt; size; i++) &#123; if (i % 2 == 0) &#123; qq += queue.get(i); continue; &#125; queue.add(queue.get(i)); size++; &#125; System.out.println("解密后qq:" + qq); &#125;&#125; 代码实现的不是很优雅，但是应该算是很明了的。ArrayList就是我们要用来做破译的队列。每经过一个数，确认是删除，还是往后面继续排队。在第一个位子设置标记，每往list中增加一个元素，size就会+1，但是标记会一直移动，所以移动到两者相等的时候，即达到终点，这个时候我们就能得到正确的qq号了。 ps: ArrayList的源码中，实现的方式是一个空的数组{},默认大小为10,ArrayList中有一个ensureCapacity方法来保证容量得到保证。在这个机制下很多人都习惯不设默认值，其实如果你的数据量是均衡的，既你能大概平均预计到最大值的情况下，最好是初始化list的容量，ensureCapacity 这种机制是通过直接将原list拷贝到新list，容量变为(原大小+原大小*2)，然后让引用指向新的list，而之前的旧list就没有用只能等待gc回收。想想看如果你平凡的去这种扩容(想想都浪费啊，程序员都是很抠的，尤其是压榨性能方面~~~)]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 基础数据类型]]></title>
    <url>%2F2017%2F03%2F21%2Fjava-basic-type%2F</url>
    <content type="text"><![CDATA[Java 基础数据类型 Java 拥有着八大基本数据类型：byte,short,int,long,float,double,boolean,char byte,Byte了解byte之前我们先了解一下 “字节，位”。字节的单位是Byte，位的单位是bit。一个字节(Byte)=8个位(bit)。在Java里面byte数据类型占用8位，它是带符号位的，怎么明白符号位的意思？一个位代表一个1或者一个0。8位带符号的意思是第一个位表明为符号位，如：0111111，符号位为0代表+，即最大值为127；如果是 11111111,最小值为-128。byte的包装类型为Byte,初始值为0。 short,Shortshort 在Java里面占用16位，也是带符号位的。最小值为-32768最大值为32767。short的包装类为Short，初始值为0。 int,Integer默认情况下占用32位带符号位。从$-2^(31)$-$2^(31)-1$。在官网下得知，在Java8以及之后的版本中，也支持无符号32位int类型，数值从$0$-$2^(32)-1$。int的包装类型为Integer，初始值为0。 long,Long长整形数据类型占用64位带符号位。从$-2^(63)$-$2^(63)-1$。使用长整形的主要原因是在int不满足需求的情况下使用。定义long类型的值时在数字后加L: long aLong = 1234L。Java8之后，长整形也支持无符号位64位，$0$-$2^(64)-1$。长整形的包装类型为Long，初始值为0L。 float,Floatfloat是单精度32位浮点数，值得范围还有待讨论Floating-Point Types, Formats, and Values，需要注意，浮点数不是精确的值，所以在有需要精确控制的地方一定要注意(货币，价钱方面)，可以使用BigDecimal来代替浮点数运算。定义float的时候一定要加f，不然系统会默认为double。比如 float aFloat = 0.1f。flocat的包装类型为Float，初始值为0.0f。 double,Doubledouble是双精度64位浮点数，遵从IEEE754。double也不是精确的，所以和float需要注意点基本类似。double的包装类型为Double，初始值为0.0d。 boolean,Booleanboolean数据类型仅仅有两个可能值，true，false。它在某些情况下占用一位，但是它的占用大小是一个不确定的。boolean的包装类型为Boolean，初始值为false。 charchar 的数据类型是一个16位的Unicode字符类型，最小值\u0000(0)-最大值\uffff(65535)，初始值为\u0000 可以总结一下，基本数据类型的包装类型都是首先字母大写，包装类型的主要作用是面向对象，增加更多的可操作性。基本数据类型作形参的时候是值传递，不会改变原来的值。特别注意String不是基本数据类型，只是String的对象都存储在静态区，是一个不可变对象,String 默认值为“null”。void 和 Void 也需要注意，在博客上有人把他们归类为第九大数据类型，但是官网却没有说明。参考Primitive Data Types]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法-快速排序]]></title>
    <url>%2F2017%2F03%2F16%2Falgorithm-quick-sort%2F</url>
    <content type="text"><![CDATA[快速排序介绍快速排序，听名字就能感觉到主要讲究的就是快速。它其实主要采用分治法，选取基准元，然后把小的放左边，大的放右边，遍历一遍之后就能确定基准元的位置。之后以确定后的基准元位置做左右分割，就是分治了。不说啥，直接上图： 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Created by chenzhijun on 3/16/17. */public class QuickSort &#123; public static void sort(int[] a, int i, int j) &#123; if (i &gt; j) &#123; // 递归程序要有终止条件 return; &#125; int basic = a[i]; //设置基准元 int start = i; // 左右位置 int end = j; while (start != end) &#123; while (a[end] &gt;= basic &amp;&amp; end &gt; start) &#123; // 必须先从右边先开始，即基数对面 end--; &#125; while (basic &gt;= a[start] &amp;&amp; start &lt; end) &#123; start++; &#125; if (start &lt; end) &#123; //交换位置 int temp = a[start]; a[start] = a[end]; a[end] = temp; &#125; &#125; a[i] = a[start]; // 确定基准元的位置 a[start] = basic; sort(a, i, start - 1); // 分割基准元左右两边，找出下一个排序组。 java 非基本类型传递，都是传递的引用传递。地址。 sort(a, end + 1, j); &#125; public static void main(String[] args) &#123; int[] a = new int[]&#123;1, 8, 2, 6, 5, 7, 2, 9&#125;; sort(a, 0, a.length - 1); for (int i : a) &#123; System.out.print(i + ","); &#125; &#125;&#125; 我们在程序的中设置的哨兵就是start和end，这里一定要注意，如果你的基准元是选取的第一个位置或者最后一个位置，那么循坏时候一定的是对面开始。 算法分析快速排序的空间占用是$O(n)$,最坏情况下时间复杂度为$O(n^2)$,平均情况下是$O(nlogn)$。最坏情况下是怎么出来的呢？当每次分完之后一边是1个元素，一边是n-1个元素，这种情况下，时间复杂度就是最坏的了。至于$O(nlogn)$,这个可以计算出来的，也可以看这篇链接：快速排序算法的时间复杂度分析]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法-冒泡排序]]></title>
    <url>%2F2017%2F03%2F16%2Falgorithm-bubble%2F</url>
    <content type="text"><![CDATA[冒泡排序排序原理冒泡排序是将最大或者最小的值先挑出来，然后依次类推，最终使得整个数组有序。比如我们在学校操场排队的时候，老师先叫你站好，站好之后，根据你们的身高找到一个最矮的，排到第一个，然后是第二个，然后这样依次类推…到最后就是一个身高顺序的队伍了。 代码实现12345678910111213141516171819202122public class Main &#123; public static void main(String[] args) &#123; int[] a = new int[]&#123;12, 35, 99, 18, 76&#125;; for (int i = 0; i &lt; a.length; i++) &#123; for (int j = 0; j &lt; a.length - i - 1; j++) &#123; // 如果已经冒泡出来了i个数，则已经有i个数排好序，就无需再比较了 if (a[j] &lt; a[j + 1]) &#123;//相邻两个位置交换 int t = a[j]; a[j] = a[j + 1]; a[j + 1] = t; &#125; &#125; &#125; for (int i : a) &#123; System.out.print(i + ","); &#125; &#125;&#125; 这个是冒泡排序的代码，第一个for循环，指的是有多少个数需要进行排序；第二个for循环开始交换位置，将小(大)的数一个一个交换到结尾的位置。 123456789101112131415161718192021public class Main &#123; public static void main(String[] args) &#123; int[] a = new int[]&#123;1, 5, 4, 5, 2, 32, 2&#125;; for (int i = 0; i &lt; a.length; i++) &#123; // 先确定位置 for (int j = 0; j &lt; a.length; j++) &#123; //确定位置的大小 if (a[i] &lt; a[j]) &#123; // 找到第i个位置，然后再第i个位置找到最优值 int t = a[i]; a[i] = a[j]; a[j] = t; &#125; &#125; &#125; for (int i : a) &#123; System.out.print(i + ","); &#125; &#125;&#125; 这个代码是我自己写的，也是可以排序，只是这里将a[i]也用作交换基数（上面代码，a[i]没有参与运算），我这个算法貌似也算冒泡吧？ 算法分析冒泡代码实现里面可以看到有两个for循环，时间复杂度为$O(N^2)$，空间复杂度为N。这个意思是什么？假设计算机每秒运行1亿次，对1亿个数排序，冒泡排序需要1亿秒(现代计算机肯定比这个快)。一天是多少秒了？86400秒。排序时间约等于1150天~，因此冒泡排序虽然易懂，很形象，但是一般公司都不会用到它。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2F2017%2F03%2F15%2Ffirst-blog%2F</url>
    <content type="text"><![CDATA[随笔最近几天将博客由jeklly换成了hexo，之前的博客确实很丑，并且东西很杂乱，什么都有，而且也没有什么分类的，让人感觉很乱。偶然的机会看到hexo的next主题，心里一种欢呼，这不就是我想要的样子吗？当夜折腾到凌晨1点，总算把博客搭好，顺便了解下一些配置用用法。可是当一切具备的时候，我却有几天都没有写博客，为什么？我也不知道，我想着重新搭建了一个博客，总不能还像以前那样吧？这个博客应该写技术,而且应该全是技术，而且我觉得如果只能写一些比较浅显的技术，那不是有点丢人？所以就这样拖拉几天。我却始终没有动手。今天突然觉悟，其实为啥我要去纠结写什么？写的人是我，博客的目的是记录，是总结。不该给它太多其他的定义，技术浅显有怎样？那是你的感悟。写了技术之外的博客又怎样？那是你的感悟，你人生经历的一种，难道不重要吗？就算程序员，他的世界不应该只有代码，代码是一种艺术，艺术是一种美，而这种美来自于生活。不管怎样，只要你去做了，只要你能坚持到了，那有何尝不是一种成长。顺便献上一句鸡汤： 你只负责努力，余下交给命运]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简]]></title>
    <url>%2F2017%2F03%2F13%2Fnew-step%2F</url>
    <content type="text"><![CDATA[Hello, A new starting point, 2017.]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
</search>
